{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xDiTP_67MV",
        "colab_type": "code",
        "outputId": "0e853de7-a582-4e34-d811-b51f96687868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import (Dense, Conv2D, BatchNormalization, Activation, \n",
        "                          AveragePooling2D, Input, Flatten, \n",
        "                          Concatenate, Dropout, Lambda, \n",
        "                          Reshape, Embedding, Multiply)\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, Callback\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from __future__ import print_function\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSaEOfjG6ztu",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_hPw_jy8jea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset_CIFAR10:\n",
        "    def __init__(self, num_labeled):\n",
        "\n",
        "        def preprocess_imgs(x):\n",
        "            # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "            return x\n",
        "\n",
        "        def preprocess_labels(y):\n",
        "            y = y.reshape(-1, 1)\n",
        "            y = to_categorical(y, num_classes = 10)\n",
        "            return y\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train = preprocess_imgs(x_train)\n",
        "        y_train = preprocess_labels(y_train)\n",
        "        x_test = preprocess_imgs(x_test)\n",
        "        y_test = preprocess_labels(y_test)\n",
        "\n",
        "        # Number labeled examples to use for training\n",
        "        self.num_labeled = num_labeled\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        del x_train, y_train, x_test, y_test\n",
        "\n",
        "    def batch_labeled(self, batch_size):\n",
        "        # Get a random batch of labeled images and their labels\n",
        "        idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        labels = self.y_train[idx]\n",
        "        return imgs, labels\n",
        "\n",
        "    def batch_unlabeled(self, batch_size):\n",
        "        # Get a random batch of unlabeled images\n",
        "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        return imgs\n",
        "\n",
        "    def training_set(self):\n",
        "        return self.x_train, self.y_train\n",
        "\n",
        "    def test_set(self):\n",
        "        return self.x_test, self.y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5nWXxcZdgQb",
        "colab_type": "text"
      },
      "source": [
        "## Check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5jJexNdigk",
        "colab_type": "code",
        "outputId": "80c9c905-369a-40b7-e388-6b7a3d1d67c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Training set\n",
        "d_ytrain = {}\n",
        "for i in range(10):\n",
        "  d_ytrain[i] = 0\n",
        "for i in range(len(y_train)):\n",
        "  d_ytrain[y_train[i][0]] = d_ytrain.get(y_train[i][0]) + 1\n",
        "print(\"CIFAR-10 training set:\")\n",
        "print(d_ytrain)\n",
        "\n",
        "# Test set\n",
        "d_ytest = {}\n",
        "for i in range(10):\n",
        "  d_ytest[i] = 0\n",
        "for i in range(len(y_test)):\n",
        "  d_ytest[y_test[i][0]] = d_ytest.get(y_test[i][0]) + 1\n",
        "print(\"CIFAR-10 test set:\")\n",
        "print(d_ytest)\n",
        "\n",
        "del x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "CIFAR-10 training set:\n",
            "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n",
            "CIFAR-10 test set:\n",
            "{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrpoGSKdmrp",
        "colab_type": "text"
      },
      "source": [
        "# Number of labeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvYSG5vKdoSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labeled = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjhxJulqdyFs",
        "colab_type": "text"
      },
      "source": [
        "# SCGAN-2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcTHBiShR9m",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA0DtW_2d14g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "input_shape = img_shape = (32, 32, 3)\n",
        "num_classes = 10\n",
        "z_dim = 100   # Size of the noise vector, used as input to the Generator\n",
        "n = 3\n",
        "depth = n * 6 + 2   # Depth of ResNet model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgzlORUA7eUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eZ2B42_7fiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abhpWtyBfaxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "def build_generator(z_dim):\n",
        "  \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ), dtype='float32')\n",
        "    label_embedding = Dense(z_dim, input_dim=num_classes)(label)\n",
        "    joined_representation = Multiply()([z, label_embedding])\n",
        "    \n",
        "#     model = Sequential()\n",
        "\n",
        "    # Reshape input into 8x8x256 tensor via a fully connected layer\n",
        "    model = Dense(256 * 8 * 8, input_dim=z_dim)(joined_representation)\n",
        "    model = Reshape((8, 8, 256))(model)\n",
        "\n",
        "    # Transposed convolution layer, from 8x8x256 into 16x16x128 tensor\n",
        "    model = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(model)\n",
        "\n",
        "    # Batch normalization\n",
        "    model = BatchNormalization()(model)\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model = LeakyReLU(alpha=0.01)(model)\n",
        "\n",
        "    # Transposed convolution layer, from 16x16x128 to 16x16x64 tensor\n",
        "    model = Conv2DTranspose(64, kernel_size=3, strides=1, padding='same')(model)\n",
        "\n",
        "    # Batch normalization\n",
        "    model = BatchNormalization()(model)\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model = LeakyReLU(alpha=0.01)(model)\n",
        "\n",
        "    # Transposed convolution layer, from 16x16x64 to 32x32x3 tensor\n",
        "    model = Conv2DTranspose(3, kernel_size=3, strides=2, padding='same')(model)\n",
        "\n",
        "    # Output layer with tanh activation\n",
        "    conditioned_img = Activation('tanh')(model)\n",
        "    \n",
        "#     conditioned_img = model(joined_representation)\n",
        "\n",
        "    model = Model([z, label], conditioned_img)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ09_xGfffbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_net(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes, kernel_initializer='he_normal')(y)\n",
        "    # outputs = Dense(num_classes,\n",
        "    #                 activation='softmax',\n",
        "    #                 kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx6rxPK2hiqj",
        "colab_type": "code",
        "outputId": "2697ff1f-5054-4484-f55e-efdd96fc938a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "build_discriminator_net(img_shape, depth).summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTS0nBXhMhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_supervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    # Softmax activation, giving predicted probability distribution over the real classes\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuxULq-IhOlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_unsupervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    def predict(x):\n",
        "        # Transform distribution over real classes into a binary real-vs-fake probability\n",
        "        prediction = 1.0 - (1.0 / (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
        "        return prediction\n",
        "\n",
        "    # 'Real-vs-fake' output neuron defined above\n",
        "    model.add(Lambda(predict))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3LcMzjZhQFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ))\n",
        "    img = generator([z, label])\n",
        "    output = discriminator(img)\n",
        "    model = Model([z, label], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X4FZirFhWA7",
        "colab_type": "code",
        "outputId": "edd82943-81a3-4298-d19e-1e8116786c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "# Core Discriminator network:\n",
        "# These layers are shared during supervised and unsupervised training\n",
        "\n",
        "discriminator_net = build_discriminator_net(input_shape=img_shape, depth=depth)\n",
        "\n",
        "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
        "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "# discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "#                                  metrics=['accuracy'],\n",
        "#                                  optimizer=Adam(lr=lr_schedule(0)))\n",
        "\n",
        "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
        "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "                                metrics=['accuracy'],\n",
        "                                optimizer=Adam())\n",
        "# discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "#                                 metrics=['accuracy'],\n",
        "#                                 optimizer=Adam(lr=lr_schedule(0)))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1nVA_1egKoX",
        "colab_type": "code",
        "outputId": "086537e3-5a04-4b68-8b8e-22cf20ffb108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "discriminator_unsupervised.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_1 (Model)              (None, 10)                274442    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpsqRccThV02",
        "colab_type": "code",
        "outputId": "3fe3c7dc-eea9-4e30-e1a0-24b7178a587b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Build the Generator\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "discriminator_supervised.trainable = False\n",
        "discriminator_unsupervised.trainable = False\n",
        "gan = build_gan(generator, discriminator_unsupervised)\n",
        "gan.compile(loss='binary_crossentropy', \n",
        "            metrics=['accuracy'], \n",
        "            optimizer=Adam())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yrZyUchhhER",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW25Txjghf7v",
        "colab_type": "code",
        "outputId": "4186bff9-e181-4b70-f7b3-45bdf642815b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%mkdir models\n",
        "%mkdir losses\n",
        "%mkdir models/models-label-2000\n",
        "%mkdir losses/losses-label-2000"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n",
            "mkdir: cannot create directory ‘losses’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNb1Q6IQkL-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'models')\n",
        "model_name = 'cifar10_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.accs = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.accs.append(logs.get('acc'))\n",
        "\n",
        "history = LossHistory()\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3q3OrbkUGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR2Hun3ChbSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain(iterations_pre, batch_size, save_interval, iter_epochs):\n",
        "  for iteration in range(iterations_pre):\n",
        "      imgs, labels = dataset.training_set()\n",
        "      # imgs, labels = dataset.batch_labeled(batch_size)\n",
        "      x_test, y_test = dataset.test_set()\n",
        "\n",
        "      # Compute quantities required for featurewise normalization (std, mean, and principal components if ZCA whitening is applied).\n",
        "      datagen.fit(imgs)\n",
        "      discriminator_supervised.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  epochs=iter_epochs, verbose=1, workers=4,\n",
        "                  callbacks=callbacks)\n",
        "      \n",
        "      if (iteration + 1) % save_interval == 0:\n",
        "          \n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [D loss class: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, history.losses[-1], 100 * history.accs[-1]))\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "          losses.append(history.losses[-1])\n",
        "          accs.append(history.accs[-1])\n",
        "          discriminator_supervised.save_weights(\"./models/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "          \n",
        "          # x, y = dataset.training_set()\n",
        "          # _, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "          # print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGIfXoRgW-0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def pretrain(iterations_pre, batch_size, save_interval):\n",
        "#   for iteration in range(iterations_pre):\n",
        "#       # imgs, labels = dataset.training_set()\n",
        "#       imgs, labels = dataset.batch_labeled(1000)\n",
        "      \n",
        "#       loss, acc = discriminator_supervised.train_on_batch(imgs, labels)\n",
        "      \n",
        "#       if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "#           losses.append(loss)\n",
        "#           accs.append(acc)\n",
        "#           iteration_checkpoints.append(iteration + 1)\n",
        "          \n",
        "#           # Output training progress\n",
        "#           print(\n",
        "#               \"%d [D loss class: %.4f, acc: %.2f%%]\"\n",
        "#               % (iteration + 1, loss, 100 * acc))\n",
        "#           discriminator_supervised.save(\"./models/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "          \n",
        "#           # x, y = dataset.training_set()\n",
        "#           # _, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "#           # print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nIEI7nOhsUa",
        "colab_type": "code",
        "outputId": "241fde2e-55f6-4307-ab3e-53c49544424e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Pretrain\n",
        "\n",
        "# Set hyperparameters\n",
        "iterations_pre = 1\n",
        "iter_epochs = 40    # 20\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "losses = []\n",
        "accs = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "dataset = Dataset_CIFAR10(num_labeled)\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "pretrain(iterations_pre, batch_size, save_interval\n",
        "         , iter_epochs\n",
        "         )\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Use time:\" + str(endtime-starttime) + \"s\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 111s 71ms/step - loss: 1.6138 - acc: 0.4730 - val_loss: 1.7896 - val_acc: 0.4511\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.45110, saving model to /content/models/cifar10_model.001.h5\n",
            "Epoch 2/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 1.1657 - acc: 0.6473 - val_loss: 1.0870 - val_acc: 0.6867\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.45110 to 0.68670, saving model to /content/models/cifar10_model.002.h5\n",
            "Epoch 3/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.9958 - acc: 0.7075 - val_loss: 1.2205 - val_acc: 0.6415\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.68670\n",
            "Epoch 4/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 57ms/step - loss: 0.9027 - acc: 0.7434 - val_loss: 1.0303 - val_acc: 0.7053\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.68670 to 0.70530, saving model to /content/models/cifar10_model.004.h5\n",
            "Epoch 5/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 0.8480 - acc: 0.7635 - val_loss: 0.9716 - val_acc: 0.7299\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.70530 to 0.72990, saving model to /content/models/cifar10_model.005.h5\n",
            "Epoch 6/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.7956 - acc: 0.7838 - val_loss: 0.9086 - val_acc: 0.7563\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.72990 to 0.75630, saving model to /content/models/cifar10_model.006.h5\n",
            "Epoch 7/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 87s 56ms/step - loss: 0.7670 - acc: 0.7944 - val_loss: 0.9140 - val_acc: 0.7485\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.75630\n",
            "Epoch 8/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.7398 - acc: 0.8054 - val_loss: 0.8269 - val_acc: 0.7808\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.75630 to 0.78080, saving model to /content/models/cifar10_model.008.h5\n",
            "Epoch 9/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.7190 - acc: 0.8121 - val_loss: 0.9485 - val_acc: 0.7453\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.78080\n",
            "Epoch 10/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 57ms/step - loss: 0.6951 - acc: 0.8234 - val_loss: 0.7600 - val_acc: 0.8037\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.78080 to 0.80370, saving model to /content/models/cifar10_model.010.h5\n",
            "Epoch 11/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.6741 - acc: 0.8296 - val_loss: 1.0303 - val_acc: 0.7481\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.80370\n",
            "Epoch 12/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 86s 55ms/step - loss: 0.6695 - acc: 0.8320 - val_loss: 0.8983 - val_acc: 0.7763\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.80370\n",
            "Epoch 13/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 0.6525 - acc: 0.8397 - val_loss: 0.8253 - val_acc: 0.7952\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.80370\n",
            "Epoch 14/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 0.6439 - acc: 0.8436 - val_loss: 0.7094 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.80370 to 0.82730, saving model to /content/models/cifar10_model.014.h5\n",
            "Epoch 15/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 0.6314 - acc: 0.8469 - val_loss: 0.9351 - val_acc: 0.7451\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.82730\n",
            "Epoch 16/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 87s 56ms/step - loss: 0.6216 - acc: 0.8503 - val_loss: 0.8585 - val_acc: 0.7764\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.82730\n",
            "Epoch 17/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6120 - acc: 0.8540 - val_loss: 0.7688 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.82730\n",
            "Epoch 18/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6112 - acc: 0.8557 - val_loss: 0.6695 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.82730 to 0.83650, saving model to /content/models/cifar10_model.018.h5\n",
            "Epoch 19/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 57ms/step - loss: 0.5956 - acc: 0.8622 - val_loss: 0.7341 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.83650\n",
            "Epoch 20/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 95s 61ms/step - loss: 0.5905 - acc: 0.8636 - val_loss: 0.8288 - val_acc: 0.7993\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.83650\n",
            "Epoch 21/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5896 - acc: 0.8620 - val_loss: 0.7656 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.83650\n",
            "Epoch 22/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5812 - acc: 0.8648 - val_loss: 0.7635 - val_acc: 0.8173\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.83650\n",
            "Epoch 23/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 0.5786 - acc: 0.8682 - val_loss: 0.7358 - val_acc: 0.8192\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.83650\n",
            "Epoch 24/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.5701 - acc: 0.8709 - val_loss: 0.7537 - val_acc: 0.8226\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.83650\n",
            "Epoch 25/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 87s 56ms/step - loss: 0.5724 - acc: 0.8680 - val_loss: 0.7113 - val_acc: 0.8371\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.83650 to 0.83710, saving model to /content/models/cifar10_model.025.h5\n",
            "Epoch 26/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 87s 55ms/step - loss: 0.5656 - acc: 0.8730 - val_loss: 0.7686 - val_acc: 0.8182\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.83710\n",
            "Epoch 27/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 85s 55ms/step - loss: 0.5625 - acc: 0.8740 - val_loss: 0.7639 - val_acc: 0.8257\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.83710\n",
            "Epoch 28/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 87s 56ms/step - loss: 0.5573 - acc: 0.8766 - val_loss: 0.6713 - val_acc: 0.8387\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.83710 to 0.83870, saving model to /content/models/cifar10_model.028.h5\n",
            "Epoch 29/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.5546 - acc: 0.8782 - val_loss: 0.8246 - val_acc: 0.7983\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.83870\n",
            "Epoch 30/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 57ms/step - loss: 0.5531 - acc: 0.8759 - val_loss: 0.7062 - val_acc: 0.8317\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.83870\n",
            "Epoch 31/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.5468 - acc: 0.8799 - val_loss: 0.6984 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.83870\n",
            "Epoch 32/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5498 - acc: 0.8786 - val_loss: 0.6351 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.83870 to 0.85680, saving model to /content/models/cifar10_model.032.h5\n",
            "Epoch 33/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5449 - acc: 0.8807 - val_loss: 0.6895 - val_acc: 0.8427\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.85680\n",
            "Epoch 34/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5425 - acc: 0.8811 - val_loss: 0.6858 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.85680\n",
            "Epoch 35/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5377 - acc: 0.8833 - val_loss: 0.6598 - val_acc: 0.8450\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.85680\n",
            "Epoch 36/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5398 - acc: 0.8814 - val_loss: 0.9920 - val_acc: 0.7607\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.85680\n",
            "Epoch 37/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5379 - acc: 0.8831 - val_loss: 0.6164 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.85680 to 0.85730, saving model to /content/models/cifar10_model.037.h5\n",
            "Epoch 38/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.5340 - acc: 0.8834 - val_loss: 0.6867 - val_acc: 0.8425\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.85730\n",
            "Epoch 39/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5310 - acc: 0.8865 - val_loss: 0.7047 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.85730\n",
            "Epoch 40/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 96s 61ms/step - loss: 0.5254 - acc: 0.8887 - val_loss: 0.7724 - val_acc: 0.8175\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.85730\n",
            "1 [D loss class: 0.4943, acc: 84.38%]\n",
            "Use time:5488.735918s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlf871AuWzWb",
        "colab_type": "code",
        "outputId": "3f814a8e-340d-4fe6-e6b4-e7576c6e6d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "%ls models"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar10_model.001.h5  cifar10_model.008.h5  cifar10_model.028.h5\n",
            "cifar10_model.002.h5  cifar10_model.010.h5  cifar10_model.032.h5\n",
            "cifar10_model.004.h5  cifar10_model.014.h5  cifar10_model.037.h5\n",
            "cifar10_model.005.h5  cifar10_model.018.h5  discriminator_supervised-1.h5\n",
            "cifar10_model.006.h5  cifar10_model.025.h5  \u001b[0m\u001b[01;34mmodels-label-125\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lzo6IL5z9wE",
        "colab_type": "code",
        "outputId": "0c933c37-9523-4836-823e-e88c15a7c400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "# tmodel.load_weights(\"./models/discriminator_supervised-2000.h5\", by_name=False)\n",
        "tmodel.load_weights(\"./models/cifar10_model.037.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 17s 347us/step\n",
            "Training Accuracy: 89.48%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etkdmP6Ez7Jy",
        "colab_type": "code",
        "outputId": "5eafb462-3dc4-4cc5-d9f5-4c6a607dd1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/cifar10_model.037.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 325us/step\n",
            "Test Accuracy: 85.73%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xEVpUqmWb90",
        "colab_type": "code",
        "outputId": "d3a2212b-586c-4273-92ec-9b45237b1e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "div = 1000\n",
        "ty = [history.accs[i*div] for i in range(0, len(history.accs)//div)]\n",
        "tx = [x for x in range(1*div, (len(ty)+1)*div, div)]\n",
        "print(max(ty))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, ty, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc918fd1860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFiCAYAAACkvHqaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBCCDEg/D8xznk3wZgBDQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOHm2K6nn-Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accs = []\n",
        "# tx = [x for x in range(100,2100,100)]\n",
        "# acc_max = [0,0]\n",
        "\n",
        "# x, y = dataset.test_set()\n",
        "\n",
        "# tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "# tmodel.compile(loss='categorical_crossentropy',\n",
        "#                          metrics=['accuracy'],\n",
        "#                          optimizer=Adam())\n",
        "\n",
        "# for e in tx:\n",
        "#   tmodel.load_weights(\"./models/discriminator_supervised-\"+ str(e) +\".h5\", by_name=False)\n",
        "#   _, acc = tmodel.evaluate(x, y)\n",
        "#   accs.append(acc)\n",
        "# print(max(accs))\n",
        "\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "# plt.xticks(tx, rotation=90)\n",
        "# plt.title(\"accs with epoch\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.ylabel(\"accs\")\n",
        "# plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSbSVx1khsOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations, batch_size, save_interval, iter_epochs, k):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    # Labels for real images: all ones\n",
        "    real = np.ones((batch_size, 1))\n",
        "\n",
        "    # Labels for fake images: all zeros\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        for _ in range(k):\n",
        "\n",
        "            # -------------------------\n",
        "            #  Train the Discriminator\n",
        "            # -------------------------\n",
        "\n",
        "            # Get labeled and unlabeled examples\n",
        "            imgs, labels = dataset.batch_labeled(batch_size)\n",
        "            imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "            # Generate a batch of fake images\n",
        "            z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "            fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "            fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "            gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "            discriminator_supervised.trainable = True\n",
        "            discriminator_unsupervised.trainable = True\n",
        "\n",
        "            # Train on real labeled examples\n",
        "            datagen.fit(imgs)\n",
        "            discriminator_supervised.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=iter_epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "            loss_d_supervised, acc_d_supervised = history.losses[-1], history.accs[-1]\n",
        "\n",
        "            # Train on real unlabeled examples\n",
        "            # Error\n",
        "            # datagen.fit(imgs_unlabeled)\n",
        "            # discriminator_unsupervised.fit_generator(datagen.flow(imgs_unlabeled, real, batch_size=batch_size),\n",
        "            #             validation_data=(x_test, np.ones((len(x_test), 1))),\n",
        "            #             epochs=iter_epochs, verbose=1, workers=4,\n",
        "            #             callbacks=callbacks)\n",
        "            # loss_d_unsupervised_real, acc_d_unsupervised_real = history.losses[-1], history.accs[-1]\n",
        "            loss_d_unsupervised_real, acc_d_unsupervised_real = discriminator_unsupervised.train_on_batch(imgs_unlabeled, real)\n",
        "\n",
        "            # Train on fake examples\n",
        "            loss_d_unsupervised_fake, acc_d_unsupervised_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "            # Calculate loss and acc\n",
        "            loss_d_unsupervised = 0.5 * np.add(loss_d_unsupervised_real, loss_d_unsupervised_fake)\n",
        "            loss_d = np.add(loss_d_supervised, loss_d_unsupervised)\n",
        "            acc_d_unsupervised = 0.5 * np.add(acc_d_unsupervised_real, acc_d_unsupervised_fake)\n",
        "            acc_d = np.add(acc_d_supervised, acc_d_unsupervised)\n",
        "        \n",
        "        # ---------------------\n",
        "        #  Train the Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "        fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "        gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "        discriminator_supervised.trainable = False\n",
        "        discriminator_unsupervised.trainable = False\n",
        "\n",
        "        # Train Generator\n",
        "        loss_g_unsupervised, acc_g_unsupervised = gan.train_on_batch([z,labels], real)\n",
        "\n",
        "        # Calculate loss and acc\n",
        "        loss_g = loss_g_unsupervised\n",
        "        acc_g = acc_g_unsupervised\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "            # Save losses to be plotted after training\n",
        "            losses_d_supervised.append(loss_d_supervised)\n",
        "            losses_d_unsupervised.append(loss_d_unsupervised)\n",
        "            losses_d_unsupervised_real.append(loss_d_unsupervised_real)\n",
        "            losses_d_unsupervised_fake.append(loss_d_unsupervised_fake)\n",
        "            losses_d.append(loss_d)\n",
        "            losses_g.append(loss_g)\n",
        "            \n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # Output training progress\n",
        "            print(\n",
        "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%]\"\n",
        "                % (iteration + 1, \n",
        "                   loss_d_supervised, 100 * acc_d_supervised,\n",
        "                   loss_d_unsupervised, 100 * acc_d_unsupervised, \n",
        "                   loss_g, 100 * acc_g))\n",
        "            \n",
        "            discriminator_supervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "            discriminator_unsupervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_unsupervised-\" + str(iteration+1) + \".h5\")\n",
        "            generator.save(\"./models/models-label-\" + str(num_labeled) + \"/generator-\" + str(iteration+1) + \".h5\")\n",
        "            file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_d_supervised.json\"\n",
        "            file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_d_unsupervised.json\"\n",
        "            file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_g.json\"\n",
        "            with open(file1, 'w') as json_file:\n",
        "                  json.dump(str(losses_d_supervised), json_file)\n",
        "            with open(file2, 'w') as json_file:\n",
        "                  json.dump(str(losses_d_unsupervised), json_file)\n",
        "            with open(file3, 'w') as json_file:\n",
        "                  json.dump(str(losses_g), json_file)\n",
        "\n",
        "            # x,y = dataset.training_set()\n",
        "            # _, acc = discriminator_supervised.evaluate(x,y)\n",
        "            # print(str(100*acc)+\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__5V6FJn6xB",
        "colab_type": "code",
        "outputId": "ac704ebb-0267-4412-8b65-f69392816671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 500 # 8000\n",
        "iter_epochs = 1 # 10\n",
        "batch_size = 32\n",
        "save_interval = 10\n",
        "k = 1 # iteration of Discriminator\n",
        "\n",
        "losses_d_supervised = []\n",
        "losses_d_unsupervised = []\n",
        "losses_d_unsupervised_real = []\n",
        "losses_d_unsupervised_fake = []\n",
        "losses_d = []\n",
        "losses_g = []\n",
        "\n",
        "iteration_checkpoints = []\n",
        "\n",
        "# discriminator_supervised = load_model(\"./models/discriminator_supervised-1200.h5\")\n",
        "discriminator_supervised = load_model(\"./models/cifar10_model.037.h5\")\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the SCGAN-2D for the specified number of iterations\n",
        "train(iterations, batch_size, save_interval, iter_epochs, k)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 19s 19s/step - loss: 0.5193 - acc: 0.9062 - val_loss: 0.5935 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2918 - acc: 1.0000 - val_loss: 0.5925 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5031 - acc: 0.9062 - val_loss: 0.5812 - val_acc: 0.8666\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.86590 to 0.86660, saving model to /content/models/cifar10_model.001.h5\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4422 - acc: 0.9062 - val_loss: 0.5804 - val_acc: 0.8687\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.86660 to 0.86870, saving model to /content/models/cifar10_model.001.h5\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3459 - acc: 0.9688 - val_loss: 0.5831 - val_acc: 0.8680\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3263 - acc: 1.0000 - val_loss: 0.5866 - val_acc: 0.8667\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4375 - acc: 0.8750 - val_loss: 0.5925 - val_acc: 0.8640\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3982 - acc: 0.9375 - val_loss: 0.5961 - val_acc: 0.8616\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4845 - acc: 0.9062 - val_loss: 0.6013 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4002 - acc: 0.9688 - val_loss: 0.6021 - val_acc: 0.8616\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "10 [D loss supervised: 0.4002, acc.: 96.88%] [D loss unsupervised: 0.1532, acc.: 100.00%] [G loss: 0.153560, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3633 - acc: 0.9375 - val_loss: 0.6078 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3688 - acc: 0.9375 - val_loss: 0.6094 - val_acc: 0.8576\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4307 - acc: 0.8750 - val_loss: 0.6098 - val_acc: 0.8599\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2668 - acc: 1.0000 - val_loss: 0.6090 - val_acc: 0.8601\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3871 - acc: 0.9688 - val_loss: 0.6083 - val_acc: 0.8613\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2662 - acc: 1.0000 - val_loss: 0.6109 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3119 - acc: 0.9688 - val_loss: 0.6138 - val_acc: 0.8583\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4060 - acc: 0.9062 - val_loss: 0.6135 - val_acc: 0.8590\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 0.6138 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4163 - acc: 0.9375 - val_loss: 0.6144 - val_acc: 0.8587\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "20 [D loss supervised: 0.4163, acc.: 93.75%] [D loss unsupervised: 0.1520, acc.: 100.00%] [G loss: 0.233351, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2319 - acc: 1.0000 - val_loss: 0.6147 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3508 - acc: 0.9375 - val_loss: 0.6152 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2892 - acc: 1.0000 - val_loss: 0.6177 - val_acc: 0.8583\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3337 - acc: 0.9375 - val_loss: 0.6209 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2451 - acc: 1.0000 - val_loss: 0.6237 - val_acc: 0.8569\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2259 - acc: 1.0000 - val_loss: 0.6260 - val_acc: 0.8561\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3301 - acc: 0.9688 - val_loss: 0.6296 - val_acc: 0.8554\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3267 - acc: 0.9688 - val_loss: 0.6326 - val_acc: 0.8546\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2639 - acc: 1.0000 - val_loss: 0.6354 - val_acc: 0.8533\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2304 - acc: 1.0000 - val_loss: 0.6416 - val_acc: 0.8519\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "30 [D loss supervised: 0.2304, acc.: 100.00%] [D loss unsupervised: 0.1543, acc.: 100.00%] [G loss: 6.572622, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2642 - acc: 0.9688 - val_loss: 0.6456 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2323 - acc: 1.0000 - val_loss: 0.6504 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3159 - acc: 0.9688 - val_loss: 0.6521 - val_acc: 0.8501\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2410 - acc: 1.0000 - val_loss: 0.6546 - val_acc: 0.8497\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2380 - acc: 1.0000 - val_loss: 0.6590 - val_acc: 0.8479\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86870\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfUFnOB4qLYj",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rt3ZhdCn6mk",
        "colab_type": "code",
        "outputId": "e0a9b039-a46b-4686-c3a5-a2b4fcfd723c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-300.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 22s 448us/step\n",
            "Training Accuracy: 88.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO7jfk1Pa-J8",
        "colab_type": "code",
        "outputId": "e8ae5ac1-25da-4a46-8b40-88cac26b3f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-300.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 385us/step\n",
            "Test Accuracy: 85.86%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk87Xx_Aa-Bc",
        "colab_type": "code",
        "outputId": "33c84839-fc5e-494f-9d1e-45708f01e27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "div = 10\n",
        "accs = []\n",
        "tx = [x for x in range(1*div, (len(iteration_checkpoints)+1) * div, div)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"SCGAN-2D's accs with epoch, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 461us/step\n",
            "10000/10000 [==============================] - 5s 452us/step\n",
            "10000/10000 [==============================] - 5s 457us/step\n",
            "10000/10000 [==============================] - 5s 459us/step\n",
            "10000/10000 [==============================] - 5s 457us/step\n",
            "10000/10000 [==============================] - 5s 458us/step\n",
            "10000/10000 [==============================] - 5s 458us/step\n",
            "10000/10000 [==============================] - 5s 454us/step\n",
            "10000/10000 [==============================] - 5s 460us/step\n",
            "10000/10000 [==============================] - 5s 455us/step\n",
            "10000/10000 [==============================] - 5s 450us/step\n",
            "10000/10000 [==============================] - 4s 440us/step\n",
            "10000/10000 [==============================] - 4s 447us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 443us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 438us/step\n",
            "10000/10000 [==============================] - 5s 454us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 440us/step\n",
            "10000/10000 [==============================] - 4s 435us/step\n",
            "10000/10000 [==============================] - 4s 432us/step\n",
            "10000/10000 [==============================] - 4s 435us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 438us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 425us/step\n",
            "10000/10000 [==============================] - 4s 436us/step\n",
            "10000/10000 [==============================] - 4s 427us/step\n",
            "10000/10000 [==============================] - 4s 428us/step\n",
            "10000/10000 [==============================] - 4s 425us/step\n",
            "10000/10000 [==============================] - 4s 432us/step\n",
            "10000/10000 [==============================] - 4s 430us/step\n",
            "10000/10000 [==============================] - 5s 450us/step\n",
            "10000/10000 [==============================] - 5s 452us/step\n",
            "10000/10000 [==============================] - 4s 444us/step\n",
            "10000/10000 [==============================] - 4s 430us/step\n",
            "10000/10000 [==============================] - 4s 428us/step\n",
            "10000/10000 [==============================] - 4s 434us/step\n",
            "10000/10000 [==============================] - 4s 426us/step\n",
            "10000/10000 [==============================] - 4s 433us/step\n",
            "10000/10000 [==============================] - 5s 450us/step\n",
            "10000/10000 [==============================] - 4s 439us/step\n",
            "10000/10000 [==============================] - 4s 437us/step\n",
            "10000/10000 [==============================] - 4s 429us/step\n",
            "10000/10000 [==============================] - 4s 444us/step\n",
            "10000/10000 [==============================] - 4s 430us/step\n",
            "10000/10000 [==============================] - 4s 432us/step\n",
            "10000/10000 [==============================] - 4s 439us/step\n",
            "0.8637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc90b9d03c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAFWCAYAAAAxJ1M2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZI0oP4fIPxDRbTB1F8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXfwxMiWbBLD",
        "colab_type": "code",
        "outputId": "0597a257-9546-4350-bf50-49fa07eef7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "d_supervised_loss = np.array(losses_d_supervised)\n",
        "d_unsupervised_loss = np.array(losses_d_unsupervised)\n",
        "d_unsupervised_real_loss = np.array(losses_d_unsupervised_real)\n",
        "d_unsupervised_fake_loss = np.array(losses_d_unsupervised_fake)\n",
        "d_loss = np.array(losses_d)\n",
        "g_loss = np.array(losses_g)  # Generator unsupervised loss\n",
        "all_loss = np.add(d_loss, g_loss)\n",
        "\n",
        "# Plot Discriminator supervised loss\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, d_supervised_loss, label=\"Discriminator supervised loss\", color='blue', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, d_unsupervised_loss, label=\"Discriminator unsupervised loss\", color='green', linestyle='dashed')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_real_loss, label=\"Discriminator unsupervised real loss\", color='yellow')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_fake_loss, label=\"Discriminator unsupervised fake loss\", color='yellow')\n",
        "plt.plot(iteration_checkpoints, g_loss, label=\"Generator unsupervised loss\", color='tab:red', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, all_loss, label=\"All losses\", color='black')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"SCGAN-2D's Discriminator Loss + Generator Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc8ff5f0f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAFWCAYAAAAR586OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXxN1/r48c9KIjELMVRFE1FDJCeC\nhGoMKTW09CqlVAzhoqYqOtBvS40tipp66ax6TbdatHp7r6rG2J+xMWtNiRguchAJCRnW7499zmmG\nk3nE83698qrstfbaz9n7nHQ/Z629ltJaI4QQQgghhBCi5HIo7gCEEEIIIYQQQmRNEjchhBBCCCGE\nKOEkcRNCCCGEEEKIEk4SNyGEEEIIIYQo4SRxE0IIIYQQQogSThI3IYQQQgghhCjhJHETQgghhBBC\niBJOEjchhMiCUmqZUmpSAbcZopTanMd9Wyul/ijIeIQQBqVUsFLqQg7rhiqldubxOMWyrxDi/iaJ\nmxAPCKVUK6XUbqVUjFLqulJql1IqMFV5TaXU50qpy0qpWKXUSaXUVKVUOUu5UkqNVkodVkrdUUr9\nTykVppTqY+dYy5VSSUqpmum2T1FKaaXUi6m2OVm2eWYS9xNKqZ8tMV9TSn2Tul3Lse5ZYo5VSh1V\nSr2vlKqUqk6oUmp5Hs5ZhFIq3tLuTcv5G66Usv1t1FoP11pPz23bWdFar9Rad8zjvju01g0KIg7L\n9R1SEG2lajNCKfV0QbZZ2JRS9ZRSayzvv1tKqVNKqcVKKffiji29wrhpt3xu/1mQbYqSRyk1XSl1\nxPK3e4qd8r5KqUil1G2l1AalVJVUZVWUUustZZFKqb453VcIUXAkcRPiAaCUqghsAhYDVYBawFTg\nrqW8CvAbUAZoqbWuAHQAXIG6lmYWAWOB1wA3SxvvAJ3THasc8AIQA/SzE851YKpSyjGH4VcGPgE8\nAQ8gFvgyXZ05lpirAYOAJ4Bd1qQzn56ztO0BzAImAJ8XQLt2KaWcCqvtomRJ9Evs/0OUUjqH9R4H\n9gCXgCZa64pAEHAGaFV4EdqNpdDfGw/K+0/kyWngTeDH9AVKKR/gY6A/UAO4A/wjVZWPgHuWshBg\nqWWfnOwrhCggJfZ/ukKIXKkPoLVerbVO1lrHa603a60PW8rHYyRE/bTWEZa6UVrrV7XWh5VS9YGR\nQB+t9c+W/ZO11ju11qHpjvUCcBOYBgy0E8t/MP4Hby+py0Br/ZPW+hut9S2t9R1gCcaNs726CVrr\nfcDfMJLLQenrKKVKK6X+qZQyW3rR9imlauQgjhit9fdAb2CgUsrX0t5ypdQMy7+rKqU2Wdq9rpTa\nYU1elFK1lVLfWXptzEqpJZbtocro/fxQKWUGpqTvNbH0SI609PTEWr4Zr2vpAbyllPqXUsrZUjfN\nUC5LD9fryugpjVFKrVVKlbaUVbbEe00pdcPyb3dL2UygNbBEKRWXKt4nLecsxvLfJ1MdK0wpNVMp\ntQvj5swru/Oa7toMVUqdtpy775VSj1q2K8v5uWp5vUdSnf9nlVLHLeflolLq9dwcMwemALu01uO1\n1hcAtNZXtdYLtNZrUsXeVSkVrv7qmfVLVZbpNcjhvhOUUoeB28rooZ6olDpjec3HlVLdLXW9gWVA\nS8s1u2nZXkkptcJynSOVUu+kel9meP/l5uQopbwt1/2mUuqYUupvqcrsXpusPifZHCtYKXVBKfWa\n5b1wWSk1KFV5mh7i/HyOcvH67V6LtFXUEst1P6mUap+qoJL6a5TDRaXUDJXJF1pKqYbqr5EHf6i0\noxbcLJ+XW0qpvfz1ZVuuaK2/0lr/hPH/gvRCgB+01tu11nHAJKCHUqqC+uvLukla6zit9U7ge4xE\nLct98xKnECJzkrgJ8WD4E0hWSn2llHpGKVU5XfnTwHda65RM9m8HRGmt9+fgWAOB1cAaoKFSqlm6\nco3xP+53lVKlcv4SbNoAx7KqoLWOBX7GSDzQWi9PlWAOBCoBtTGSu+FAfE4PrrXeC1ywtp3Oa5ay\nahjfLP8foC03Y5uASIyew1oY58eqBXDWss/MTA7dCWiG0Zv4JkYvZD/L6/AFXsoi7BcxekbrAH5A\nqGW7A0bvpQfwGMZ5WGJ5nW8DO4DRWuvyWuvRyuiZ/RGj99UNmA/8qJRyS3Ws/sAwoILl9eaIUqod\n8L4l1pqWfa3nqCPGda+Pce1eBMyWss+Bly29or7A1pweM4eeBr7NJvYmwBfAyxjn5WPge6WUS6pq\ndq9BDvd9CegCuGqtkzB6+1pjnIupwD+VUjW11icw3s+/Wa6Zq2X/xZa6XkBbYABpv9TIyfvP3usu\nBfwAbAaqA68AK5VS1qG6mV0bu5+THB72EctrqQX8HfjIzt+zrOTnc2SP3WuRqryFpU5V4F3gO/XX\nMMHlQBLwONAE432eYWiyJTH6GViFcZ77AP9QSjWyVPkISMD43Ay2/KTe/7AlSbb3k9OeLx/gkPUX\nrfUZjC/g6lt+krTWf6aqf8iyT3b7CiEKkCRuQjwAtNa3MIZ1aeBT4JrlG1prT5MbcDmLJqoC/0u9\nwfLN902lVIJSysOy7THgKWCV1voK8AvGTWL6eL4HrmHnJiUrlp6IycAbOah+CWNYaHqJGK/3cUuv\n4QHL+cmNrNquCXhorRMtz5tpoDnwKPCG1vq2pWcw9XNIl7TWi7XWSVrrzJLIOZZex2PAUWCz1vqs\n1joG+Anjxi8zi7TWl7TW1zFutP0BtNZmrfW3Wus7lmR3JsaNfWa6AKe01l9bYl0NnASeS1Vnudb6\nmKU8MYu20gsBvtBaH9Ra3wXewug58sQ4rxWAhoDSWp/QWlvfr4lAI6VURa31Da31wVwcMyfSvPeV\n8ZznTUuP1qeWzcOAj7XWeyzvqa8whiE/kaodu9cgF/tGWd8blh7oS1rrFK31WuAUxnssA8uXBn2A\nt7TWsZYe9Xn81RsCOXv/2fMEUB6YpbW+p7XeivEFhTX5yezaZPY5yYlEYJplv38DcUBununMz+co\ngxxci6vAAku8a4E/gC6Wv73PAmMtfxOuAh9iXKv0ugIRWusvLdfod4wvE3pZru8LwGRLO0eBr9LF\n6Ke1ds3kZ2QOX2p5jOHvqcVgfC7LA+n/hlrLsttXCFGAJHET4gFhudkN1Vq7Y3yz/CiwwFJsxriR\nykyGcks7VQEXQFk29wdOaK3DLb+vBPpm0rP2DvA2kHrI2GOWG+I4pVRc6srKeNboJ+BVrfWObF+w\n8Y38dTvbvwb+C6xRSl1SSs3JQ89fZm1/gPGcyGal1Fml1ETL9tpApKW3xJ6oHBzzSqp/x9v5vXwW\n+6ZOuu9Y6yqlyiqlPlbG8LlbwHbANbPhWhjvmfS9aJEY58MqJ68l27YtQ6rMQC1LQrAEo2fhqlLq\nE2U8twnGTeuzQKRSaptSqqW9xpUxOY+tp8GyLXXPQ2bPq6V572utl1h6shYA1veNB/BauvZrW16T\nld1rkMN905xTpdQA9dfQypsYn+eqmcRf1RJn6utWkNcsKl1Pfeq2M7s2mX1OcsKc7nOU+lzmRH4+\nRxnk4FpcTJeURmKcNw+M63I51b4fY/SopecBtEj3HgnB6H2sBjiR9hrmuKc7F+KAium2VcQYVplV\nWXb7CiEKkCRuQjyAtNYnMYbp+Fo2bQG6q8yfM9kKuCulArJpegDgpYwZJ/+HMZSuKsbNW/oYfsa4\neRuZatt5yxCv8lpr2w2UpUdvCzBda/11dq9PKVUeY4hbhgTP8s33VK11I+BJjG+zM/QKZtF2IMaN\naYaZ+yw9Gq9prb0wnrMbr4xnWqKAx1TmEz/ktLehoL2G0VvRQhuTbrSxbLcm4unjuoRxE5naY8DF\nVL/n9bWkadsyPMzN2rbWepHWuhnQCGOI1RuW7fu01t0wbng3AP+y17g2nse09TRYtqXuechsJsZf\ngB7ZxB4FzEzXXllLj2R2crKv7ZxaPgufAqMBN8trOUrm1ywao5cq9XUryGtWO93fDVvbmV2bLD4n\n+XUbKJvq90cKoM1M5eBaANRSSqX+/TGM8xaF0bNaNdV1r6i19iGjKGBbuvdIea31CIyRC0kYyX7q\nY6SO81jqL8TS/SzL4cs9BjRO1aYXxpd2f1p+nJRS9VLVb8xfQ9qz2lcIUYAkcRPiAaCMB9tfU39N\nPFEbYzjT/7NUmY/xDehXqYY91lJKzVdK+Wmt/8D4NniNUqqDUqqMpVcm9cQULTEeim+OMQzMHyMx\nXEXmidHbGM+ZZBV7LYzEcYnWOsubDKWUizKeqdsA3CDj7JMopZ5SSpks8d/CuKnN7Nm+1PtVVEp1\nxXju6p9a6yN26nRVSj1uuVGLAZItbe/FGIo6SylVThkTpNidYKWIVcDoZbipjOdu3k1XfoW0E4z8\nG6ivjKm9nZRSvTESqU25PG4pyzmw/jhhPBc5SCnlr4znu94D9mitI5RSgUqpFpae0dsYz/OkKKWc\nlbHmXSVtDMu8RQ6uZS5NAVpbPgu1wJhcA/BOVedTYLglRmW5xl1UziZfyO2+5TASrWuWWAbx1xcw\nYFwzd2WZZENrnYyRMM1UxkQSHhiTEeV2en+HdNfMBWO2zTvAm0qpUkqpYIxhs2uyujZZfE6sk/0s\nz2VsVuEYk16UVUYP/d/z2E5OZXctwEhax1jOTy+M982/tTHUdzMwz/K3xUEZE6XYG6q8CeNz19/S\nTinLZ8Lbcn2/w5jUqKwynntLMymU1ton9Rdi6X6GW+tZ2i2Nce/nZLnO1t73lcBzylgnshzG5FPf\nWZLw25YYplnev0FAN4zRDVnum9sTLoTImiRuQjwYYjEekt+jlLqNkbAdxehxQRvP3TyJkcTsUUrF\nYvQ0xGD0igGMwpiUYj7GMMELwHSMWRbPY9wsbNRaH9Fa/8/6AywEuio76/ZorXdhJDVZGYKRPExR\nmQyjxLhxjMUY1rYCOAA8abmhSO8RYB3GjeQJYBt/3WDY84Ol7SiMRHM+dmartKiH0TMYh7G8wj+0\n1r9abq6ew5iE4DzGueud9csuEgswloCIxnhP/Cdd+UKgpzJmnFyktTZj9FC+hnGu3wS6aq2jc3nc\nf2MkjNafKVrrLRiT1nyLkeTW5a/nfSpiJDg3MIaBmTGG24ExPDdCGUM9h2MMISsw2phwoQXgDhyy\nvBd2YfSaTLLU2Q8MxRjOeQPjMxOaw/Zzta/W+jjGM2q/YSRpJks8Vlsxejj+p5SyXpdXMBLesxg9\nxaswJkTJjZdIe83OaK3vYbyvn8F4D/0DGGDp0YfMr43dz4mlrHa615MbH2JMenEF4zmvlXlsJ0dy\ncC3ASG7rYZyfmUBPy+cIjC+0nIHjGNd+HXaGrFsSnI4Yn4dLGMNuZ2P0WoHR41fesn05dr6wyqFP\nMa7tSxh/6+KxPAupjWcCh2Oc06sYX/qkfj5uJMbfkqsYX8KMsOyTk32FEAVE6Rw/LyyEEEIIkTeW\nXsJDgJ/O3cQ2QgghkMRNCCGEEEIIIUo8GSophBBCiIeGUur/lP2JPH4q7tiEECIr0uMmhBBCCCGE\nECWc9LgJIYQQQgghRAmX2ZpDxaJq1ara09OzuMMQQgghhBBCiGJx4MCBaK11tfTbS1Ti5unpyf79\n+4s7DCGEEEIIIYQoFkqpSHvbZaikEEIIIYQQQpRwkrgJIYQQQgghRAkniZsQQgghhBBClHAl6hk3\nIYQQQgjx4EpMTOTChQskJCQUdyhCFLvSpUvj7u5OqVKlclRfEjchhBBCCFEkLly4QIUKFfD09EQp\nVdzhCFFstNaYzWYuXLhAnTp1crSPDJUUQgghhBBFIiEhATc3N0naxENPKYWbm1uuep8lcRNCCCGE\nEEVGkjYhDLn9LEjiJoQQQgghHhqOjo74+/vj4+ND48aNmTdvHikpKQDs37+fMWPG5PsYy5YtY8WK\nFbna58knn8zz8ZYvX86lS5fyvH9JkpdzZ09wcLDd9aEz234/kGfchBBCCCHEQ6NMmTKEh4cDcPXq\nVfr27cutW7eYOnUqAQEBBAQE5Kv9pKQkhg8fnuv9du/enedjLl++HF9fXx599NEc75OcnIyjo2Oe\nj5kfSUlJODnZT0Pycu4eFtLjJoQQQghRwH7++WcSExOLOwyRjerVq/PJJ5+wZMkStNaEhYXRtWtX\nALZt24a/vz/+/v40adKE2NhYAGbPno3JZKJx48ZMnDgRMHpxxo4dS0BAAAsXLmTKlCnMnTvXVjZu\n3DgCAgLw9vZm37599OjRg3r16vHOO+/YYilfvjwAYWFhBAcH07NnTxo2bEhISAhaawCmTZtGYGAg\nvr6+DBs2DK0169atY//+/YSEhODv7098fDy//PILTZo0wWQyMXjwYO7evQuAp6cnEyZMoGnTpnzz\nzTdpzsU333yDr68vjRs3pk2bNoCREI4ePdpWp2vXroSFhdniHTduHD4+PrRv355r164BcObMGTp3\n7kyzZs1o3bo1J0+eBCA0NJThw4fTokUL3nzzTTw9Pbl586at7Xr16nHlypU0527RokU0atQIPz8/\n+vTpA8Dt27cZPHgwzZs3p0mTJmzcuBGA+Ph4+vTpg7e3N927dyc+Pj7b67969WpMJhO+vr5MmDAB\nMBLa0NBQfH19MZlMfPjhh5nGUtSkx00IIYQQogCdOnWKjh07snz5cgYOHFjc4YhseHl5kZyczNWr\nV9Nsnzt3Lh999BFBQUHExcVRunRpfvrpJzZu3MiePXsoW7Ys169ft9W/d++ebQjelClT0rTl7OzM\n/v37WbhwId26dePAgQNUqVKFunXrMm7cONzc3NLU//333zl27BiPPvooQUFB7Nq1i1atWjF69Ggm\nT54MQP/+/dm0aRM9e/ZkyZIlzJ07l4CAABISEggNDeWXX36hfv36DBgwgKVLlzJ27FgA3NzcOHjw\nYIbzMG3aNP773/9Sq1atNAlVZm7fvk1AQAAffvgh06ZNY+rUqSxZsoRhw4axbNky6tWrx549exg5\nciRbt24FjFlFd+/ejaOjI8nJyaxfv55BgwaxZ88ePDw8qFGjRppjzJo1i3PnzuHi4mKLaebMmbRr\n144vvviCmzdv0rx5c55++mk+/vhjypYty4kTJzh8+DBNmzbNMv5Lly4xYcIEDhw4QOXKlenYsSMb\nNmygdu3aXLx4kaNHjwLYjmsvlqImPW5CCCGEEAXo8uXLALbheCJzwcEZf/7xD6Pszh375cuXG+XR\n0RnLClJQUBDjx49n0aJF3Lx5EycnJ7Zs2cKgQYMoW7YsAFWqVLHV7927d6Zt/e1vfwPAZDLh4+ND\nzZo1cXFxwcvLi6ioqAz1mzdvjru7Ow4ODvj7+xMREQHAr7/+SosWLTCZTGzdupVjx45l2PePP/6g\nTp061K9fH4CBAweyffv2bOMMCgoiNDSUTz/9lOTk5GzODjg4ONja6tevHzt37iQuLo7du3fTq1cv\n/P39efnll22fB4BevXrZhmf27t2btWvXArBmzRq7cfn5+RESEsI///lP29DKzZs3M2vWLPz9/QkO\nDiYhIYHz58+zfft2+vXrZ9vPz88vy/j37dtHcHAw1apVw8nJiZCQELZv346Xlxdnz57llVde4T//\n+Q8VK1bMNJaiJombEEIIIUQBMpvNABw5cqSYIxE5cfbsWRwdHalevXqa7RMnTuSzzz4jPj6eoKAg\n25C/zJQrVy7TMhcXF8BIdqz/tv6elJSUaX0wJlNJSkoiISGBkSNHsm7dOo4cOcLQoUPztJB5ZnEu\nW7aMGTNmEBUVRbNmzTCbzTg5OdkmbgGyPJ5SipSUFFxdXQkPD7f9nDhxwu6xW7ZsyenTp7l27Rob\nNmygR48eGdr88ccfGTVqFAcPHiQwMJCkpCS01nz77be29s+fP4+3t3euz0NmKleuzKFDhwgODmbZ\nsmUMGTIk01iKmiRuQgghhBAFyDp8ThK37IWFZfwZOdIoK1vWfnloqFFetWrGsty6du0aw4cPZ/To\n0RmmZj9z5gwmk4kJEyYQGBjIyZMn6dChA19++SV37twBSDNUsrBZk6aqVasSFxfHunXrbGUVKlSw\nPYPXoEEDIiIiOH36NABff/01bdu2zbb9M2fO0KJFC6ZNm0a1atWIiorC09OT8PBwUlJSiIqKYu/e\nvbb6KSkpthhWrVpFq1atqFixInXq1LE9P6e15tChQ3aPp5Sie/fujB8/Hm9v7wzDRa3HfOqpp5g9\nezYxMTHExcXRqVMnFi9ebHvu7/fffwegTZs2rFq1CoCjR49y+PDhLF9v8+bN2bZtG9HR0SQnJ7N6\n9Wratm1LdHQ0KSkpvPDCC8yYMYODBw9mGktRk2fchBBCCCEKkLXH7erVq1y9ejVDT44oXvHx8fj7\n+5OYmIiTkxP9+/dn/PjxGeotWLCAX3/9FQcHB3x8fHjmmWdwcXEhPDycgIAAnJ2defbZZ3nvvfeK\nJG5XV1eGDh2Kr68vjzzyCIGBgbYy68QfZcqU4bfffuPLL7+kV69eJCUlERgYmKOZGt944w1OnTqF\n1pr27dvTuHFjAOrUqUOjRo3w9vZO89xYuXLl2Lt3LzNmzKB69eq2YY8rV65kxIgRzJgxg8TERPr0\n6WNrK73evXsTGBjIcuv411SSk5Pp168fMTExaK0ZM2YMrq6uTJo0ibFjx+Ln50dKSgp16tRh06ZN\njBgxgkGDBuHt7Y23tzfNmjXL8vXWrFmTWbNm8dRTT6G1pkuXLnTr1o1Dhw4xaNAgW0/j+++/n2ks\nRU1Zs9WSICAgQN+v6yoIIYQQQgBMmDCBOXPmALBlyxbat29fzBGVHCdOnCjQYW2i+JQvX75Yep0e\nNPY+E0qpA1rrDOtSyFBJIYQQQogCZDabKVOmDIBtZjohhMgvSdyEEEIIIQqQ2Wymbt26VKtWTZ5z\nEw8s6W0revKMmxBCCCFEATKbzbi5uVG9enVJ3IQQBUZ63IQQQgghCpA1cTOZTBw7dizNdOpCCJFX\nkrgJIYQQQhSg1Inb7du3OXfuXHGHJIR4ABRq4qaUclVKrVNKnVRKnVBKtSzM4wkhhBBCFCettS1x\n8/X1BWQ9NyFEwSjsHreFwH+01g2BxsCJbOoLIYQQQty3YmNjSUpKws3NDR8fH0ASt5LG0dERf39/\nfHx8aNy4MfPmzbMNZ92/fz9jxozJ9zGWLVvGihUrcrXPk08+mefjLV++nEuXLuV5//vdkCFDOH78\neL7bKV++fK62F7VCm5xEKVUJaAOEAmit7wH3Cut4QgghhBDFzbr4tpubG+XLl8fLy0uWBChhypQp\nQ3h4OGAskt63b19u3brF1KlTCQgIICAgw/JZuZKUlJSjBa/T2717d56PuXz5cnx9fXn00UdzvE9y\ncjKOjo55PmZRyyrezz77rIijKR6F2eNWB7gGfKmU+l0p9ZlSqlwhHk8IIYQQolhZEzeXP0+R8Mcf\nmEwm6XErwapXr84nn3zCkiVL0FoTFhZG165dAdi2bRv+/v74+/vTpEkTYmNjAZg9ezYmk4nGjRsz\nceJEAIKDgxk7diwBAQEsXLiQKVOmMHfuXFvZuHHjCAgIwNvbm3379tGjRw/q1avHO++8Y4vF2qsT\nFhZGcHAwPXv2pGHDhoSEhKC1BmDatGkEBgbi6+vLsGHD0Fqzbt069u/fT0hICP7+/sTHx/PLL7/Q\npEkTTCYTgwcP5u7duwB4enoyYcIEmjZtyjfffJPmXISGhrJu3bpcxTNx4kQaNWqEn58fr7/+erbt\ntGnThi5dutCgQQOGDx9u6+ncvHkzLVu2pGnTpvTq1cu21EDqeD/44AOaN29uazciIgKTyWQ7x/v3\n7yc5OZnQ0FB8fX0xmUx8+OGHAJw5c4bOnTvTrFkzWrduzcmTJwE4d+4cLVu2xGQypbkWmdFa88Yb\nb9jaX7t2LQCXL1+mTZs2+Pv74+vry44dOzKNJT8KczkAJ6Ap8IrWeo9SaiEwEZiUupJSahgwDOCx\nxx4rxHCEEEIIIQrX9evXAdArVxJb2x2TycSmTZu4e/cuLi4uxRydsMfLy4vk5GSuXr2aZvvcuXP5\n6KOPCAoKIi4ujtKlS/PTTz+xceNG9uzZQ9myZW3XG+DevXvs378fgClTpqRpy9nZmf3797Nw4UK6\ndevGgQMHqFKlCnXr1mXcuHG4ubmlqf/7779z7NgxHn30UYKCgti1axetWrVi9OjRTJ48GYD+/fuz\nadMmevbsyZIlS5g7dy4BAQEkJCQQGhrKL7/8Qv369RkwYABLly5l7NixgNEbfPDgwVydI3vxeHt7\ns379ek6ePIlSips3b2bbzt69ezl+/DgeHh507tyZ7777juDgYGbMmMGWLVsoV64cs2fPZv78+bbX\nmTreNWvWcO7cOerUqcPatWvp3bt3mvbDw8O5ePGirZfbGtOwYcNYtmwZ9erVY8+ePYwcOZKtW7fy\n6quvMmLECAYMGMBHH32Ubfzfffcd4eHhHDp0iOjoaAIDA2nTpg2rVq2iU6dOvP322yQnJ3Pnzp1M\nY8mPwkzcLgAXtNZ7LL+vw0jc0tBafwJ8AhAQEKALMR4hhBBCiEJl7XGr4uZGYmQkpmbNSE5O5sSJ\nE/j7+xdzdCVP8PLgDNte9HmRkYEjuZN4h2dXPpuhPNQ/lFD/UKLvRNPzXz3TlIWFhhVYbEFBQYwf\nP56QkBB69OiBu7s7W7ZsYdCgQZQtWxaAKlWq2OqnTyJS+9vf/gaAyWTCx8eHmjVrAkbSGBUVlSFx\na968Oe7u7gD4+/sTERFBq1at+PXXX5kzZw537tzh+vXr+Pj48Nxzz6XZ948//qBOnTrUr18fgIED\nB/LRRx/ZEres4syMvXieeOIJSpcuzd///ne6du1q66nMrh0vLy8AXnrpJXbu3Enp0qU5fvw4QUFB\ngJEAt2z513yGqeN98cUXWbt2LRMnTmTt2rW2Hi8rLy8vzp49yyuvvEKXLl3o2LEjcXFx7N69m169\netnqWXsgd+3axbfffgsYifCECROyjH/nzp289NJLODo6UqNGDdq2bcu+ffsIDAxk8ODBJCYm8vzz\nz+Pv7283lvwqtKGSWuv/AVFKqQaWTe2B/D81KIQQQghRQlkTtwqxscTt3GUbyiXDJUuus2fP4ujo\nSPXq1dNsnzhxIp999hnx8fZd2gkAACAASURBVPEEBQXZhtdlply5zJ8Isva2Ojg4pOl5dXBwICkp\nKdP6YEymkpSUREJCAiNHjmTdunUcOXKEoUOHkpCQkKPXmJM4nZycbEMXU1JSuHfvr6kp7MXj5OTE\n3r176dmzJ5s2baJz587ZtqOUSnNMpRRaazp06EB4eDjh4eEcP36czz//3G68vXv35l//+hd//vkn\nSinq1auXpr3KlStz6NAhgoODWbZsGUOGDCElJQVXV1db++Hh4Zw4cSJNDPnVpk0btm/fTq1atQgN\nDWXFihV2Y8mvwuxxA3gFWKmUcgbOAoMK+XhCCCGEEMXGmrhVcnQkOSaGxx9/HGdnZ0ncMpFVD1nZ\nUmWzLK9atmq+e9iuXbvG8OHDGT16dIYb+DNnzmAymTCZTOzbt4+TJ0/SoUMHpk2bRkhIiG2oZOpe\nt8JkTdKqVq1KXFwc69ato2dPo8exQoUKtmfwGjRoQEREBKdPn+bxxx/n66+/pm3bttm27+npyYED\nB3jxxRf5/vvvSUxMzLJ+XFwcd+7c4dlnnyUoKMjWk5ZVO3v37uXcuXN4eHiwdu1ahg0bxhNPPMGo\nUaNs8d6+fZuLFy/aegxTq1u3Lo6OjkyfPt1uz2F0dDTOzs688MILNGjQgH79+lGxYkXq1KnDN998\nQ69evdBac/jwYRo3bkxQUBBr1qyhX79+rFy5Mttz1Lp1az7++GMGDhzI9evX2b59Ox988AGRkZG4\nu7szdOhQ7t69y8GDB3n22WczxJJfhZq4aa3DgfxNzSOEEEIIcZ8wm81ULF0aJ6UgKQmHO3fw9vaW\nxK0EiY+Px9/fn8TERJycnOjfvz/jx4/PUG/BggX8+uuvODg44OPjwzPPPIOLiwvh4eEEBATg7OzM\ns88+y3vvvVckcbu6ujJ06FB8fX155JFHCAwMtJWFhoYyfPhwypQpw2+//caXX35Jr169SEpKIjAw\nMEezXA4dOpRu3brRuHFjOnfunGUPIhhLX3Tr1o2EhAS01syfPz/bdgIDAxk9ejSnT5/mqaeeonv3\n7jg4OLB8+XJeeukl2xDGGTNm2E3cwOh1e+ONN+wubH/x4kUGDRpk6/F7//33AVi5ciUjRoxgxowZ\nJCYm0qdPHxo3bszChQvp27cvs2fPplu3btmeo+7du/Pbb7/RuHFjlFLMmTOHRx55hK+++ooPPviA\nUqVKUb58eVasWJFpLPmhrLPClAQBAQHa+lCnEEIIIcT9JiQkhF0//cRPNR/l8V+34lStGgMGDODX\nX3/lwoULxR1esTtx4gTe3t7FHYYoBmFhYcydO5dNmzYVdyglir3PhFLqgNY6Q+dXYS/ALYQQQgjx\n0DCbzbg6O+Ps4UGp6tVRSmEymbh48SI3btwo7vCEEPcxSdyEEEIIIQqI2Wymmrs7lUP6Ev3Jp9z6\n+WfbBCWyELd4mAUHB0tvWz5J4iaEEEIIUUDMZjM1GjakSkgIN9euJfY//5WZJYUQBUISNyGEEEKI\nAmI2m6lcujQpd+7g7OHBvchIatWqhaurqyRuQoh8KezlAIQQQgghHgqJiYncunULNv3ItXr1cfb0\nIOYHY2iYr6+vJG5CiHyRHjchhBBCiAJw/fp1AFy1xtHVFWcPD1JiY0m+cQOTycTRo0cpSbN5CyHu\nL5K4CSGEEEIUAOvi266Ojji6ulLKwwNVqhSJly9jMpmIiYkhKiqqmKMUV65coW/fvnh5edGsWTNa\ntmzJ+vXriy2esLAwdu/eXWzHL27Lli1jxYoV+W4nODgYe8uKZbb9fiRDJYUQQgghCoCtx82SuJVv\n1YoG4b+jHB0xWZYCOHr0KI899lhxhvlQ01rz/PPPM3DgQFatWgVAZGQk33//faEeNykpCScn+7fd\nYWFhlC9fnieffLJA2iuJsoo3J4uDC4P0uAkhhBBCFABrj1slS+KmnJxQjo6A8YwbyMySxW3r1q04\nOzunSRY8PDx45ZVXAEhOTuaNN94gMDAQPz8/Pv74Y8BIroKDg+nZsycNGzYkJCTENuz1wIEDtG3b\nlmbNmtGpUycuX74MGD09Y8eOJSAggIULF/LDDz/QokULmjRpwtNPP82VK1eIiIhg2bJlfPjhh/j7\n+7Njxw4iIiJo164dfn5+tG/fnvPnzwMQGhrK8OHDadGiBW+++Waa17V8+XJGjx5t+71r166EhYUB\nUL58ed5++20aN27ME088wZUrVwD45ptv8PX1pXHjxrRp0yZH7YwbNw4fHx/at2/PtWvXADhz5gyd\nO3emWbNmtG7dmpMnT9qN19PTk5s3b9rarlevHleuXGHKlCnMnTsXgEWLFtGoUSP8/Pzo06cPALdv\n32bw4ME0b96cJk2asHHjRgDi4+Pp06cP3t7edO/enfj4+Gyv/+rVqzGZTPj6+jJhwgTbNQ8NDcXX\n1xeTycSHH36YaSzF7f5J1YUQQgghSjBr4vb46FG4eNUB4NqSj1CODlQdMYLatWtL4lbMjh07RtOm\nTTMt//zzz6lUqRL79u3j7t27BAUF0bFjRwB+//13jh07xqOPPkpQUBC7du2iRYsWvPLKK2zcuJFq\n1aqxdu1a3n77bb744gsA7t27Zxumd+PGDf7f//t/KKX47LPPmDNnDvPmzWP48OGUL1+e119/HYDn\nnnuOgQMHMnDgQL744gvGjBnDhg0bALhw4QK7d+/G0fKFQE7cvn2bJ554gpkzZ/Lmm2/y6aef8s47\n7zBt2jT++9//UqtWrTQJVVbtBAQE8OGHHzJt2jSmTp3KkiVLGDZsGMuWLaNevXrs2bOHkSNHsnXr\n1gzxJicns379egYNGsSePXvw8PCgRo0aaY4xa9Yszp07h4uLiy2mmTNn0q5dO7744gtu3rxJ8+bN\nefrpp/n4448pW7YsJ06c4PDhw1leV4BLly4xYcIEDhw4QOXKlenYsSMbNmygdu3aXLx40bbOovW4\n9mIpbpK4CSGEEEIUAGviVvfll3GqUAGA+MOHSLoWTdURIzCZTJK4pRPZf0CGbRWe6UyVvn1JiY8n\natjLGcorde+Oa4/uJN24wcUxr6Yp8/g6d89KjRo1ip07d+Ls7My+ffvYvHkzhw8fZt26dQDExMRw\n6tQpnJ2dad68Oe7u7gD4+/sTERGBq6srR48epUOHDoDRe1OzZk1b+71797b9+8KFC/Tu3ZvLly9z\n79496tSpYzem3377je+++w6A/v37p+ld69WrV66SNgBnZ2e6du0KQLNmzfj5558BCAoKIjQ0lBdf\nfJEePXpk246Dg4Pt9fTr148ePXoQFxfH7t276dWrl63e3bt37cbbu3dvpk2bxqBBg1izZk2ac2Pl\n5+dHSEgIzz//PM8//zwAmzdv5vvvv7f1yiUkJHD+/Hm2b9/OmDFjbPv5+fllGf++ffsIDg6mWrVq\nAISEhLB9+3YmTZrE2bNneeWVV+jSpYstUbcXS3GToZJCCCGEEAXAbDZTqlQpSl29atvm7OHJvchI\ntNb4+vpy4sQJEhMTizHKh5uPjw8HDx60/f7RRx/xyy+/2Ib9aa1ZvHgx4eHhhIeHc+7cOduNvIuL\ni20/R0dHkpKS0Frj4+Njq3/kyBE2b95sq1euXDnbv1955RVGjx7NkSNH+Pjjj0lISMh1/KnbS83J\nyYmUlBTb76nbLlWqFEqpNHGDMSnIjBkziIqKolmzZpjN5izbSU8pRUpKCq6urrbXHx4ezokTJ+zG\n27JlS06fPs21a9fYsGGD3WTxxx9/ZNSoURw8eJDAwEDbOf72229t7Z8/fx5vb+/sTlWOVa5cmUOH\nDhEcHMyyZcsYMmRIprEUN0nchBBCCCEKgHXx7YheL9q2OXt4oO/cIenaNUwmE4mJifz555/FGGXJ\n4vH1igw/Vfr2BcChTBm75a49ugPgVLlyhrLstGvXjoSEBJYuXWrbdufOHdu/O3XqxNKlS23J9Z9/\n/snt27czba9BgwZcu3aN3377DTDW8jt27JjdujExMdSqVQuAr776yra9QoUKxMbG2n5/8sknWbNm\nDQArV66kdevW2b4uT09PwsPDSUlJISoqir1792a7z5kzZ2jRogXTpk2jWrVqREVFZdlOSkqKrSdy\n1apVtGrViooVK1KnTh2++eYbwEh8Dx06ZPd4Sim6d+/O+PHj8fb2xs3NLU259ZhPPfUUs2fPJiYm\nhri4ODp16sTixYttzxT+/vvvALRp08Y2wczRo0c5fPhwlq+3efPmbNu2jejoaJKTk1m9ejVt27Yl\nOjqalJQUXnjhBWbMmMHBgwczjaW4yVBJIYQQQogCYDabqezigqNrJds2Zw8PABIjIzGZTIAxQYmP\nj0+xxPiwU0qxYcMGxo0bx5w5c6hWrRrlypVj9uzZAAwZMoSIiAiaNm2K1ppq1arZni+zx9nZmXXr\n1jFmzBhiYmJISkpi7Nixdq/vlClT6NWrF5UrV6Zdu3acO3cOMJ5p69mzJxs3bmTx4sUsXryYQYMG\n8cEHH1CtWjW+/PLLbF9XUFAQderUoVGjRnh7e2f7vBfAG2+8walTp9Ba0759exo3bgyQaTvlypVj\n7969zJgxg+rVq7N27VrASC5HjBjBjBkzSExMpE+fPra20uvduzeBgYEsX748Q1lycjL9+vUjJiYG\nrTVjxozB1dWVSZMmMXbsWPz8/EhJSaFOnTps2rSJESNGMGjQILy9vfH29qZZs2ZZvt6aNWsya9Ys\nnnrqKbTWdOnShW7dunHo0CEGDRpk62l8//33M42luKmStBBkQECAflDWWRBCCCHEw6Vt27bcPXWK\nVU88gZflGaV7Fy5wfmAoj7w7mVItWlCuXDkmTJjAzJkzizna4nHixIkCHeYmik758uVLRK/Tg8be\nZ0IpdUBrHZC+rvS4CSGEEEIUALPZjLuDA06pvpl3dnfn8V+22H5v0KCBbfY6IYTIDXnGTQghhBCi\nAJjNZioBjlkMqZKZJcX9Snrbip8kbkIIIYQQ+aS1xmw2U6tVKyq/9FKasuilS4kaMRIwFuI+d+5c\nmskohBAiJyRxE0IIIYTIp7i4OBITE6nZrBllAwPTlCXH3OL27t3olBTbBCWZzTwohBCZkcRNCCGE\nECKfrl+/DkD5mzdJsizEbeXs6Ym+e5ek//0vzcySQgiRG5K4CSGEEELkk9mSrCUt/4rbO3emKXP2\nNJYEuBcZiaenJ+XKlZPETQiRa5K4CSGEEELkkzVxc3V0zDA5iXUtt3uRkTg4OODr6yuJWzHbsGED\nSilOnjxp2xYREYGvry8AYWFhdO3aNcN+mW0XoihI4iaEEEIIkU9ZJW5ONWpQtkULHCpUAP6aWbIk\nraX7sFm9ejWtWrVi9erVxR2KEDkmiZsQQgghRD5llbgpBwc8vlpOpS5dACNxM5vNXLlypcjjFMZE\nMjt37uTzzz9nzZo1eW7n+vXrPP/88/j5+fHEE09w+PBhALZt24a/vz/+/v40adKE2NhYLl++TJs2\nbfD398fX15cdO3YAsHnzZlq2bEnTpk3p1auXbcr9iRMn0qhRI/z8/Hj99dfz/6LFA0ESNyGEEEKI\nfLImbpXsJG5W1h4263A8GS5ZPDZu3Ejnzp2pX78+bm5uHDhwIE/tvPvuuzRp0oTDhw/z3nvvMWDA\nAADmzp3LRx99RHh4ODt27KBMmTKsWrWKTp06ER4ezqFDh/D39yc6OpoZM2awZcsWDh48SEBAAPPn\nz8dsNrN+/XqOHTvG4cOHeeeddwry5Yv7mFNxByCEEEIIcb8zm81UrFCBOv/4yDYkMk358uVEL11G\n/d270sws2aFDh6IOtcQYO3Ys4eHhBdqmv78/CxYsyLLO6tWrefXVVwHo06cPq1evplmzZrk+1s6d\nO/n2228BaNeuHWazmVu3bhEUFMT48eMJCQmhR48euLu7ExgYyODBg0lMTOT555/H39+fbdu2cfz4\ncYKCggC4d+8eLVu2pFKlSpQuXZq///3vdO3aVZ6pEzaSuAkhhBBC5JPZbMatalUqtG9vt9yhXDlS\nYmJIvHyZau7u1KhRQ3rcisH169fZunUrR44cQSlFcnIySik++OCDAjvGxIkT6dKlC//+978JCgri\nv//9L23atGH79u38+OOPhIaGMn78eCpXrkyHDh3sPme3d+9efvnlF9atW8eSJUvYunVrgcUn7l+S\nuAkhhBBC5JPZbKZy6dLc2bcvwwLcAC6engDci4jE2d3dNkHJwyy7nrHCsG7dOvr378/HH39s29a2\nbVt27NjBY489lqu2WrduzcqVK5k0aRJhYWFUrVqVihUrcubMGUwmEyaTiX379nHy5EnKlCmDu7s7\nQ4cO5e7duxw8eJC3336bUaNGcfr0aR5//HFu377NxYsXefTRR7lz5w7PPvssQUFBeHl5FfRpEPcp\necZNCCGEECKfzGYz5WNiuDpvvt3yUtYlASIiAGOCkmPHjpGcnFxUIQqMYZLdu3dPs+2FF17I0+yS\nU6ZM4cCBA/j5+TFx4kS++uorwEhIfX198fPzo1SpUjzzzDOEhYXRuHFjmjRpwtq1a3n11VepVq0a\ny5cv56WXXsLPz4+WLVty8uRJYmNj6dq1K35+frRq1Yr58+2/p8TDR5WkqWgDAgL0/v37izsMIYQQ\nQohcqVu3Lr737rGkS1dqL1uaoVxrzZ/NAqj0wgs88vb/8eWXXzJ48GD+/PNP6tWrVwwRF48TJ07g\n7e1d3GEIUWLY+0wopQ5orQPS1y3UHjelVIRS6ohSKlwpJRmZEEIIIR5IZrOZSlpnOqOkUorKIX0p\nYzJmlEw9QYkQQuREUQyVfEpr7W8vaxRCCCGEuN8lJSURExNDpeTkTBM3gOqvvUalv/0NgEaNGqGU\nksRNCJFj8oybEEIIIUQ+3LhxA4CKKSlZJm4ASTduoJOTKVu2LHXr1pXETQiRY4WduGlgs1LqgFJq\nmL0KSqlhSqn9Sqn9165dK+RwhBBCCCEKlnXx7XrjxlGp298yrRfz/fecavkkiVFRAA/tzJIlaX4F\nIYpTbj8LhZ24tdJaNwWeAUYppdqkr6C1/kRrHaC1DqhWrVohhyOEEEIIUbCsiVvNJk0oVbNmpvVK\nubsDcC8yEjASt9OnTxMfH1/4QZYQpUuXxmw2S/ImHnpaa8xmM6VLl87xPoW6jpvW+qLlv1eVUuuB\n5sD2wjymEEIIIURRsiZupX7/naTAQJyqVLFbz9m6JECqxC0lJYXjx4/TrFmzogm2mLm7u3PhwgVk\nlJUQxhcZ7pYvdHKi0BI3pVQ5wEFrHWv5d0dgWmEdTwghhBCiOFgTt+RPPiWpe/dMEzfHKlVwqFCB\nexF/JW5gzCz5sCRupUqVok6dOsUdhhD3pcLscasBrFdKWY+zSmv9n0I8nhBCCCFEkbMmbq6OjllO\nTqKUwtnDw7YI9+OPP46LiwtHjx4tijCFEPe5QkvctNZngcaF1b4QQgghRElgNptxcnCgnINDtrNK\nVhk4EOXkCICjoyONGjV6KCcoEULkXqE+4yaEEEII8aAzm81ULlsWh7JlcXBxybJupee6pvndZDLx\n888/F2Z4QogHhKzjJoQQQgiRD2azmcoupbPtbQNIuXePhJMnSY6NBYzE7fLly7bhlkIIkRlJ3IQQ\nQggh8sFsNlO9fj08vvwi27p3T5zg3PPdubNvH5B2ghIhhMiKJG5CCCGEEPlgNpupWqMGzp6e2da1\nLQlgZ2ZJcf/66quvOHjwYHGHIR5wkrgJIYQQQuTD9evXKX/jBnE7dmZb19HVFcdKlWxrudWsWZMq\nVapI4nYfCw8PJzQ0lH79+pGcnFzc4YgHmCRuQgghhBB5pLXGbDbj8ucpYn/ZkqN9nD09bYmbUgpf\nX19ZEuA+NnnyZJycnDhx4gQrVqwo7nDEA0wSNyGEEEKIPLpz5w53796lUlJSjiYnAXD2/GstNzCG\nSx49ehStdSFFKQrLnj17+OGHH3j33XcJDAzk3XffJSEhobjDEg8oSdyEEPkSHx/P7NmzuX37dnGH\nIoQQRc62+HYO1nCzqtyvHzWnT7f9bjKZiI2NJdLSCyfuH5MmTaJq1aq8+uqrzJo1i6ioKJYuXVrc\nYYkHlCRuQoh8+fnnn5k4cSKLFi0q7lCEEKLI2RI3R8ccJ25lTCbKt25l+10mKLk/bd++3fb/wAoV\nKtCuXTs6dOjAzJkzuXXrVnGHJx5AkrgJIfIlwjLcZ8GCBcTHxxdvMEIIUcTykril3L1L7K+/2oZL\n+vr6ApK43U+01rzzzjvUrFmTESNG2La/9957mM1m5s2bV4zRiQeVJG5CiHyJjIxEKcXVq1f54ovs\n1zASQogHiTVx81uzmnItW+ZoH52YxIURI7m1+WcAKlasiIeHhyRu95EtW7awY8cO/u///o+yZcva\ntgcEBNCrVy/mzZvH1atXizFC8SCSxE0IkS8RERE0aNCAJ598kg8++IDExMTiDkkIIYqMNXGr4eWF\ng4tLjvZxLF8Ox6pVuRcZYdtmMpkkcbtPaK2ZNGkStWvXZujQoRnKp0+fTkJCAjNnziyG6MSDTBI3\nIUS+RERE4OnpyVtvvUVkZCRr1qwp7pCEEKLIWBO3pBVfk3LvXo73c/bwsC0JAMZwyT/++IN7uWhD\nFI8ff/yRPXv2MHnyZFzsJOsNGjRg8ODBLF261PY4gRAFQRI3IUS+WBO3Ll26YDKZmDVrFikpKcUd\nlhBCFAmz2Ux5Fxdiv/wS5eiY4/3SJ24mk4mkpCT++OOPwghTFJCUlBQmTZqEl5cXAwcOzLTeu+++\ni6OjI5MnTy7C6MSDThI3IUSexcbGcv36dTw9PVFKMXHiRI4fP84PP/xQ3KEJIUSRMJvNVC5TBseK\nFXOXuHl6knwtmuQ4YykVmVny/rB+/XrCw8OZMmUKpUqVyrRerVq1GDNmDP/85z/lmooCI4mbECLP\nrGsOeXp6AvDiiy/i5eXF+++/LwvJCiEeCmazmcrOzjmeUdKqUrdueP3wPQ5lSgPG8DonJye5yS/B\nkpOTmTx5Mg0bNqRv377Z1p8wYQIVK1bk7bffLoLoxMNAEjchRJ5Zx+57eHgA4OTkxBtvvMGePXsI\nCwsrvsCEEKKIXL9+HVcnp1wnbqVqVMelXj1bL52zszMNGzaUxK0EW7NmDcePH2fq1Kk45qB3tUqV\nKkyYMIEffviBXbt2FUGE4kEniZsQIs+siZu1xw0gNDSUGjVq8P777xdPUEIIUYTMZjMVc7GGm5XW\nmhtr1hCX6oZeZpYsuZKSkpgyZQp+fn707Nkzx/uNGTOGRx55hIkTJ8pIFJFvkrgJIfIsIiKC0qVL\nU6NGDdu20qVLM27cOH7++WcOHDhQjNEJIUThM5vNPPbcc7gvXpSr/ZRSRC9dxq3v/3om2GQycf78\neWJiYgo6TJFPK1as4PTp00yfPh0Hh4y3zzolhfhjxzJsL1euHJMnT2bnzp389NNPRRGqeIBJ4iaE\nyLPIyEg8PDxQSqXZPmLECCpVqsSsWbOKKTIhhCh8ycnJ3Lx5k6pVq6KcnXO9v70lAQCOHj1aYDGK\n/Lt37x7Tpk0jMDCQ5557zm6da4sXE9nnJRIvXcpQNmTIEOrWrctbb70lsy6LfJHETQiRZ9alANKr\nWLEio0aN4ttvv5WprYUQD6wbN26gtcZp925u796d6/3tLQkAkriVNJ9//jmRkZFMnz49wxeVKXfv\nEr1sGRU7dwaluLZ4SYb9S5UqxfTp0zl8+LCsdSryRRI3IUSeRURE2CYmSe/VV1/FxcWFOXPmFHFU\nQghRNKyLb7scO8a98+dzvb+zpwfJN26QfOsWYEz0VKFCBXnOrQSJj49nxowZtGrVio4dO2Yov7l2\nLdcWLCT5ZgyVQ0KI2biRu6dOZajXu3dv/P39mTRpkiyyLvJMEjchRJ7ExcURHR1tt8cNoHr16gwZ\nMoSvv/6aCxcuFG1wQghRBKyJm2seJicBYy03gHvnowDjuTdfX19J3EqQZcuWcenSJWbMmJGxty0+\nnuhPPqXsE09QrkVz3IYNxaFsWa4uWJihHQcHB95//33Onj3Lp59+WlThiweMJG5CiDxJv4abPa+/\n/jopKSnMmzeviKISQoiiY0vcHBxxrFQp1/uXCwqi/r69lPH1sW2zziwpMxAWv7i4OGbNmkX79u1p\n27ZthvIbq1aRHB1NtTFjAHCqXBm3IX/n3pkzJMfGZqjfqVMn2rZty/Tp04mLiyv0+MWDRxI3IUSe\n5CRx8/DwoG/fvnzyySdER0cXUWRCCFE08tvj5lC6NI4VKqTZZjKZuHHjBpfsTHIhitaSJUu4evUq\n06dPz1CWHHcb86efUa51a8o2bWLbXmXwYLx++D7DdQWjR/X999/nypUrLFyYsVdOiOxI4iaEyBN7\na7jZM2HCBO7cucPixYsLPyghhChC1sStSsWKeUrcAMxfLuf6iq9tv1snKJHhksUrJiaGOXPm0KVL\nF1q2bJmhPCUuljJNmlBtzCtptjs4O6NKlSLl9m3unj2XYb+WLVvSrVs35syZY3v/5EdiYiLr16/n\n9OnT+W5LlHySuAkh8iQiIgJnZ2cqJyZx9rnnSMhk9kgfHx+6devG4sWLibUzdEQIIe5XZrMZJycn\nmv1+kFI1a+apjds7thPzw19ruVmXBJDErXgtWLCAGzduMG3aNLvlpR55hNpL/0EZS6Kd3vlhL3Nx\n7Fh0cnKGspkzZxIbG5uvJXNu3brFvHnz8PLyokePHrz11lt5bkvcPyRxE0LkiXVGyYS9e7h76jTX\nFmXeo/bWW29x48YNPvnkkyKMUAghCpfZbKZKlSoZJq3IjVKWJQGsz7S5ublRs2ZNSdyKkdlsZv78\n+fTo0YOmTZtmKL+1eXOaZRzsqdz3Je7++Se3fvwxQ5mPjw8DBgxg8eLFuZ6869KlS0yYMIHHHnuM\n119/nbp162IymWTpnYeEJG5CiDyxruGWFG0M9ag194NM67Zo0YKnnnqK+fPnc/fu3aIKUQghCpXZ\nbMbV0Yn/TZ+R5zacO1pkVwAAIABJREFUPTxIuXWL5Js3bdtMJhM//vgjAwcO5J133uGTTz7hp59+\n4tixY9yyLB0gCs/cuXOJjY1l6tSpGcqSbtzg8sS3uLpgQZZtVHzmGVwaeXNt4SJS7Ez/P3XqVLTW\ndo9hz7Fjxxg0aBCenp7MnTuXTp06sXfvXsLCwmjfvj2nT5+WCW0eAk7FHYAQ4v4UGRmJv78/iRcv\n4ujmhkOZMiTHxeFYvrzd+m+99RYdO3ZkxYoVDB06tIijFUKIgnf9+nUq6hTu7N+f5zacLWth3ouI\nwKlyZQBGjhzJ7Nmz2bp1K5cuXSIlJSXNPpUqVeKxxx6jdu3adv9bq1YtnJ2d8/7CHmJXrlxh0aJF\n9OnTxzZsNTXzZ5+REh9PtVGjsmxHOThQffxrRA0Zws01a6kyoH+acg8PD0aOHMmiRYt47bXXaNiw\nYYY2tNZs27aNDz74gH//+9+UKVOGl19+mXHjxuHl5WWrV69ePeLj4/8/e/cdV1X9P3D8de5kT9lT\nBTHTi0DfMs3MlrlS07ScuUc5KldpfUsrK3Olln0dTU0zR9oyS23hzxI0N5oIMkThAoKsC9zz+4OR\nJijCHYzP8/HgkXLO+Xze1xj3fc7n836TmpqKn59fLV+50BCYPXGTJEkJHARSZFnuZe75BEEwv/z8\nfC5dukRwcDDFCQmo/fzIi44m6ZnJBH/2KTZt2lx3zYMPPkhUVBRvv/02o0aNQqlUWiFyQRAE09Hr\n9XjWsqJkBU1QMAp7e0qz/nni1qdPH/r06QNASUkJqampJCUlkZSUxPnz56/574EDB64rcqFQKLj7\n7rvp168f/fr1u+ZNvnBjb731FoWFhbzyyivXHStJTydr/QacevVCGxJy07HsO3XErkMH8g/FXpe4\nAbz44ousWbOGuXPn8uWXX/4zT0kJW7duZeHChRw8eBAPDw/mzZvHpEmTcHd3v26c0NBQAM6cOSMS\nt0bOEk/cpgInAScLzCUIggVUtAIICgoiYPZsjPn5IMtIKhXpK1YS8N7K666RJIkXXniBAQMGsGXL\nFgYOHGjpsAVBEExKr9fTCqluiVvzYFod/LPafXIqlYrAwEACAwOrHSM/P78ysUtKSuLMmTN8++23\nTJ8+nenTp6PT6ejXrx99+/YlPDy8TnvyGrOUlBTee+89RowYQatWra47nrF6NXJxMR5PT6rReJIk\n4b9iBQp7uyqPe3h4MH36dF555RX+/PNP2rRpw4cffsjixYs5d+4coaGhrFq1iuHDh2Nra1vtPFcn\nbvfdd1+NYhMaJrPucZMkyR/oCawx5zyCIFjW1a0AJIUCpYMDSkdH3J4awZU9eyg4drzK6/r27Uur\nVq1YsGCBWIsvCEKDp9frcZKNtWq+XUGSpDonUnZ2doSFhfHggw8ycuRI3njjDQ4fPszZs2dZtGgR\nzs7OzJs3j4iICFq2bMlzzz3Hr7/+SmkVFQ+bstdffx2j0cjLL79c5XFJocTl8QFobtIG52pKB3sk\nSaI4LY2SrKzrjj/33HM0a9aMIUOGEBgYyOTJk/H29mbr1q2cPHmS8ePH3zBpAwgICECj0XDmzJka\nxyU0TOYuTrIUmAkYb3aiIAgNR0Xi5u/sTOqcOZWJmtvw4SicnclYsaLK65RKJbNmzeLw4cPs2rXL\nUuEKgiCYXH5+PoWFhbg7OaH29a3TWPp1H5I62/Tl3Fu0aMFzzz3HL7/8QlpaGqtXr+a2225j5cqV\n3Hvvvfj4+DBmzBi++eYbCgsLTT5/Q5KQkMCaNWsYPXp0tf1JvWbPwvu//73lsUsvXya+R08y3nv/\numOOjo7MmzePv//+m86dO/Pbb78RHR1Nv379arylQKlU0qJFC5G4NQFmS9wkSeoFXJJlOeYm542T\nJOmgJEkH09PTzRWOIAgmlJiYiFqtxr2ggMtbtlKalQmA0sEB95FPceXnnzGcP1/ltUOHDsXf358F\nCxZYMmRBEASTqthXFvrcczSbML5OYxWnppL7ww9mXYng6elZmaRlZGSwadMmHnjgAb744gt69eqF\nh4cHgwYNYuPGjU2ycuUnn3xCSUkJc+bMue5Y8YULlQVoavN0VOnsjFPPnmRt3IihivL/EydOJDs7\nm+3bt9OpU6dbD56y5ZIicWv8zPnErRPwqCRJCcBG4H5Jkj7790myLP9PluU7ZFm+w8PDw4zhCIJg\nKgkJCQQGBlJ64QIAaj//ymOuQ4fRfPt2NNXsx9BoNDz//PP88ssvREdHWyReQRAEU6tI3KoqFnGr\nNEFBGPPzKc3IqPNYNeHo6MjAgQP5/PPPSU9P59tvv2Xw4MH8/PPPPPnkk7Rq1YpTp05ZJJb64uDB\ng4SFheHv73/dsfSVKzk/ajQlmZm1Hr/ZM08jKRRkLK+656mTU91KQYSGhnL27NnrKpAKjYvZEjdZ\nll+QZdlfluVg4AlgjyzLQ801nyAIllPRw604JQUAta9P5TGlgz02YWWbuuWSkiqvHzt2LO7u7uKp\nmyAIDVZF4lb8/ioKjh6r01ia4PKWADdp6mwOWq2W7t2788EHH5CSksLevXuRZZkHH3ywcll8UxAb\nG0tUVNR1nzckJnJ523ZcBg1C5eZW6/HVXl64DRvK5R07KTRDs+zQ0FAKCwtvuaG30LCIBtyCINyy\nisTNkJKC0qMZChub685Jmzef5KefqfJ6e3t7pkyZwtdff83Ro0fNHa4gCILJVSRutn//jVxcXKex\nKnu5WSFxu5pSqeS+++5j9+7d5Ofn88ADD5CammrVmCzh4sWLpKSkEBkZed2x9JUrkdRqmo2re/9R\n97FjUTo7k//Hn3Ue698qKkv+/fffJh9bqD8skrjJsrxP9HAThMahoKCAixcvlm3eLi5GGxRc5Xkq\nH2+u/PwzBYcPV3n8mWeewd7enjfffNN8wQqCIJhJReLmUsc+bgBqX1+0YWFQT/pb6nQ6vvvuOy5d\nusRDDz1EhoWWcFpLTExZOYZ/P3ErOnuWnJ1f4zp4MCoTbOdROjvTcvcPuA0z/QK0q1sCCI2XeOIm\nCMItOV9edCQ4OBjft94i8NNPqjzPbfBglK6upK+4vqcbgJubG+PHj2fjxo3Ex8ebLV5BEARzqEjc\nnJVKlK51S9wklYoWX23HpW9fU4RmEnfddRc7d+4kPj6ebt26cfnyZWuHZDaxsbEAREREXPN5Q+J5\n1L6+uI8ZbbK5lI6OABSdOWPSYjT+/v7Y2NiIxK2RE4mbIAi3pGLPQ1D50p7qKmwp7O1xHzOavN9+\nI//QoSrPee6551AqlbzzzjtmiVUQBMFc9Ho99lotGoUCZR0LS9RX9913H1u2bOHIkSP07NmTvLw8\na4dkFjExMYSGhl5XIMTx/q60/GFXnfa2VeXK778T3/tRruzdW6dxSnNzSV+5ktP3dCZj+XJatmwp\nErdGTiRugiDckorEzc/egfOjRpP/Z/Vr9V2ffBKluzv6D/5X5XE/Pz969+4teroJgtDgZGZm4urg\ngG1kJJIJljhmrl/P3926IdezqoA9evRgw4YN7N+/n379+lFUVGTtkEwuJibmumWSeQf+QC4tNcn/\n23+zv+suNMHBpC9ZglyLJugVeyrloiL0q9dgvHKF/D//FC0BmgCRuAmCcEsSEhJQqVR4GIrIi47G\nWGSo9lyFnR1+ixfjM39etedEREQQHx9Pbm6uOcIVBEEwC71ej0dgIMHrr+t0VCuSQkFx4nlKLl0y\nyXim9Pjjj7NmzRp2797NE088QUk1FYMbovT0dJKSkq5J3AqOHef8iBFkrd9gljkllQqPadMoOvM3\nl7/aUePrCo4cIXnqNBJHjgRA1awZIT/9iFOPHhgSEytbApTWIhkUGgaRuAmCcEsSExPLerilpQGg\n8fe74fn2d92JysOj2rX8Op0OgGPH6lZOWxAEwZL0er1JerhVqKwsmWDdypLVGTlyJMuWLWP79u2M\nHDmy0fQLq9jfdnVFyfR3l6F0dsb5sX5mm9ex28PYtGtH+orlGG/wFFM2Gsndt4/EYcNJGDiIvOho\n7CIiKp+6qdzd0QQFUZqeQQv/AAwGA0lJSWaLW7AukbgJgnBL/t3DTeXre9NrDMkpJA4ZWuWyyorE\nTbQFEAShIdHr9dgmJnJxgWkq49aXlgA3MmXKFF577TU+++wznn76aZMW17CWioqSFYlb/qFD5P3y\nK26jR6N0cDDbvJIk4fn8cxizL1N08mS1513+agfJEyZiSE7Gc9YsQvbuwfP555HU6spzXAY+TsjP\n+2jV9nZAVJZszFTWDkAQhIYlISGBRx55hOLkFFSenig0mpteo3J3w3D+POkrVhL08UfXHAsKCsLR\n0ZEjR46YKWJBEATT0+v1ONnaUpqdZZLxVD4+SBpNvU7cAF588UVyc3N56623cHR05K233qq2SFVD\nEBsbS8uWLXEpb+mQ/u67KN3dcRs6xOxz23foQMi+vdcUtynNySFr0ybUvr449+yJU7eHkVRKnB55\n5Jpk7WoqV1cAWpU/hTtz5gwPPfSQ2eMXLE8kboIg1FhhYSEXLlwgODgYpVqDbRXNSquisLWl2dgx\nXFzwJnl//IH9nXdWHpMkiXbt2onETRCEBqO0tJSsrCycNdo693CrICkUOPfpgyY4qE7jFKemcnHB\nApQuLtjffTd2HTqYtCqiJEksWLCA3NxcFi5ciJOTE3PnzjXZ+JYWExPDf/7zHwBKs7MpSbuI+9gx\nKOzsLDK/0skJWZbJi44m79ffyP7iC4z5+bgMHIhzz54o7Oxw7t37hmPIsox+zRocW7bE1tZWNOFu\nxETiJghCjV3dw81r+PBbutZl0CD0a9aSsXwF9v/q/abT6fj888+RZblB37kVBKFpyM7ORpZlnI2l\nKJydTTbujQo51YQhIYHEkaMwXr4MSiXZm79E7edHyE8/AlAYdxpNYAAKW9s6zSNJEsuXLyc3N5eX\nXnoJR0dHpk6dWqcxrUGv15OQkMCECRMAULq40GLnDrDw/r30ZcvQr/oAlEqcunfHfdRIbNq0qfH1\nkiSR9eln2HfsSEhIiFgq2YiJxE0QhBpLLF/CExwcfMvXKmxscB87lotvvEHegT+wv+ufp246nY5V\nq1aRlJREYGCgqcIVBEEwi8rm2wqlyZ64VZCLi0Glqt1NrPKecgErV6Bt1YrCEycoKY9VlmXOjx6N\n8fJlbCMisO94N/Z3343N7bcjqW797aBCoWDdunXk5eUxbdo0HB0dGTVq1K3HbEWHynuMRkVFYUhO\nRuXqisLe3uJxuA0ZgsLeHucePVD73bjgV3U0QUGVlSWPHz9u4giF+kIUJxEEocYqe7jZ2nK2R0+u\n/PLLLV3vMmggXi/NxVbX7prPVxQoEcslBUFoCCoSN+/wcLS1uJFVncvffMOp9hEUp6Te0nWG8+eR\nZRlNYCDNt23Fpk0bJJUKW50Ox65dy06SZXzffBPX4cMovZJL+tJlJAx6gksL3yk7XFpKUXz8LRUc\nUalUbNiwgW7dujFmzBg2bdp0S3FbW0VhkoiICDI/+YT43o9apeCKysODZmPH1jppA9AE/5O4xcfH\nN6qWDcI/xBM3QTCREydOsGzZMoqLi1EoFCiVyhv+t6rP9e3bl7Zt21r7pVQrISEBpVJJs+JiUuPj\nb/kurUKrxW3I9Ru+K17zkSNH6NWrl0liFQRBMJeKxO22ea9es2e3rlQeHlBaiiEh4aatVirkHfiD\n5IkTcR83jmYTxiMpqr4nLykUONzTCYd7OgFQkplJ/oEDldUsC48fJ2HgIFReXjj17Inn88/VqPm0\nVqtl69atPPLIIwwdOhR7e/sG83M8JiaG4OBg3N3dSTh+ApWnZ4Ndrq8JCqI0M5OWAQEUFxdz/vx5\nWrRoYe2wBBMTiZvQqMTHx/PRRx8xffp0nK6q0mRuv/32G71796a4uBhXV1eMRiOlpaVV/vffn7v6\n7t7+/fv55ptvLBb3rUpISCAgIAA57SJAre8O5uz6gdyffsS3vBqZs7MzwcHB4ombIAgNQkXiZso+\nbgCaoGAADIkJUJ5g3Ujuvn2kTJ2GOsAf53631nNM5eaGU/fulX9X+/vj/eqrXPn1FzLXrcOYm4v3\nvFdrlMjY2dnx9ddf88ADDzBgwAD27dtHhw4dbikea4iNjSUqKgrZaKTw5ElcbvHfsD5RBwWBSkXz\n8j2XZ86cEYlbIySWSgqNysyZM5k/fz4dO3bk3LlzFplz27ZtPPjgg3h6enLs2DGSkpJISUkhLS2N\nS5cuodfrycrKIicnhytXrpCfn09RURHFxcWViVxJSQlDhgzhr7/+skjMtXVNDzdJQuXjU6txSjLS\nydmxk/z9+ys/p9PpROImCEKDUJG4ZY94ipL0dJONq/L0QLKzq1FLgJxvvyX5mcloQ0II+vRT1F6e\ndZvbzQ3XQQMJWLEC9/Hjyd68mYwVK2t8vZOTE99//z3u7u68+OKLdYrFErKzszl79iyRkZEYEhKQ\n8/NvqSBIfePYpQutD8Vy+wMPAKKXW2MlEjeh0Th37hzbtm2jZ8+epKamcuedd/Lrr7+adc5Vq1Yx\nYMAA2rdvz++//16roh2SJKFUKmnfvj0pKSmVbwjqo8TExMrETeXlVaMeblVxefxxVN7epC9fUfnE\nUafTcfr0aQoLC00ZsiCYzJUrVzBauNqcUD9lZmaikCRs0tNNWsxCkiQ0gYE3TdyKL10i9YUXsQ0P\nJ/CjDyv7eJmKx7SpuI8dg8P9XW/pOnd3d5577jn27t3Ln3/+adKYTC02NhYoK0xSWF7Mw6a8gXVD\nJGk0SGo13t7e2Nvbi8StkRKJm9BoLF++HIVCwapVqzhw4ABubm488MADfPjhhyafS5ZlXnrpJSZO\nnEj37t356aefaNasWZ3GDA8PB+pvgY6ioiJSU1MJDg5G07w5jvffX+uxFBoNzcaPo+DQIfJ+jwbK\nErfS0lJOnjxpqpAFwWTOnDlD8+bN6dWrl9j0L6DX63G1s0Op1SLVsbT+v7k+MQinmzRPVnt6ErD6\nfwSuWY3S0dGk80NZAun5/PPY3l6WyBTdQl+wsWPH4uzszMKFC00elylVJG6RkZHYtmuH58yZaFu2\ntHJUdZO+fAWZa9eKlgCNWI0SN0mSWkqSpC3/832SJE2RJMm09W8FoQ5ycnJYs2YNAwcOxN/fn9DQ\nUP7v//6PLl26MGrUKKZPn05paalJ5iopKWHMmDG89tprjBo1iu3bt2NvgjuuFZUV6+tyyaSkJGRZ\nJigoiGbjx+H98kt1Gs+5f39UPj5kLF+OLMuisqRQb+n1enr06EFBQQHfffcds2bNsnZIgpXp9Xpc\nbGxQuriYvJiF6xNP4DJgwHWfl2WZjPff5/LOrwGwv/POOvdjq4nsrduI79OX3D17anS+k5MTEydO\nZMuWLfW6EXRMTAwBAQF4eHigCQ7GfdTIWrVFqE/yY2LI2b2b0NDQev1vL9ReTZ+4bQFKJUkKAf4H\nBAAbzBaVINyidevWkZuby7PPPlv5OVdXV7777jueeeYZFi1aRJ8+fcjJyanTPPn5+fTr149169Yx\nd+5c1qxZg8pEP+i9vLzw8vKqt4lbRSuAoKAgk5RLVmg0eL0wG7eRTwEQEhKCjY2NSNyEeqWoqIh+\n/fqRlJTEDz/8wJQpU1i8eDEfffSRtUMTrEiv1+OiVqM0YfPtCrLRSPGFCxgLCv75nCxzaeE7pC97\nl7wD/2fyOW/EqdvD2Nx+OynTniXvjz9qdM2UKVNQqVQsXrzYzNHVXkxMTGVhktw9eyjJyrJ2SHWm\nCQqiOKGsJcC5c+fE6oBGqKaJm1GW5RKgH7BcluUZQO2qEgiCiZWWlvLuu+9yzz33cMcdd1xzTKVS\nsXz5ct5//32+//77OhUtycjI4IEHHuCbb77hvffeY/78+Sa/0xoeHl5vE5fKHm42NpyOuoOcXT/U\neUynhx/G6ZFHKvf53X777fX29QtNjyzLjB07ll9//ZUPP/yQjh07smjRIh588EHGjx/P/quK6whN\ni16vp5mXFw4P1H7JeHUKDh3i7673k1++R0wuLSXtv6+QuW4droMH4zNvnsnnvBGFvT0BH6xCHRBA\n8qSnKTxx4qbX+Pj4MGzYMD788EMuXbpkgShvTU5ODmfOnCEqKori8+dJnvQ0V376ydph1ZkmKIjS\ny5dp6edHSUlJ5e9tofGoaeJWLEnSk8AI4Ovyz6nNE5Ig3JqvvvqKc+fOXfO07d8mTJjArl27al20\nJCEhgU6dOnHo0CG+/PJLJk6cWNewq6TT6Th+/Hi9vEuWmJiIUqnEq9SIMT8fpbNp2i0UxceT+eln\nlcslReIm1Bevv/46n376KfPmzePJJ58Eym4Gbdq0iYCAAPr160dycrKVoxSsQa/X4xMRgee0aSYf\nu6KvmiEhEdloJHXWbLK/+AL3cePwemlutX3azEnl6krg2jUonBw5P348pVfybnrN9OnTKSwsZOXK\nmlemtJRDhw4BZfvbCioKk9zecAuTVNAEl33tBJdv3xD73Bqfmn73jwTuBl6XZfmcJEnNgU/NF5Yg\n1NySJUto3rw5ffr0ueF5DzzwQK2Klvz111/cfffdXLp0id27d/PYY4+ZIuwqhYeHU1RUxOnTp802\nR20lJCTg5+eHfDENqH0Pt3/LP3CAi6+/TklaGjqdjkuXLnHx4kWTjC0ItfX555/z0ksvMWzYMObO\nnXvNMTc3N3bs2EF+fj59+/YlPz/fSlEK1qLX63FzczPL2Ep3dxT29hgSEpAUCtT+fng8+yyezz1r\n1ebQam9vAtesxfuFF1A63Hxfd+vWrenTpw8rVqwgL+/miZ4lxcTEABUVJU8gqdVoQ0KsHFXdaYKD\nUfn40Ly8v6BI3BqfGiVusiyfkGV5iizLn0uS5Ao4yrL8lpljE4SbOnjwIL/99htTpkxBqVTe9Pxb\nLVqyZ88e7r33XlQqFb/99hudO3c2ZfjXqc8FSq7p4aZQoPb2Nsm42rAwAArj4kSBEqFeiI6OZuTI\nkXTu3JnVq1dX+Wa5TZs2bNiwgdjYWEaPHm2SfZ9Cw1BQUEBBQQHyhs9Jf+89k48vSRIqT09y9+4F\nwHPaNJqNH2fyeWpD26I5Tj16AJD3xx833Rc2c+ZMMjMzWbdunSXCq7HY2Fj8/Pzw8vKi8MQJtGFh\nSOqGv5BM26IFoXv30LxXLxwdHUXi1gjVtKrkPkmSnCRJcgNigdWSJNXfHadCk7FkyRIcHR0ZNWpU\nja+padGSTZs28cgjj+Dv7090dDS3W2AZRevWrVGr1fU+cVN5e5nsl5w2NBSAorjTtGvXDhCJm2A9\n8fHx9OnTh4CAALZt24ZWq6323F69erFgwQI2btzIm2++acEoBWuq6LXpLMsotDZmmUMTHEzJhQsY\n6+nT3NLsbJInTiJpwgSMN3ia1rFjRzp16sSiRYvq1RaAmJgYIiMjkWWZwhMnGsUyyatJkkRoaKhI\n3Bqhmi6VdJZlOQd4DPhEluW7gAfNF5Yg3FxKSgpffPEFY8aMwcnp1vZb3axoybJly3jiiSfo0KED\nv/32GwEBAaYOv0oajYY2bdrUu8TFYDCQkpJCcHAwdv/5T5WlqmtL6eiI2s+Porg4PDw88PHxqXev\nX2gasrKy6NmzJ6WlpXzzzTe4ly83upGZM2cyePBg5syZw44dOywQpWBtFYmbi1KB0sU8nZG8X/kv\nwZs3o7CzM8v4daV0ccHnzQUUHj1G8pSpyAZDtefOnDmTxMRENm/ebMEIq5ebm0tcXBxRUVEABG/8\nHPdRI60clemkL19B8rPPisStkapp4qaSJMkHGMg/xUkEwapWrFiB0WhkypQptR7j30VLfvnlF2bO\nnMm0adPo168fu3btwtXV1YRR35xOp6t3T9ySk5ORZZng4GBcBgzAY9Ikk46vDQuj6EzZvj6dTsfR\no0dNOr4g3IzBYGDAgAGcPXuWbdu20apVqxpdJ0kSa9asISoqiiFDhnDs2DEzRypY2z+JmxKlq3kS\nN7WXF7bt2pplbFNxeughfObPI+/330mdPRu5mm0HvXr1onXr1rz99tv1YknxX3/9hSzLREVFIUkS\n2hYtKgvCNAalWZnk/fY7LVu2JCEhAcMNkmqh4alp4jYP2AWclWX5T0mSWgAijResJi8vjw8++IB+\n/foRHBxcp7GuLlrSpUsXFi5cyMSJE9m8eTO2Fmhu+m/h4eGkpqaSkZFh8bmrU1FSONDfn9LLl00+\nvvcr/yX4iy+A+l1ZU2icZFlm0qRJ7Nmzh9WrV9OlS5dbut7W1pbt27fj4ODAo48+WvnGXmicrknc\nzPTEraFw6d8fzxnTyfn2O3K++abKcxQKBTNmzODw4cP8+OOPFo7welcXJsnZvZvsLVutHJFpaYKC\nMObm0tLXF6PRKFoCNDI1LU6yWZZlnSzLE8v/Hi/Lcn/zhiYI1fvkk0/Iysq6YQuAW1FRtGTIkCEs\nXLiQlStX1qjYiTmEh4cD9WufV2UPN1tbTt/Vgezt2006vtrTE0V5kqzT6TAYDPWysqbQOL399tus\nXbuWOXPmMGLEiFqN4efnx/bt20lNTeXxxx+nuLjYxFEK9UVmZiYAgf37o/b1tXI01uc+ejQBH6zC\nqXfvas8ZMmQIPj4+vP322xaMrGoxMTF4e3vj4+ND9uefk7V+vbVDMil1kGgJ0JjVtDiJvyRJ2yRJ\nulT+sUWSJH9zBycIVTEajSxdupT//Oc/dOzY0WTjurq68tlnnzF9+nSrllyuj5UlExISUCgUeBmN\nAKh9TPtmxZiXx8WFC8nbv19UlhQs6ssvv2T27NkMGjSIeXVsbHzXXXexevVq9u7da7KbSkL9U/HE\n7bbXXjNZdd2GzqFLFyRJwnD+PNlbt113XKvVMm3aNH788UdiY2OtEOE/YmNjiYqKQpZlCo43vsIk\nFcs+A8vfx4jErXGp6VLJD4EdgG/5x87yzwmCxX333XecPn2aZ5+1bk8bU6tY++/p6Ym3t3e9Slwq\nerhR3l/NVD283XItAAAgAElEQVTcKkhaLVmfrefKL7/SunVrVCpVvXr9QuP0xx9/MGzYMDp06MCH\nH36IwgSNjYcNG8aMGTNYuXIlH3zwgQmiFOobvV6PnZ0dWiutyqjP9GvWcuHFF8nds/e6Y+PHj8fR\n0ZGFCxdaIbIyeXl5nDx5kqioKIpTUjBevozN7W2sFo85aPz9sY2MxMPLG2dnZ5G4NTKqGp7nIcvy\n1YnaR5IkTTNHQIJwM0uWLMHf358BJqxsaArFKSnoP/oY45UryAYDssGATbt2NBs3FoDzo0ZRkplV\ndqy4GNlgwPHhh/Ge8yIAp9q2w330aDyfe7beFShJTEz8p4ebUona28uk40sqFdqQEIri4tBoNLRu\n3VokboJZJSYm8uijj+Lt7c1XX31l0v2sCxYs4NixYzzzzDPcdttt3HvvvSYbW7A+vV6Pi1rN6U73\nEPbHAWuHU694zXmR/IMHSV++HIeu911zc9XZ2ZkJEyawaNEi3njjDZo3b27x+P766y+MRiORkZEU\nHjsO0OieuElqNcEbypZ/isqSjU9Nby/qJUkaKkmSsvxjKCB2XwsWd+TIEX766SeeeeYZ1PWoWaYs\ny6TMmEn2xo3k7d9PwbGjGBLOUar/p8CI0sUVta8v2tBQbNu3x/6eTti0DvtnkNJSrvz6K1C2z+34\n8eP1Zp9MQkICQUFBFKekovb2RlLV9J5PzWnDwig8/U9lSZG4CeZy+fJlevbsSWFhId988w2enp41\nuq7g6FFK0tNvep5SqeTzzz+nZcuW9O/fXxQHaGT0ej2uGi1KZ2drh1LvKLRa3EePoujkSfKio687\nPnXqVJRKJYsXW6cVcMUyzaioKAxJ50GtRlvDCrINkUjcGp+aJm6jKGsFkAZcAAYAT93oAkmSbCRJ\n+kOSpL8kSTouSdKrdYpUEIClS5diZ2fHuHHjqjxecPw4qbNmUVK+edxSJEnC980FBH32KaH79hKy\naxctdu7E64UXKs/xW7yIgPdW4r9sKX4L38b39ddx6f9PjR/3MaMx/P03cnEx4eHh9aZAR3FxMcnJ\nyQQHB+P0SDfcq/m3ryubsFaUZmRQkpGBTqcjKSmJrKwss8wlNF0lJSUMGjSIuLg4vvzyS9q0qdky\nqdx9+0gY9AQJQ4ZSUoOvS2dnZ3bs2EFJSQl9+vThypUrdQ1dqCfKnripmnxFyeo49e6NytOTzLVr\nrzvm5+fH0KFDWbt2rVUqJ8fExODp6Ymfnx/Nxo6l1f79KLRai8dhbhmrV3Omy32EhIRw/vx5ioqK\nrB2SYCI1rSqZKMvyo7Ise8iy7CnLcl/gZlUli4D7ZVkOB9oDj0iS1KGO8QpN2MWLF1m/fj1PPfVU\ntb3V0hct5vJXO0h++hmL9IuRDQayt2xBlmU0gYHYlleErA1tWGvk4mKK4s/VqwIlycnJGI1GgoOD\ncXzgAVwHDTTLPNqwMJTOzhRfSKt8/aKfm2BKsiwzefJkdu3axXvvvceDDz5Yo+sKT50i5dnn0DRv\nTklaGimTp9yw4XCFVq1asWnTJo4dO8bw4cMxlhf3ERo2vV6Pi8J8zbcbOoVGQ7OJE7C5vW2Vvd2m\nT59OQUEB7733nsVji4mJITIysnIJp9LB3uIxWILCxpaSixdp6eOD0WgkPj7e2iEJJlKX9U7PAUur\nOyiXvWuuuMWoLv+wfudFocF6//33MRgMTJ06tcrjRWfOkBcdjeNDD+E28imzFy6RjUZS584lZ8dO\n1AEB2N95Z53Gq1g2WRR3itbdu6NWqzly5AiDBw82Rbi1lpiYCECQvz+FcXFoAgMrS/ebkt2ddxL6\nf/uRJAmdW1lifuTIEbE/SLiG0Wjk8uXLXL58mezs7MqPq/9+o2OZmZnMmDGDsWPH1nhOtX8ATj17\n4Dl1KnkH/uDy1i0YDQaUGs1Nr3344YdZtGgRzz77LPPmzeOVV16pw6sX6gO9Xs8dNrYicbsB1yef\nrPZYmzZt6N27N8uXL2f69OnY2dlZJKaCggJOnDjBo48+SvGFC6TNm0+zSZPqfaPz2tAEBQIQVP67\n+syZM9x2223WDEkwkbokbjd9VyxJkhKIAUKAlbIsi128Qq0UFhby3nvv0atXL1pVsx4989PPkLRa\nvOe9iqr8iVz2tu04dL4HVbNmJo/p0qJF5OzYice0qXVO2gA0zZvj8/pr2N1xB2q1mjZt2tSLJ24V\n+3N8tVrO9emLz5sLcOnb1+TzSFdV9PP19cXNzU08cROuYTQaue222266hNjBwQEXFxecnZ1xcXHB\nx8eH1q1b4+LiQlhYGM8880yN5ivNzkbSaFA62OP72msAOPfqiVPPHkiShGw0XvN1W52pU6dy5MgR\nXn31Vdq0acPAgeZ5ai2Yn9FoJCsrC58eHXF8+CFrh1OvyUYjV375BW1IKBr/aysRz5w5k86dO/PR\nRx8xadIki8Rz5MgRSktLiYqKouDoUa7s3UuzCeMtMrelVbYEKH+r/vfff1szHMGE6pK43fTpmSzL\npUB7SZJcgG2SJLWVZfnY1edIkjQOGAcQGBhYh3CExmzDhg2kp6ffsDeSbbgOlbdXZdJWfOkSafPn\no/bxIeijD1F5eJgsHv1HH5G5dh2ugwfjPt40P/glleqaPW/h4eHs3r3bJGPXRUJCApIk4S3DRcpK\nDZuLfu1aCo+fwG/xIlGgRLjOqVOnOH36NCNHjuTee++tTMyuTtKcnJxQmaB4jrGwkKRJT4NCIujT\nT695gi9JEiVZWSQ//QxuT43A6eGHbziWJEm8//77nD59mhEjRtC8eXP+85//1DlGwfKys7MxGo34\nP3A/Tg+JxO1GSjIySJ48BdfHH8f75ZeuOdapUyc6dOjAokWLGDdunEm+Z28mJiYGoKyi5LZtoFSi\nDQu7yVUNk9rPD1QqHDIzcXV1FQVKGpEb3iqUJClXkqScKj5yKevnViOyLGcDe4FHqjj2P1mW75Bl\n+Q4PE76xFhoPWZZZunQpOp2Orl27VnueS//+eFx1507t6UngB6sovnCBxBFPUXzpkkniKb5wgfRF\ni3Hs1g2vOS+adElmcUoKl7/+BlmWCQ8P58KFC6TXoIqdOSUkJODr64tU/u9n6h5uVyvRZ5L744/I\nJSXodDqOHj0q9gUJlaLLq9TNnj2bp556in79+tG1a1ciIiJo0aIFbm5uJnkDKBuNpM6aTUFsLG5D\nh1b5Pa6ws4PSUlJnzqLg6LEqRrmWVqtl27ZteHt706dPH1JSUuocp2B5Fc23nRUKjDXY59iUqT09\ncX60N9lbt15XMEySJGbOnEl8fDxbt261SDyxsbG4u7sTGBhI4fETaENCUNjYWGRuS5NUKlyffBJt\nWCtRWbKRuWHiJsuyoyzLTlV8OMqyfMPfjpIkeZQ/aUOSJFvgIeCU6UIXmoqffvqJo0ePVttwWy4u\nJnvLFoz5+dcds/vPfwhc/T9K0tI4P3wExRfrnrypfXwI+vQTfN9+C8nEDVhzf/qJ1OnTKUlPryzQ\nYe2nTgkJCf/0cFOpUNWwdHpt2IS1QjYYMCQmotPpyMvL49y5c2abT2hYoqOjcXd3JzQ01KzzXHrr\nbXJ37cJz1iycHrnufiNQVvbcf+UKVG5uJE+aRHFa2k3H9fDwYOfOneTm5vLoo4+SX8XPLKF+q0jc\nit5eSO4P1l8RUd+5jxqFXFhI1voN1x179NFHadWqFW+//bZFionFxMQQFRUFQOHx49jUsKJsQ+U9\n50Wce/YUiVsjU9N2ALXhA+yVJOkI8CewW5blr804n9BILVmyBC8vL56sZrNz7u7dXJgzl7z/q3oL\npV1UFAFrVlOSnk7e/uv7ytRUwbHj5Hz3HQC27dubpYSwtnVrAIri4ggvr1Bp7cTt6ubbah8fkyer\nV6tYtlIUF1dvEleh/oiOjqZjx45mLTyU9fnnZH78Ma7DhuH21Igbnqtq1gz/Ve9jzM8naeIkjHl5\nNx2/bdu2bNy4kUOHDjFixAjxRLmBySx/cuSiVIriJDWgbdkSh/vvJ2v9+uturiqVSqZPn05MTAx7\n9+41axyFhYUcO3aMyMhIjHn5aIKCsI2MMOuc9UFpTg4hISEkJSVRWFho7XAEEzBb4ibL8hFZliNk\nWdbJstxWluV55ppLaLxOnTrFt99+y6RJk9BWkyhlfvIp6sBAHO7rUu04dpGRtNz1fWVRDfkW3ywZ\nEhNJGjeOS4sWYzRjPxSb8sSl8NQpPDw88Pb2tmqBkpKSEpKSkggKCsJ1yBC8Zs0063yaFi1ApaIw\n7jS33347kiSJxE0AICMjg7i4ODp27GjWeew7dcJtxAi8Zs+qUYJo06oVfkuXIBcW1qi/G0DPnj15\n5513+PLLL0WVyQamcqmkSNxqzH3MGCRbWwznz193bNiwYXh5efH222+bNYZjx45RUlJCVFQUSgd7\ngj/fgOvjj5t1TmvL2vQFp++8ixZeXsiyzNmzZ60dksktXbqUZcuWWTsMizLnEzdBqLNly5ah1WqZ\nMGFClccLjhyh4PDhsn0oN6nuVlFZMv/gQc491p/iCxdqFENJejrnx4wFo5GA//3PrM06lc7OqHx9\nKDoVB5QVKLFm4paSkkJpaSnBwcHYRUbgWMO+V7Wl0Ghw7NoVpbMzdnZ2hISEiMRNAGD//v0AZkvc\nDMkplf0YvV6YfUtPlh06d6bFjq/Q+PvXeMnXs88+y+jRo5k/fz4bNly/jEyonyoSN/HErebsIiMI\n2f0DNuUrSq5mY2PD1KlT2bVrl1l/11UUJqlYKtkUqH3LSlE0L2+30BiXSy5dupR3333X2mFYlEjc\nhHpLr9fz8ccfM3ToUDyr2VeV+cmnKBwccH7ssRqPK6nVFCcnkzh8BMWpqTc8t/RKHufHj6ckI4OA\nD1ahbdH8ll5DbdiEtaYwrmw7aHh4OCdOnKC4uNjs81alohVAoJ8fufv2UZKRYfY5/Ze/i/uokQCi\nsqRQKTo6GpVKZZZqjEXx50jo35/0JdW2Jr0pSa1GNhi4MGcuWV98cfPzJYn33nuPe++9l1GjRnHg\ngOiW0xDo9XoUkoSTaMB9SySVCqPBgCEp6bpjEyZMwMHBgYULF5pt/piYGFxdXQkODiZ56jRSZph3\n9Uh9oAmuaAlQprElbnq9nsTEROLj48nNzbV2OBYjEjeh3vrf//5HQUEB06ZNq/K4XFpKcdoFXPo/\nhtLBvsbj2oaHE7huLaXZ2WXJ2w2qu+V+/x1FcafxX7YU2/I9Z+bmNXsWgevWAWWJi8FgIC4uziJz\n/1tF4uavtSF5wkTyomu/R/BWyLKMLMvodDrOnj3LlStXLDKvUH9FR0cTGRmJrYmbv5dkZJA0bhwo\nlbgM6H/zC25EoaAkPZ20efPJK39CeCMajYYtW7bg5+dHnz59SKriTa1Qv+j1elydnfGeNQuFvWUa\nRzcWSePGkzxl6nVPpV1dXRk3bhwbN24kMTHRLHPHxsYSGRmJJEnkx8aYda92faH28QG1GtuMDJo1\na9boErdDhw5V/rkp9XwViZtQLxkMBlasWMFDDz1E27ZtqzxHUioJ/uwzPJ9//pbHt9XpypK3y5fL\nkrdqWgW4DBhAi6+243Dvvbc8R21pgoJQlz9hrChQYq3lkhW/RL3L2zaasxVAhYLDhzlzd0cKYmPR\n6XTIsszx48fNPq9QfxUXF/PHH3+YfJmkMT+fpAkTy56or3ofTR17iUoqFX6LF6FtXnZXvyg+/qbX\nNGvWjJ07d1JQUEDv3r3FTYp6Tq/X4+7pifuokWYtktMYOffuRdHJk1XeAJw2bRqSJLFkyRKTz2sw\nGDh69ChRUVEUX7xEaXpGo68oCWU/jzT+/hgSEgkJCWl0TbhjY2Mr/9yUVuaIxE2olzZv3kxqamq1\nDbeNRUWVhQAkjaZWc9i2a0fgunXY33MPKje3ys/Lskz6ypUUnjgBgDYkpFbj15axqIiM1avJ++MP\nwsLC0Gg0VvuhlJCQgI+PD4ryXnJqMzbfrqDy9qY0O5vCqypLNqW7acL1Dh8+TGFhockTt9RZsyk8\ncQK/xYuxLf9aqyuloyP+769CUqnKksIaFCxp06YNmzZt4ujRowwbNkxUmqzH9Ho9bg4ON1ypIVTN\nqXdvVB4eZK5de92xgIAABg8ezJo1ayord5rKsWPHMBgMZY23y28C2rS93aRz1FfuY0bj1KtXo2wJ\nEBsbS1BQEM7OziJxEwRrkmWZJUuWcNttt9GtW7cqz8nZuZO/7+tKUR17fNm2a4vPq68gqVQUX7yE\nITmZzHXryFi+orL0v6VJajUZ768id/ePqNVq2rRpY7Unbtf0cFOrUXl4mH1OlZcXCicniuJOExwc\njIODQ5P6oSxc7/fffwdMX5jEdfCT+Mx7Fcf7u5p0XI2/H/4rVmC8cgVDDZ66ATzyyCMsWbKE7du3\nM3fuXJPGI5iOXq/H/uJFUmfNtnYoDY5Co8FtxHDyovdTUMUqiunTp5OXl8f7779v0nkrnsxERUWV\nJW6SVGWhlMbIpX9/nLo9TGhoKMnJyY2qd2RsbCxRUVHodDqrFnGzNJG4CfXOb7/9RkxMDNOmTUNR\nRaVIWZbJ/PgTNMHBaIKDTTKnLMukTJ1KwuMDubTwHZx6dMejmqd95iYpFNiEhlJ06p8CJdZ84hYc\nHIwhORm1r89NK3eagiRJ2LRqRdHp0ygUCtq1aycStyYuOjqaoKAg/Ey0VLcovuyGj/3dd+MyYIBJ\nxvw3u8gIQn7cjd0tVLGbPHky48ePZ8GCBXz66admiUuoG71ej7MkoXQVhUlqw2XQIBT29uRWcWO0\nXbt29OjRg2XLlpm02ERMTAzOzs60bNkSbWgIrkOGoLBrGvsTjQYDhXGnaVm+DLyxtATIycnhzJkz\nREZGotPpOHr0aJNZqSASN6HeWbJkCe7u7gwbNqzK4/kHDlB05gxuw4ebbI+BJEl4//dlAOw73o3P\nm29aJEmpjrZ1awrj4ioLdFy4cIH08uWKllJaWkpSUhLBwcF4Tp2K75tvWmxubVgYRadPIxuNlZUl\na1pmXWhcZFnm999/N9nTtss7dhDfqxdXfv7ZJOPdiMLOruxG0yefoF+z5qbnS5LE8uXLuf/++xkz\nZgzRFioGJNScXq/H2WgUFSVrSenoSPNtW/GoZm/6f//7X9LT03njjTdMNmdMTAwRERFIkoTTI4/g\nPXeOycau7woOHuRcnz4Elv/6bCzLJQ8fPgxAZGQk4eHh5Obmmq2wTX0jEjehXklOTuarr75i3Lhx\n1VaPy/z4E5Rubjj16mnSuW1uu42Qn34kYM0aFLXcN2eyWFqHYczJoeTCBasVKElNTaWkpITg8ieb\ndhERFpvb4b4uuDwxCNlgQKfTkZWVRYrYU9IkJSUlkZqaesPEzVhYSPHFixSdOUN+TAy5e/eSs3t3\n5fGsjRtJfXEOSc88Q+qcudjdcQd2d99tifABKPjrCJfeWUTOrh9ueq5arWbz5s0EBgbSt2/fJvNm\npCEoLCwkPz8fp5JSkbjVgSYwEEmSkA2G647deeedDB06lMWLFxNfw2XGN1JcXMyRI0eIiorCWFhY\noz2njYkmqKwlQED5jc/GkrhVLH+teOIG1iviZmkicRPqlY8++gij0ciYMWOqPF586RJXfv4Z1ycG\nmaURtsLOzqpP2ipow1qDWo3hfFLlDyVLLxesaAUQ4OtL1sZNGM6ft9jcDp074zVjBgobG6u9fqF+\nqHjqVJG4pc1/jXMDBxH/aJ/Kcy68/DJ/d7mP+N6PkjhkKMkTJ3Fh7kuVxwsOHSIvOprixPM43n8/\n/iuWW+zmjCRJ+LzxOrbt25M6axYFNSi04+bmxtdff43BYKB3795NqkdRfVbZfFtCJG51lPPdd5zp\nej8lVRQiefPNN1GpVMyYMaPO85w4cYKioiKioqLIP3CAM3d3JP+qMvKNncrHB0mjwSY9HU9Pz0aV\nuPn6+uLl5UXbtm2RJKnJvEdQWTsAQahgNBpZt24d999/Py1atKjyHLWnJy2+/hqli7OFo7MsW107\nWsccRNJosAd8fHwsfjfpnx5uWtKmz8B34cI6l0u/FcaCAoxXrtCuXTugLHHr0aOHxeYX6ofo6Gjs\n7OzQ6XQUX7xE1vr1aFu3RhsSglxaiqRU4tK3L3ZRd6B0ckTh5ISy/KOC71tvWfEVgEKrxX/lChIG\nDiJp0iSab9qE2tf3hteEhYWxefNmunfvzuDBg9m+fTvKJtB7qj6rSNyaPzUShy5drBxNw6Zt1YpS\nvZ6s9RvwmPzMNcf8/PyYPXs2L7/8Mvv27eO+++6r9TwxMTFA2ZOZgp9+AklCG9qqLqE3KJJCgTow\nAENiYqOqLFnRlw/A3t6ekJAQ8cRNECxt3759nDt3jtGjR9/wPG2L5teU72+MJJXqmjYH1ihQUpG4\neZf/3RI93K52rt9jpL32Os7OzgQGBjaZu2nCtX7//XfuuusuVCoVBeX7GnxefQW/xYsqm+jad+yI\n66CBOHXvjkOnTti2a1e5RKi+ULm7E7DqfeQiA3l//FGjax566CHeffddvv76a1544QUzRyjcTEWZ\n+oCHH7J4m5jGRtuyJQ7330/W+vUYq6h0OH36dAIDA5k2bRqlpaW1nic2NhZHR0dCQ0MpPH4CTXAw\nSgf7uoTe4GiCgjEkJjSaxC0/P5+TJ09WJm5A5V74pkAkbrVQUFBAQkICBw4cYMeOHaxevZrXXnuN\nyZMnM3DgQLp06ULr1q1xdXUlMDCQuXPncq6OZeubgrVr1+Li4kK/fv2qPJ756WckT56CsajIwpFZ\nR/aWraTMmAmU/VA6ceIExcXFFps/ISEBb29vlBU93CycuGlDQyiKiwOa1g9l4R9Xrlzhr7/+olOn\nTkDZkkdJq8XmttusHFntaENDabnre1z69q3xNZMmTeLpp59m4cKFrKlBgRPBfCqeuNmlplaZbAi3\nxn3MaEqzs8nesvW6Y7a2tixcuJC//vqLtVX0faupisIkCoWCwuPHsbm9afRvu5r7qJF4zX6B0NBQ\nLly4wJUrV6wdUp0cOXIEo9F4TeIWHh7O2bNnG/xrqwmRuN1AWloakyZNon///txzzz2Ehobi5OSE\nnZ0dzZs3p0OHDvTp04dx48bx0ksv8dlnn1W+udTpdAwdOpS2bdvyxhtv0LJlSx5++GE2b96MoYoN\nuU1dVlYWW7ZsYciQIVUWJZFLS8n8+GNK9Hqz7G2rj4rTLpDz9dcY8/MJDw/HYDBwqrxFgCUkJiZW\n9nCTNBpUHs0sNjeAtlUYhsREjAUF6HQ6Tp06RVETSdqFMn/++SelpaWV+9tchw7Fb9nSa55GNzQq\nV1cArvz+O5cWL6nRNUuXLqVbt26MGzeODRs2mDM84QYqEreC/75CcWqqlaNp+OwiI7GNiCDzo4+Q\nq3iq9vjjj9O5c2fmzp3L5cuXb3n8kpIS/vrrLyIjIynR6ylJS2uSiZtdVBQO93QipPwpcUNvCXB1\nYZLsLVvI3rIFnU6HLMscO3bMytGZn9jjdgNGo5EvvvgCLy8vvLy8uOOOOyr//O8PT09PtNUkFElJ\nSaxbt461a9cycOBAPDw8GDFiBGPGjCEsLMzCr6p+2rBhA0VFRdUuk7yybx/Fycl4Tp9u4cisx+a2\n20CWKTp9urKy5JEjRyr3fJlbQkICd9xxB8Upqah9fS1etEUb1qrs9f99Fp1OR2lpKadOnar8t2jK\nMjMzefjhh5k/fz7du3e3djhmU1GYpEOHDkBZY2uNv2Wf/JpLXnQ0mWvXofLwwG3Y0Bueq1Kp2Lp1\nKz179mT48OFoNBoGmKn/nFC9yuIkSqUoTmIiXi++gKRWVy57vpokSSxdupQ77riD+fPn884779zS\n2KdOnaKgoICoqCgkrRaf11/DNiLy5hc2MsaCAvL//JPg8q/ZM2fONOjfo7GxsTRr1gx/f39OPfQw\nAOHfl/UFPHLkSOXvi0ZLluV68xEVFSU3ZiUlJfK3334r9+vXT1apVDIgd+7cWf7kk0/k/Px8a4dn\nVe3bt5cjIiKqPZ4wfIR8umtX2VhcbMGorMuQnCyfCGstZ36+UTYYDLJGo5FnzJhhkblLSkpktVot\nz5o1Sy7JyZGLEhMtMu/VihIS5BNhreWsL7+UT5w4IQPyJ598YvE46qPZs2fLgNy+fXvZaDRaOxyz\n6dGjh9ymTRtZlmW58Gy8nLlpk1ySk2PlqEzDWFIin5/0tHzitjZyzt69NbomNzdX7tSpk6xSqeTt\n27ebN0DhOs8//7xsq1bLJ8Jay0aDwdrhNBmjRo2S1Wq1fPr06Vu67qOPPpIB+cSJE2aKrGEwXLgg\nnwhrLSeuXSsD8htvvGHtkOokIiJCfvjhh2VZluUL81+TT4S1lkuuXJEdHR3lSZMmWTk60wEOylXk\nSmKppAUplUq6d+/O1q1bSUpK4s033+TChQsMHz4cX19fJk+e3CT38cTGxnL48OFqWwAUxsWRf+AA\nbkOGIKmazkNila8vCicnCuNOoVaruf322y329XHhwgWKi4sJDg5G6eho0WqSFdQBAXi9+AK2EZGE\nhoai1Wqb5PfHv6WlpfHuu+/i6+vL4cOH2X1Vv7LGxGg0sn///splklf2/ETay/9FtuA+T3OSlEr8\nFr6NTevWpD73PIU1WAbt4ODAt99+S1RUFI8//jjffvutBSIVKuj1elxt7VA4OCCp1dYOp9Eoycoi\nZcZM8qppOP/6669jY2PD89U07a5OTEwM9vb2tGrVirz/O0BRE601oPL0RLKxQXvxEt7e3g26QElR\nURHHjh37p6Jkh7sAMPz9d5PZCy8SNyvx9vZm1qxZnD59mj179tCjRw9Wr15NeHg4d911F2vWrGkS\nmyyhrCiJjY0NgwcPrvK4qlkzmk2ahEsTWxokSRL2HTuisCnb86fT6SxW7rai6W+gtzeXli6l8ORJ\ni8x7NUmhwG348LIqoiqVRRPX+mzBggUUFRWxa9cufHx8eMvKpe7N5dSpU2RlZVUWJsk/dBhNcHCj\nqiirsAy60PUAACAASURBVLPD//33UTg6cnnHzhpd4+TkxPfff0+7du147LHHGm3iXh/p9XpctBqx\nTNLEFPb25P/f/6GvpviOt7c3c+bMYefOnbf09R4bG0v79u1RKpWkvvgCGcuXmyrkBkVSKNAEBjaK\nlgDHjx+nuLiYyMhIcvfsJXffPgAKT8VVVt+Wy5uNN1YicbMySZLo2rUr69evJyUlhaVLl5KXl8fY\nsWPx8fFp9BvRCwoKWL9+Pf3798elml+GKnd3PKZMRuncuHu3VcV/6RK8ZpVVlgwPDyctLY1Lly6Z\nfd6KVgB+Wi36VR9QFB9v9jmrUpKRQe7evciy3GTupt1IUlISq1at4qmnnqJt27ZMmzaNPXv2cPDg\nQWuHZnJXN96WZZmCQ4ewjYiwclSmp/byJHjzF3jOqPn+XRcXF3744QfCwsLo06cPP//8sxkjFCro\n9Xo8W7bE57X51g6lUVFoNLiNGE5e9H4Kjh+v8pxp06bRokULnn32WUpKSm46ZmlpKYcOHSIqKoqS\nrCxKUi80ycIkFTRBQY0icbu6MEnOd9+R98uv+Lw2H/uOd6PT6cjJyam88dxYicStHnF3d2fq1Kkc\nPXqU/fv3ExERwbBhw9i69fpSuY3F1q1buXz5crVFSS7v/Jrcn36ycFT109UFSszt3z3cNP7+Zp+z\nKjnf7yJ54iRKLqWj0+kslrjWV/Pnl71hfPnllwGYMGECzs7OjfKpW3R0NO7u7oSGhlJ8/jylmZnY\nRrS3dlhmofb0RJIkDImJXFywANlovOk17u7u/PjjjzRv3pyePXvy+++/WyDSpk2v1+MREIB9Yy9+\nYAUugwahsLcns5rS/1qtlnfeeYfjx4/zwQcf3HS8uLg48vPziYqKovD4CYCmnbgFB2FITiakZUsu\nXrxITk6OtUOqldjYWJydnWnRogWFJ05g07YtLgMGoAkIQKfTAZZ5j2RNInGrhyRJokOHDnz77bd0\n6NCBJ554gu+//97aYZnF2rVradGiBV26dLnumGwwcPHtt8j6fKMVIqsfDElJnO32CDm7d1f+ULLE\ncsmEhAQ8PT1R68sazlq6h1sFm7BWABSdjqt8/UePHrVKLNb2999/s27dOsaNG0dg+Z5DJycnJk6c\nyJYtWxr0XdSqREdH07FjRyRJovBk2f4vu0b4xO1qV37/ncyPP+HSokU1Ot/Dw4OffvoJPz8/unfv\nzoEDB8wcYdOm1+txKiyksLy/pGA6SkdHXJ4YRM73uzAkJVV5Tt++fenatSsvv/xyZTP06lz9ZKaw\n/CleQ+3/aAoug56g+ZebCQ0NBcp+nzREsbGxREREIBcUYIiPx6ZNG4rT0rj8zTe0bdsWsMx7JGsS\niVs95uDgwDflX4z9+vVrdMthzp49y969exk1ahSKKkrN5+zaRWl6Bm4jhlshuvpB5eGBISmJopMn\nadasGb6+vhZL3IKDgylOTkaysUHp7m72OauibVWeuMXFNZm7adV59dVX0Wg0zJkz55rPT506FY1G\nc8ulsuuzjIwM4uLiKguTOD3SjdD90WhatrRyZObl+uSTuA5+ksy168j64osaXePt7c2ePXvw9PSk\nW7dulW9YBdMyGo1kZmaiPniQ7C82WzucRslt+Ajchg9HUUUvV/inPUB2djavvvrqDceKiYnB1taW\n1q1bU3j8OOqAgCa53aKCxt8Pm7AwWpW3oGqIN/qu7stXeCoOZBmbNm24sm8fqc9PxyYnh5YtWzb6\n9wgicavnKvYytGjRgl69ejWqO6rr1q1DoVDw1FNPXXdMlmUyP/4ETYsW2JcXJ2iKFDY2aJo3L/sh\nBZWbb82tovl2yaVLqP38kCTJ7HNWRensjMrbm8LTp/Hw8MDLy6vR/1CuyvHjx1m/fj2TJ0/G29v7\nmmPe3t6MGDGCjz/+mLS0NCtFaFr79+8HqEzcoKxxtaV7CVqaJEl4vfgi9p07kzZvfrVV9v7Nz8+P\nPXv24OLiwkMPPdQkv0fM7fLlyxiNRpxKSkVxEjNRe3niNXsWqmbNqj1Hp9MxduxYVq5cyckbFM2K\niYmhffv2qFT/z955hkdVdW34PjOZZNJ7AgkhoSXUREKVIgqIgEgTqVIEqSoIKEUBQQWkl48uICKC\nIMKLCijSRAnICwGkJAHpECDJpJfJtPP9SHnBFEIyLeHc18WFzt7n7DUQZs7ae63nsaHSjOn4Ly2Z\n2X1FRdTpSNq+g8ppaUD5PHGLjo5GrVbnGKo/uI9ga4uyXl3scpNRdUyM2Z6RLEnF/hasIHh5efHb\nb7/h6+tLx44dK8QxsE6nY9OmTXTs2BH/Qsrwss6eRX3xIh4D36zwD2tPQhkSQnauVHhoaCiXL19G\no9GYbD2DwcCtW7cIDAzEb9FCqu3YbrK1SoJdSDDZMVcAnlmBkk8++QQnJycmTZpU6PgHH3yARqNh\n2bJlZo7MNERERGBjY0OTJk3Qp6ZyZ8w7ZEaetXRYZkGwscF/yWLsqlVDtWFjiRXSqlatyuHDh3Fw\ncKBdu3ZcvnzZxJE+W+SV5knm26YnMzKy2HLhzz77DCcnJyZMmFDouMFg4OzZs/mS8TZeXtg/w/1t\nAMjlxM2bh/7o7/j5+ZXLE7dHy19dOncm5MxpbHx8sKv1eGXO1atXycjIsGSoJuXZfiIuR/j5+XHw\n4EGcnJx4+eWXiS6B54818+uvvxIbG1ukKIkhLQ1lvXq4dutm5sisD7vatdHGxqJPTSUsLAytVkuM\nCXssHjx4gEajISgoCEEQkDk6mmytkuAzYWL+bmle4loSVbGKQmRkJD/88AMTJkzAs4iS1Vq1avH6\n66+zevXqctt0/igRERGEh4djb29P1vm/ST98GFGTbemwzIbcyYmA9eupsuL/nuq0u3r16hw+fBiF\nQkG7du24cuWKCaN8tlCpVICUuJmDzNNnUH25nvRjxwod9/b2ZsaMGfzyyy+FehlevXqV9PT0HGGS\nmCuo1q9Hn5xs6rCtGkEQUAQGorl1s9wqS0ZGRuLg4EBwbguFoFAgCAJyJ0cUVauijs5J3ERR5FIR\n6qQVASlxK0cEBQVx6NAhZDIZ7du350Y5NpPcsGEDPj4+dOnSpdBxpzZtqPbDTmQODmaOzPpwaNwY\n19d7YlCrzSJQkqcoGeBbidjJk8k8c8Zka5UEZUgwdtWqATmJm1qtLpdlHqVl2rRpuLu7M378+GLn\nTZ48mZSUlBIprlkzWq2WU6dO5ZdJZp09CzIZ9g0aWDgy86Lw9UFmb48+PZ2HCxZgyC5Z4lqrVi0O\nHTqEwWCgbdu2XLt2zcSRPhtIiZv58BwyGNtq1Xjw+ewif+7fffddgoODmTBhAlqt9rGxM7nfWY0a\nNSLjj2PELSyZ2E9Fp7xbAuT58gl6PTfffJO0w4fzx5QhwWTnlkpCxRYokRK3ckZwcDC//fYbmZmZ\ntGvXjnv37lk6pKfm4cOH/PTTTwwaNAhbW9sC41kXLpb4IeVZwCG8IX6zZ6Pw8SEkJAQ7OzuzJG5V\nlEpS9vyIzsLy+wa1mqRt28j6++9nTqDk+PHj7N+/n8mTJ+P6hMb6xo0b07ZtW5YsWUJ2Of73c+7c\nOdRqdX7ilnk2ErvaIRY/+bUUWWfOkLhhI/enflTissk6depw8OBB1Go1bdu2rfC+RuYgL3Grt3wZ\n9mGhFo6mYiPY2uI77WO0t2+jKsIewNbWlkWLFhETE8PKlSsfG4uMjMTOzo46deqQdekSCn9/Kdkm\nJ3HT3r1HzerViY+PJyUlxdIhlZhHy1+zr1wl6/QZRM3/EnafDz6g6qavCAoKwsnJqUI/I0iJWzmk\nQYMG/PrrryQkJNC+fXvi4+MtHdJT8c0336DT6QotkzRkZ3P77bd5MLN4xahnDdFgQJ+cjI2NDfXq\n1TPph1LeQ15lch4SLWUFkIcgl/NgzlzSfvuNOnXqIJfLK/SHch6iKDJt2jR8fX159913S3TN5MmT\nuX//Plu2bDFxdKbjMeNtnQ71+b9xeK5i2wAUh1ObNnhPmEDqvn3EL1la4usaNGjAb7/9RmpqKi+9\n9BJ37941YZQVn7zEzb9VK+QuLhaOpuLj1LIlzh07olq7Ds3dwjeoX331VTp06MCsWbNISEjIf/3M\nmTOEhYWhUChQX7r8TPu3PYptYCDo9VT38ADKl7LkP//8Q3p6eo6i5OVce4d6dfPHbQMDUVSqhEwm\nq/C98FLiVk5p0qQJP//8M7du3aJDhw4kJSVZOqQSIYoiGzZsoEWLFtSuXbvAePrhwxhSUnB9rfAS\nymeVu2Pe4fbbw4GcckFTn7h5eXlhm+fhZiHz7TwEhQK7GjVQx8RgZ2dH7dq1K/SHch6HDx/m6NGj\nfPTRRziW8LTp5ZdfpmHDhixYsABDCUycrZHjx48TGBiIv78/+qQk7IKDcWjS2NJhWRTP4W/j1rs3\nqnXrSFhT8lLYhg0bcuDAAVQqFW3btn2mzevLikqlyuk3PHQYUa+3dDjPBL5TJuM9/n0Uvj6FjguC\nwJIlS0hLS2PGjBlAzslMZGQkjRo1Qp+aivb2bSlxy8XllQ4E/3WSOrlK3eUpcXvMl+/yZWTOzo89\nmxg0GlQbNpLx16n8Z6SSViiUN6TErRzzwgsvsHv3bi5fvkznzp1Jy5V5tWYiIiKIjo4uUpQkefdu\nbCpXxqFZMzNHZt3YBgWRffUqok5HWFgYDx8+5OHDhyZZK9/D7d49BHt75O7uJlnnaVA+Y8qSoijy\n8ccfExAQwMiRI0t8nSAITJo0iZiYGPbs2WPCCE2DKIocP348v0zSxtuboO+24dKpk4UjsyyCIFDp\nkxm4dH2NpG3b0D+FAE2TJk3Yv38/d+7coUePHqjVahNGWnFRqVS42dsT9+mn8IwrHZsLRaVKeA4Z\ngqBQFPkQXrduXUaPHs3atWu5cOEC169fJzU1lfDwcDQ3b4JCgbJu3UKvfdaQOToid3WlRq4fZnlL\n3Gxtbalbty7qy1Eo69R5TLhJsLEhYeVK0g4cIDQ0lJSUFO4UYeRe3pE+fco5r7zyCtu3b+e///0v\nXbt2JSsry9IhFcuGDRtwcnKid+/eBca0cXFk/Hkc125dEeRyC0RnvdjVDkHMzkZz65bJ+7zyEjdR\nq8WuenWLebg9il1wCLqHD9ElJREaGsqtW7fKVX3+0/Lzzz/z119/MX36dOzs7AqMGzIyuDNyFJmF\nmC336tWLatWqMW/evHK343jnzh1iY2PzEzexnJ4amgJBLsdvzhyCdmx/6lK9Fi1a8M033xAREcGw\nYcPK3c+FNaBSqXBXKpG7uVnFZ+KzRNqRI9wa8CaGIjYdZs6ciaurK+PHj39MmMQ+NJSQM6dxbC5t\nBOeR8OWXZO/bT0BAQLlL3EJDQ1EoFCj8/XB8xOMTQJDJsAsJQR0TnS9QUlE3eE2WuAmCECAIwhFB\nEC4LgnBJEIRxplrrWad79+58/fXX/P777/Tq1cukHl9lIS0tjR07dtCnTx+cnJwKjKcfPgIGA27d\nu1sgOutGmVtWqo6ONqlqUp6HW1BQEJVmTKfaDzuNvkZpyDPY1Ny4kZ+4XrhwwZIhmQyDwcD06dOp\nUaNGoeb0AInffEP6778XqpZmY2PDBx98wF9//cWxIuS0rZVH+9sArr/Wlbin6Ouq6Ag2Nih8fREN\nBh7MnkPyrt0lvrZXr17Mnj2brVu38vnnn5swyoqJSqXCTWGL3K14kSAJ4yNTKsmKjES1vnChEk9P\nT2bNmsWhQ4eYO3cutra21Mstj5TZ2iIoFOYM16pJ++VXUvfto1atWuVGnVkURSIjI/N9+aosWYLX\nqIKVKHmer/Xr1wcqrrKkKU/cdMBEURTrAs2BdwRBkM6rTcSAAQNYu3Yt+/btY8CAAVbpc7V9+3Yy\nMjKKLJN069Ob6j/9iG1QkHkDKwfYVa8OCgXZ0TF4enri7+9vkt2kuLg4srOzCbKyvwOHpk0I/u8p\nHMLDaZArC19Rd9N27tzJ+fPnmTVrFopCHjj0KSmoNmxE5uqKOiqq0Mb9t956C29vb+bNm2eOkI1G\nREQEDg4OhIaGoo2LQ3PtmqQGVwiiTofm2jXuT5tGyt69Jb5u6tSpDBo0iBkzZrB9+3YTRljxUKlU\nuNlIVgCWwPH553Hp3AnVunVoiih/GzVqFHXq1OH8+fM0aNAAW1tb7n046ak2N54FyqMlwK1bt0hK\nSiI8PLzYKgxl7doY0tKwT0ujevXqFfYZwWSJmyiK90VRjMz97zQgCrCsPF0FZ/jw4SxevJidO3cy\nbNgwqxMn2LBhA3Xr1qV58+aFjguCgF2tWmaOqnwg2NriO2kSTi+2AUwnUJJvBeDtza1Bg4s0QDU3\nMltb5M7OAFSpUgU3N7cK+aGs0+mYMWMGdevWpW/fvoXOUV++DKJI1XVrqXn4ELZVCn6s2tvbM27c\nOPbv31+udh0jIiJo1qwZNjY2ZJ07B4BDw+csHJX1IbO1pcrKFTiEhxM7aTJpBw+W6DpBEFi3bh2t\nW7dm8ODBnDx50sSRVhxUKhWugiAlbhbCZ/JksLHh4ew5hY4rFAqWLFkC5JRJ6tPSSP3pJ3RxpukF\nL6/YBgWijY2lRlAQKpWqXAjbPSpMEjdvHte7diu03NsuOATkcjS3b5tcxM2SmKXHTRCEIKAh8Jc5\n1nuWGT9+PJ9++imbN2/mnXfesZrk7fLly5w8eZJhw4YV2h8Qt2gRDz6fbYHIyg8eA9/EoVEjAMLC\nwoiKijJ6WeyjHm6Zp05hyLSensnkXbt5OG8+giAQGhpaIUslv/32W2JiYvjss8+QF9Hn6fj889Q6\n9jv2YWHYuLsjiiKGzMwC88aMGYOTkxPz5883ddhGIT09nXPnztEyV/Es6+w5BFtbSVigCGT29lRZ\nswZl/XrcHT+hxJssdnZ27Nq1C39/f7p16yZ5vJUQlUpFQOfOVPr4Y0uH8kyi8PXF+513SD96tNDe\nXsjp+V+/fj0TJ05EHRUFIH1+/AvbwEAwGKiWKzpWHk7dIiMjkcvlNGjQgKxLl5A5Ohb6HGkf2oCQ\ns5E4Pv88oaGhXL16lcxCvhvLOyZP3ARBcAJ+AN4XRbGAFJYgCCMEQTgtCMLp8uZHZq1MmzaNyZMn\ns2bNGnr37m0VgiUbNmxAoVAwcODAAmOG7GySdnyPvhzs/FgSQ1YWmWfOoE/PyCkl02qJjo426hp5\niVvl3P+3tBXAo2THRJP03XeIen1+4mYtGxPGQKPRMHPmTMLDw+nRo0ehc9RXriCKIjIHByCn9v/2\n0KHcnz6jwFx3d3dGjBjB9u3b8/9erZn//ve/6PX6/P62rLNnUTZogGBra+HIrBe5kyNVv/wS+3r1\nEHUll6j38vJi7969ZGdn06VLF1KfQqXyWSQ7O5uMjAy8AwJQVK785AskTILHoIEErF2DfcOifR2H\nDRtGcHAw6kuXASQrgH9hGxiI4OBAkGtOr2Z5Sdzq1auHna0t2ZejikzGBRsbZLnfF2FhYRgMBi5f\nvmzOUM2CSRM3QRAU5CRt34qiuKuwOaIorhNFsbEoio29vb1NGc4zgyAIzJ07l8WLF7Nr1y7atm1r\nUZNujUbD5s2b6dq1K4X9HacfOZLj3VbEw6pEDlnnznFrwJuo/z5vMoGSmzdv4unpiTI3iVb4+xn1\n/mXBLjgEMSsL7Z07hIaGkpaWVqFOCzZu3MjNmzf5/PPPC91N1D54wM1eb5CwYmX+a4IgoKxTl9T9\n+9Hcvl3gmvHjxyOTyVi0qKCIibWRJ0ySV0rt/MoruPXqZcmQygVyFxcCt36Lc9uXAEpsFVC7dm12\n7txJVFQUffv2tcq+aGshMTHH09Lu0qX8kxwJ8yMoFDi1aYMgCBgyMoqdq750CZtKlbDx9DRTdOUD\nZa7SZv3u3REEweoTN1EUOXPmTI69w61bGDIziz1FTd61m9gpU/NFzCpiuaQpVSUFYAMQJYriYlOt\nI1E4giAwfvx4du7cyblz53j++ee5cuWKRWL58ccfSUhIKN67zdcXx+cL732TyMEuX1kyhuDgYOzs\n7Ize55WnKKm5dw+Zg4NV9XPkKUuqY66Y3BLB3GRlZfHZZ5/RokULOnbsWOichNVrEEWxwAaHx+DB\nCHI5qo0bC1xTpUoVBgwYwIYNGyy6eVMSIiIiqFu3Lu65JTyebw3BrYekMFsShFxfsdQDB7j2cgey\nLlws0XXt27dn1apV7N+/n4kTJ5oyxHKNSqUCQP77MbIuluzPVsJ0pB06xNWX2qIpZuNO5uyEY6uW\nZoyqfCDIZDkbfkolVatWtfrE7f79+8TFxeUbbwMo69Ypcr42NpaUH38kqHJlHB0dK8wzwqOY8sSt\nJTAQaCsIwrncX51NuJ5VYcjK4u6497k54E2S//MfDBaS6O/ZsydHjhwhJSWF559/nj///NPsMWzY\nsIEqVarQoUOHAmP/827rJnm3PQEbd3dsfHzIjonGxsaGevXqmeTELTAwELmzCw5Nm1qVX5FdzRog\nk5EdE0O9evUQBKHCfCivWbOG2NhYZs+eXeifuebOHZJ/+AH3N3oVECNR+Prg2r07Kbt2oyskOZs0\naRJZWVmsWLHCZPGXFYPBwIkTJ/LLJDV37qCTSqefGvvQUGTOztx++23UMTElumbEiBGMHz+e5cuX\ns2rVKhNHWD7JS9zc5JKqpDWgrN8AdDoezJ5dpCdh5U8+wU+yvSiUhNWreTBnTrlQlnxUmMTW3x+3\nPn2wyzUQLwy7kGAwGNBev06DBg2kE7enQRTFP0VRFERRDBVF8bncX/tMtZ41IOp0ZF28BICgVGLI\nykSfmMj9KVP5p1074letQpdbcmFOmjdvzsmTJ/Hy8qJ9+/ZmlYG+c+cOv/76K0OGDClcbEEUce/b\nF1dpZ71E2NUOQR2d80AWFhZm1A8lURTzzbe933uXgDWrjXZvYyBTKlHWq4eo1eDk5ESNGjUqROKW\nnp7O3Llzad++PS+++GKhcxJWrECQy/EcOarQcc+3hyHqdCR9V/Dfdp06dejatSsrVqwg4wnlRZYi\nJiaGpKSkfGGSh/PmcbMIVU2JolFUqkTVTV8hs7fn9ltDyb52rUTXLViwgC5dujB27Fh+/fVXE0dZ\n/ng0cbOREjeLo/D1weu998g49gfphw8XGJcM5osn+/oN0g4ezE/crPnPKzIyEkEQCAsLw/6556g8\na2axfc/KvMqc6GhCQ0P5+++/rfr9lQazqEpWdPTp6ag2beJah1e4NWAAuqQkBEGg6rp1VN+3l4D1\n61HWqUPC8v9DfSknsSvOi8IU1KhRg4iICJo0aULfvn2ZN2+eWX6YN23ahCiKvPXWW4WOK3x9qTR9\nGnbVqpk8loqAMiSE7OvXETUawsLCiIuL4+FD48gdx8XFoVarrc7D7VGqfb8Dn9ySrrwP5fLOsmXL\niI+PL9IUOUeUJhL3AQNQ+PoUOsc2MJCAtWvxHP52oeOTJ08mMTGR9evXGy1uY3L8+HEgx3hbFEWy\nzp7D4bmiBQgkisa2ShWqfrUR5DJuD3mrRCeXcrmcrVu3Uq9ePXr37s2l3O8piRykEzfrw+PNAdjV\nqsnD2XMw/EuALembLVx7pSP69HQLRWfd2AYGorv/gJrVqpGcnJzfw2mNREZGEhISgqOjI9nXbzzx\n2VlRpQoyBweyY64QFhZGUlIS9+4V9Dotz0iJWxnQxcfzcN58/nnxJeK+mIeNX2X8Fy1E7uKSP0cQ\nBJxatcxJ4vbvw7FVKyBH/v720KGkHT1qtiTO09OT3377jT59+jBlyhTGjBlj0oZ0g8HAxo0badu2\nLdWrVy8wrrl1i8zTpyvcbogpcXv9dQK/3gQymdGbb/OEPgK8vfmnwyuk/vKLUe5rKiqC3G9SUhIL\nFizgtddeo1mzZoXOkdnbU33fXrzfGVPsvZxat0KmVBY61qJFC1q1asXixYvRarVljtvYRERE4Onp\nSa1atdDeuYNepSpWOU6ieOyqVSNw40bcBw0scaLh7OzMzz//jIODA126dCEuLs7EUZYfpMTN+hAU\nCnynT0f74AEZJx73I1RfuoghMxO5k5OForNubAMDQRSpVg6UJSMjIwkPD0d7L5brnTuTvOP7YucL\nMhkOzZsjKBQVVqBEStxKgSE7GwB9WhqJW7bg9MILBH2/g6AtW3Bu377IXi27atXy+1cUfn5kX7vO\n3VGjud75VRK3bi3Ui8nYKJVKtm7dypQpU1izZg1du3YlLS3NJGsdOXKEmzdv8vbbhZ8CJG7+httD\nh2Ew0foVEdugIBzCwxFsbIwu0JEnGe9na4v29m2QWd/Hgzoqihu93iDrwgVCQ0MRRbFcnw4sWrSI\nlJQUPv3000LHtXFxGLKzkdnaInN0fOL90o4e5WafvhjU6gJjkydP5vbt23z33XdljtvYRERE0KJF\nCwRBIOvsWQApcSsjdrVq4TV8OIIgkH31KtoSnMwHBASwZ88eHjx4QPfu3VEX8nP0LKJSqVAqlYRG\nHEcuqRRaDY5Nm1LjwK/5iqp5ZF26JNkAFINtUCAAVW1sAOtN3OLj47lz506uMEnO93xxwiR5BKxa\nie/kSTRo0ACoOCJmeVjfk5mVIur1pB08yM033yT2w0kA2FWvTq3fj+K/eBH2uT8gJcVjwABqHvwN\nv4ULkTk78/DTz3gw2zwG1DKZjLlz57J27VoOHDhAmzZtiI2NNfo669evx93dvVBPKoNGQ+rPP+Pc\nvv1jJ5QSTybt4EHSjx3D09MTf39/o+0m5Xu45W4u2FqRh1seMmcX1Bcvor4cVe4/lOPj41m6dCm9\ne/fmueeeK3TO/enTudmnb4lPpWUODmSdP0/K7t0Fxjp37ky9evWYP3++VZ1yJyQkEBMTky9Mkhl5\nFpmTU44YjUSZEbVa7owew+23hpaobLJp06Z88803nDhxgmHDhlnVz4qlUKlUeHp6YuPlla/gKWEd\n5H1PZef2ahkyM9FcvyElbsVgGxiIbbVqBPr4IpPJrDZxO5u7iZevKCmXYxccXOLrXV1dCQoKkk7c\neYG32QAAIABJREFUnjUMmZkkfvst1zp35u6776GLvY9Dkyb54zYeHqW+t6BQ4NrlVYJ2bCdw61Y8\nhw4FcuTO744fT2buD62pGDFiBD/99BNXr16lefPmXLhwwWj3TkxMZPfu3QwYMABlIeVb6UeOok9J\nkURJSkHCmrWovvoKMK5Ayc2bN3F3d8chOQUAhb//E64wPwp/P2SOjmTHxFC9enUcHByM+nNrTubM\nmUNWVhazZs0qdDwz8iwZvx/DpXPnEqt7OjRpgn1YGKoNGxH/VQYtk8mYNGkSFy9eZN8+69GJOnky\np8wpL3HzfGsIfgsXSCqzRkJQKPCbOwft3bvcfe+9Eikc9+rVi9mzZ7N161Y+++wzM0Rp3ahUKtxs\nFCR+/bWlQ5EohPQ/j3P9ta6kHzqEOjoaDAYpcSsGuYsLNfbvw6vLqwQGBlpt4panKNmwYUPUly9j\nV6NGke0Aj5J94wbXXu1C+u+/V5he+EeRErcnoNqwkYeffY7czQ3/pUuoceBXPAa+adQ1BEHAIbxh\nvsSp5sZ1Mv48zq1+/YmdPKXQsidj0alTJ/744w/0ej2tWrXi4MGDRrnvt99+S3Z2dpHebSm7d2Pj\n44Nj7sOaRMmxqx1CdnQMoigSFhZGVFQUGiPYTeQpSmrv3kXm5ITMCk9CBUHALiQE9ZUYZDIZDRo0\nKJcfytHR0axYsYKhQ4dSO9ef71FEUSR+6VLkXl54vDmgxPcVBAHPkSPQ3r1L6v6CPYr9+vUjICCA\nefPmlSl+YxIREYGNjQ1NcjfEbIOCcC5CXVOidDg0aYLfF3PJOn2GB9Onl+gUberUqQwaNIhPPvnE\nKstrzYlKpcJFk03Kjz9ZOhSJQnBs3gy74GAezJmDIJfj+npP7BvUt3RY5QJrtgSIjIykevXquLm5\noY6KKtZ4+1FsvL3RXLuGOiqKsLAwYmJiKlTZt5S4PQH3fn0J3LqVatu349KxI0JuTbApcenYkVpH\nj+A5ehQpP/7IzX790dy9a7L1nnvuOU6ePElgYCCdOnXiq9zTnNIiiiIbNmwgPDy80BIwg1pN1qWL\nuHbrKu2qlwJlSG30SUno4uIJDQ1Fp9MRFRVV5vvmmW/bBgXh0qmTVXm4PYpdSDDZMVcQRbFcyv2K\nosj48eNxdHRkdhHl0ZknTpB56hReI0Ygc3B4qvs7vfgitjVroPryywJ/LgqFgokTJ/LHH39w4sSJ\nUr8HY3L8+HHCw8Oxt7dHfeUKybv/Y5Z+32cNl86d8R43lpQ9P5Kya9cT5wuCwLp162jVqhVDhgyx\nmp8XS6BSqXCVScIk1opgY0OlGdPRxd4n7cgR/GbPxsbb29JhWTUJX37JzT59rdoSIE+YRDQYqDxr\nFu59+5ToOrmTE4oqVVDHxBAaGorBYCjXvfD/xvRZSDnHxssLGy8vs68rc3TEZ9w4HJ57jnsfTiJ5\n50583n/fZOsFBATwxx9/8MYbbzB06FBu3LjBuHHj0Gq1aDQatFptiX/FxsZy/vx5Vq5cWfh7Uyqp\ndeiQxUzJyzvK2jk+Jdkx0YSFhQE5fV55/10a8jzcOnToYPQTZWPj2LQp+qRkxMxMQkND+fLLL7l/\n/z5+fn6WDq1E7Nu3j19++YXFixfj41O4vH/qL79iU7kybiX8onoUQSbDd/JkxOxsEEX4VwL+9ttv\n8+mnnzJv3jz+85//lOo9GAutVsupU6cYNSrHny7t1wMkrF6N88vtLRpXRcVz1CjkHp64dOlSovl2\ndnbs3r2bZs2a0atXL86dO4f3M/hAnJiYSJjCFnmuCp+E9eHQuDGu3bqiWrMW167dsKsuWQwVh6jV\nknX+PDXfHkZqairx8fFFfh9ZguTkZK5du8awYcMQZDKc27Z9quvzKpNCR48Gcp6RGjVqZIpQzY6U\nuFk5Tm3aUH33Lmx8fQHQ3r+Pja+vSRqkXV1d2bt3L6NGjeKzzz4rU2+Di4sL/fv3L/C6KIogigi2\ntsiLMVGUKBq7XIPJ7CtXqNWiBXZ2dpw/f56BAweW+p4JCQlkZmYSGBiIKIpWe9oG4NKpEy6dOgHk\nJ6s///wzI0aMsGRYJUKj0TB+/HhCQkJ45513ipxXadZMdA8fIivlvxGn1q2LHHN0dGTMmDHMnj2b\n69evF2rVYS7OnTuHWq3O72/LOnsWu+BgScbbRAiCgHuf3gDoU1LQxsairFO8SpuXlxc7d+7k+eef\nZ+DAgezbtw/ZMyTQIYoiiYmJuHp7SyduVo7PBx+QsudH0o8elRK3J2AbmKMsGeTyP0sAa0rczp07\nB+QIk2SdO4dBo8GxadMSX68MDiH98BFq+vnh4OBQLlsqikJK3MoBeSIR+tRUbvbth7J+ffy+mIvc\n2dn4aykUrF+/nk6dOhEbG4tCoSjVLx8fH9wK+ZJTX77MvbHj8F+6VKpBLyVyFxdqHj6ETeXKCIJA\n/fr1yyxQkqcoWdXbm5jnGlLpk09w61lQDdSaELVaWrRowQsvvMDEiRNp27YtNWvWtHRYxbJ8+XKu\nXr3Kvn37sC0kKRMNBvQpKdi4u6OoVKlMa+nTM0jcuAHHVq1xCH9cWn/kyJH5yrKW7HeLiIgAco23\n9Xqyzp/HpetrFovnWSJ2ylSyzp0jaMd2bAMCip3bsGFDli5dyujRo5k3bx5Tp041U5SWJzU1FZ1O\nh6soSomblWPj7U3I2UiEEghYPOvYBgYBEKjISQP++ecfWrZsacGIHudRYZKETz9F8881avyyv8TX\nOzRtimtsLEJ2tlGekawJKXErR8icnfEcNoyH8+dz843eVFnxf9iZ4EFVEAR69epl9PsCpOz+D7r4\neGyrFv+gIFE8ikfKAsPCwvj555/LdL+8xM3fTomYnY3c1fqESR7lZp++KPz98V+8iC1bthAWFka/\nfv04fvx4oQmRNfDgwQM+/fRTXn31VTrlnhj+m7RffiF22nSCtm1FmXuyWloEuYyk77aTdekSVdeu\nfWysSpUqdOvWjQ0bNjBr1qxClV/NQUREBIGBgfj7+6OOicGQkYGD5N9mFnwnT+Jmn77cGTmKoG1b\nn1gGOHLkSI4ePcq0adNo1aoVrYs51a1I5Jlv15kzB683rbuMXAJk9vaWDqFcYBtYFYBKajVyudzq\nBEoiIyOpUqUKPj4+/HM5CvsiLHOKwrF5MxybNwMgNDSU3bt3W301UUl5duodKgCCIOAxaCCBm75C\nn57Ojd59SP2loHKctSLmerc5tWsr9QqUkaxz54j96GMMWVmEhoYSFxfHgwcPSn2/W7duAVA59/+t\n0QrgUeSenqivxAA5/ZkbN27k9OnTfPzxxxaOrGg+/vhj1Go1ixcvLnRc1OmIX/5/2Pr7GWVDRmZv\nj8eggWT8fgx1TEyB8TFjxqBSqfj+++/LvFZpyTPehhxzdZCMt82FbVAQ/v+3HM2dO9wd9z6iVlvs\n/DyxkurVq9OvXz/i4+PNFKllyUvcvLy8EBQKC0cjIWEc5M7OOHfogIO/P9WqVbPKxC08PBxdUlJO\nSXe9kilKPoooiujT0ggLC0OlUpnEr9gSSIlbOcShcWOq/fADyuBgknf+YJVqQIWR9vvv6JOTcSvE\nkFvi6dA+jCNl1y6y//knv8+rLKUAN2/exNXVFceUXA83Kxf6sAsJRnPjJobsbAC6d+/OmDFjWLhw\nIb9Y4WbG6dOn+eqrrxg3bhzBRRiIpuz5Ec3Nm3iNHWs0tVX3fv2QOTigWvdlgbG2bdsSEhLC6tWr\njbLW03L79m3u3r2bn7i5de9OzWO/o7BC4/eKimPTplT+7FMyT54kfvnyJ853cXFhx44dxMfHM2jQ\nIAwGgxmitCx5iZth506yr9+wcDQSEsajyvJluHbtSs2aNa0qccvIyCA6Oprw8HCyczf0SmoF8Ch3\nhg3j7ph3CA0NBagwfW5S4lZOUfj6ELj5a/yXLEYQBLRxcegSEy0dVrGk7P4Pcm8vybvNCOQpS6qj\no8v0oaRWqzlx4gQnT57M8XC7dw+ZiwtyK/RwexRlSAjo9WiuXct/beHChTRo0IDBgweX6fTR2Iii\nyNixY/H29mbatGmFz9FoSFi5EmX9+ji3N56iotzVFbd+fUndvx/NnTuPjQmCwKhRozhx4gRnz541\n2pol5dH+tjwUPj4VopSlPOHWvTuVZs3CvYTiRnn9br/88gvz5883cXSWJy9xU/zxB4aMdAtHIyFh\nXES93uosAc6fP48oioSHh6O+fBngiSJKhaEIqIr6yhUaNGgASImbhBUg2NrmC5TETp7MjZ6vk2XF\nP5huvd/A94MPzOKFV9FRBAQgc3AgOzoGDw8PqlSp8sQTN4PBQExMDN988w3vvvsuTZo0wcXFhRYt\nWnDmzBleeOEF7MPDcR9QUA3U2rALzk1cY67kv2Zvb893331HWlqaVZ0GbNu2jRMnTjB37lxciygR\nzvjrL7SxsXiPG2f0xMVj8GCc27dH1OkKjA0ePBh7e3uLnLpFRETg4OCQ40WYkMDdsePIunDR7HFI\ngHuf3ih8fBB1OrJK4Hc0atQoevfuzbRp0/jzzz/NEKHlyEvc3CQfN4kKRtK2bUQ/15CagYGkp6dz\n8uRJS4cE/E+YJDw8HPc33yToh52l+rdnFxKMISUFJ7WaqlWrVhiBEilxqyD4fvghglzOrQFvkrRj\nh6XDKRTnF1/EtVs3S4dRIRBkMuyCg1HHRAM5AiX/3k2Kj49n7969zJgxg1deeQVPT09q167NoEGD\n+Prrr3FycmLChAns2rWLe/fusXz5cly7vIrPuHGWeEtPhW1gVTwGDyog+Vy3bl2WLl3Kb7/9xqJF\niywU3f/IyMhg0qRJNGrUiCFDhhQ5z6l1a6rv24tjK+Oreil8fKiyfBl21QrKY7u7u9O/f3++/fZb\nkpOTjb52cURERNCsWTNsbGzIPHuWtAMHEHXF91lJmJb4Zcu51X/AEzcABUHgyy+/JCgoiL59+5KQ\nkGCmCM2PSqVCEARc5FLiJlGxkLu7g1bLy6Gh+Pj40KpVKyZMmEBaWppF44qMjMTHxwc/Pz9kSiX2\n9eqV6j7K2rUB8o24pRM3CatCWbcuQTu/x6FZMx7M+ITYadPy+3+sgcTN36C5e9fSYVQolHXr5gsK\nhIaGEhUVxdKlS+nfvz81atTAx8eHLl26MHv2bB48eMAbb7zB+vXruXDhAsnJyRw5coQvvviCHj16\n4OfnhyiK6BITraZcojgEuRzfqVOxL8R0fPjw4fTq1YuPPvqIU6dOWSC6//HFF1/kJ8VFeV/p0zMA\nsKte3aRlgpqbN0k9cKDA62PGjCEzM5PNmzebbO1/k5GRwblz5/Llp7POnkNQKFCW8gtawjh4DBmM\njZcXd8a8g/YJjfwuLi58//33Fb7fTaVS4apUIlcokEn+ghIViDwvN3+9gejoaEaMGMHSpUupW7du\nvgqjJcgTJjFkZPJw/gLUV648+aJCsKtVC4Ds6BjCwsKIjo5GrVYbM1SLICVuFQgbd3cC1q7Bc9RI\nsk6fQdRoLB0SkKMW93DOHNKPHbN0KBUK3+nTqLZ9OwCNGzdGp9Mxfvx4/vjjDxo2bMj8+fM5evQo\nKSkpnD9/nnXr1jFs2DDq16+PvBDxC31SEldbtCRpy7fmfiulQtRoCv1Az1O/8/Pzo1+/fqSmplog\nOrhx4wYLFiygf//+j/VxPYo+PYNrHTui+mqTyeOJX76c+1M/Qv+vP4/w8HCaNWvG6tWrzfZFferU\nKfR6/WPG28r69UttOC5hHGw8PQlYuwYxO5s7I0ehTy++p6thw4YsWbKE/fv3s2DBAjNFaV5UKhXu\njo4oKlWS+i8lKhSKgBxLAM2tW7i7u7N69WoiIiLw8PCgZ8+edOvWLV9x2lyo1WouXbqUI0wSHUXi\nxo3o7t8v1b3kzs54T5yAQ9OmhIaGotfricoVOynPSIlbBUOQy/F5/32q7foBubMzhqws4lesxJCR\nYbGYknfvRlAocO3c2WIxVEQefYjo1q0bR48e5d69e9y5c4edO3fy4Ycf0qZNG5xKuEuszT0RVfhb\nt6JkHknbtnGjazd0hZRpubu7s23bNm7dusXo0aMtsnP44YcfIpfLizW4Ttz8NfqEBBwaNzJ5PJ5v\nv40hI4Okbd8VGBszZgzR0dEcPXrU5HHA/4RJmjdvjkGjQX3xomQDYCXY1axJlWVLyb5+nftTP3ri\n/NGjR/PGG2/w8ccfc/z4cTNEaF5UKhXe1atT8+Bvlg5FQsKoyJ0ckXt7oXkkOWvevDlnzpxh4cKF\nHDp0iLp167JgwQK0T7ALMRYXL15Ep9M9LkxSCkXJPLyGD8chvGG++nZFKJeUErcKiszBAYD0P/8k\nYcUKrnV5jbTDh80eh6jRkPrTzzi1ayf1BxgZUa/n9ogRJG7+BrlcTps2bfArg4y/9t49ABT+5UOO\n3S7XoDq7iDKKFi1aMHPmTLZu3WrWMkCAI0eO8MMPPzB16lSqFCFvr09OJnHjVzi1b4d9ruqVKVHW\nrYtjq1Ykbt6M4V/lIr1798bDw4NVq1aZPA7ISdzq1q2Lu7s7+vh47GrXNkvyKlEyHFu0wG/uHDxH\nDH/i3Ire75aYmIinp6elw5CQMAkeAwfh+Hzzx16zsbFh4sSJREVF0b59+/w+7bwNN1PyqDCJ+tJl\nbLy9sfH2LvX9DFlZZJ49S/WqVVEqlRVCoERK3Co4Li+/TODWrcidnLg75h3uvPsu2lIeO5eG9D/+\nQJ+UhGt3SZTE2AhyOZpbt8j873+Ncr//JW7l48TNLtcP7VFlyX8zdepUXnzxRd555x2ulLJO/mnR\n6XSMGzeOoKAgJk6cWOQ81fr1GDIy8B471ixxAXiOGI5epSJ5167HXlcqlQwdOpTdu3eb3KTUYDBw\n4sSJ/DJJhb8/1b7fgXPbtiZdV+LpcO3aNX9DIfuff4qf6+rKjh07iIuLY/DgwRWq302lUuFw7x6q\nTZssHYqEhNHxGjEc19deK3SsatWq7Nmzh927d5OUlETLli0ZOXIkiSa0noqMjMTNzY2goCDUly+X\n6bQNIP3YH9zq1x/dP9eoX7++dOImUT5wCG9ItV0/4D1xAhl/Huf+jE/Mtrbm5i1s/Crj1KqV2dZ8\nllDWroM6JsYo99LcvYvc1RV5OWnAt/HwQO7tRXYx718ul7NlyxaUSiV9+/Yl2wyCPevWrePChQss\nXLgQe3v7QucYMjJI+n4nrl27oizCkNsUODRpgkPTpuiTkgqMjRo1Cr1ez5dfFjTrNiYxMTH5DwFA\nuRDDeZZJ+eknrnftRtqRI8XOCw8PZ/Hixezbt88qFF2NhUqlwkmlIvuK9RgUS0gYC1EU0T58WKwm\nQvfu3YmKimLChAls2LCB2rVrs2XLFpN8ducJk6DToUtKQlmvbImbMiTn+zX7So5ASZ5HXHlGStye\nEQSFAq/hw6n+809U+jinb0EbF0fWxSd79pQFz2FDqfnLL5J3m4lQ1g5Be/t2vjJhWXBu1x6vse8Z\nISrzoQwOKbJUMg9/f3+++uorzp49y5QpU0waT2JiItOnT+ell16iZ8+eRc6TOTpSfc9/8PnwA5PG\n828EQaDqpq/wfuedAmM1atSgY8eOrFu3zqT9DMdyRYpatGiBKIpc6/AKCWvWmGw9ibLh3K4dyjp1\niP1wEprbt4udO2bMGHr16sXUqVPNUlZlajQaDWlpabjo9FKpv0SFJO233/inzYuorxa/MeHk5MSi\nRYs4ffo01apVY+DAgbRv396olSxarZa///6b8PBwBIWCWn8cw2vUqDLdU1G1KoK9PeroHEuAhIQE\nHjx4YKSILYOUuD1j2Fapgm1QEAAJK1dxs3dvHsye80T1sNJgyMwEcozCJUyDXUiOT8mTkpeS4NS6\nFR4DBpT5PubEc/hwfD4ouhwxj9dee4333nuPpUuXsnfvXpPF88knn5CcnMzSpUuLVKDTJSUhiiKK\nSpWw8fIyWSxFIeTaEmRduFBg53HMmDHExsby448/mmTtpKQkPvvsM+rVq0etWrXQ3r2L9s4d5EUY\nk0tYHpmDA/7LloFczt333y/WZkYQBNavX09gYCB9+vTJN68ur+SVhLmJopS4SVRI8iwBtCVUj3zu\nueeIiIhg1apVnDlzhgYNGjBz5kyjyOxHRUWRnZ2dc+JGzudJWZ8fczxva5EdE1NhBEqkxO0Zxmfi\nBNz79iFpyxauv9qF1AMHjHqEfHPAm2Yty3wWUdatg0OzZkDZ/t5EUSTr4iWjnNyZE8fmzXAsQmr/\n38yfP5+wsDCGDBlikj6uixcvsnr1akaNGkVoaGihc0S9nttvDS2RWp8pSTt0iJtv9C7QH9m5c2eq\nVq3K6tWrTbLu2LFjefDgAZs2bUIQBLLOngXAPveLWsI6sa3ij98Xc8m+HMXD2XOKnVuR+t3yEk83\nuRy5m7S5IFHxsK36P0uAkiKXyxk9ejTR0dH07NmTWbNm0bp16zJv1DwqTBK/ciUPPp9dpvvloQyp\njTomhvr16wOUe4ESKXF7hpG7uFBpxgyCvtuG3N2de2PHkfj110a5tzo6muyoqHwBCQnToKhUicCv\nN+FQxgdfvUrFzV69SPnPf4wUmfnIuniJhBIkGkqlku+++47MzEwGDhyIXq83WgyiKPL+++/j4uLC\np59+WuS85J0/kB0djVObF4y2dmlwbNECmasrSdu2Pfa6XC5n5MiRHDp0iOjoaKOuuWvXLrZs2cK0\nadNo3LgxAJlnzyJzdMSuZk2jriVhfJxfegmvd9/FoUnjJ85t1KgRixcvZu/evSxevNgM0ZmGvAdR\nT7/K2Pj4WDgaCQnjI7O3x8bXF83Np/drq1SpEtu2bWPXrl1cuHCBF198sUxliJGRkTg5OVGrVi3S\nDx1Gc/16qe/1KO4DBhCwcgUe7u5UqVJFOnGTKP/Yh4VRbef3+E6dkq8upI2LQ/swDrGUD7cpu/8D\nCgUur0rebeZA1OnKdH15U5R8lLRf9hO/bDmpBw48cW7t2rX5v//7Pw4fPsz8+fONFsOePXs4dOgQ\ns2bNKlI6XJ+WRvyyZdg3boRzx45GW7s0yOztcevRg7TfDqKNi3tsbNiwYSgUCtYYse/s4cOHjBw5\nkkaNGvHxxx/nv5519hz2YWEIhRjCS1gf3u++k/8d8aTvhrx+tylTppTbfre8xC30yy9xfvFFywYj\nIWEibAMDn+rE7d/06NGDvXv3cv36dV544QVuP6EXtigiIyN57rnnEHQ61FevllmYJA9lSDAOjRsj\nyGSEhYVJiZtExUCwscFj8GBsPD0RDQbujR3HP23aEN0glCutW3Oj5+vcnzUrf37a4cOkHT5M1oWL\nOYpEjyQOolZLyk8/4fzSS9i4u1vi7TxTJKxdx5UWLUudZEOOoiTkSLOXN7zHjkVZrx73p88okdXF\nW2+9RZ8+fZg+fTonT54s8/pqtZqJEydSr149Ro8eXeS8hFWr0Scl4Tt1apH9b+bEvW8f0OlI3rnz\nsdd9fX3p1asXmzZtIiOj7KWzoigycuRI0tLS2Lx5MwqFIv91l44dce3RvcxrSJiXlB9/5OYbvfP7\nmAvj0X63vn37lst+t/wTN8nHTaIC4zF4EB5DhpTpHu3atePAgQM8fPiQ1q1b888TLET+jV6v59y5\nc4SHh+fYj2i1ZbYCeJTUAwfI/O9/CQ0Nze+lK69IiZtEQQQB7wnjqfTJDDxHjsCpTRvkXp6g/1+v\nQty8+dwd8w4333iDf9q8SHSDUO6+Px6A7OvXMWRkSA9kZsLGyxNDauoTFd+KQ3svp+fLthwmboKt\nLf6LFiJqtcROmvzEBFYQBNauXUtAQAD9+vUjOTm5TOsvWbKE69evs3TpUmyKUE81aDSkHTiAa88e\n2NerV6b1jIVtUBCOLVuStv+XQkVKUlJS2PavUsrSsHnzZvbs2cPs2bOp+8gXsSAIeI0aWaSHkIT1\nYuPlhToqivszZxbbF53X7/bw4UOGDBlS7mS48xK3jClT0aemWjgaCQnT4NyuHS4dXylz5U7Lli05\ncuQIGRkZtG7dmkuXSq5afvXqVTIyMnKMt6OiAIyauMXNm0/Stm2Ehoai0+mM3gpgTiSNdokCCIKA\nY9OmODZtWuScqps3o4t7iC4+Hl1cHLq4+PwyO2VICI4tWkjebWYiX1kyJga7atVKdQ/tvXvI3d2R\nOToaMzSzYRsURKXp07k/dSopu3fj1qtXsfNdXV3Ztm0brVq1olWrVoSHh+Pv74+fn99jv1eqVCn/\nhKgwYmNjmT17Nt27d6d9+/ZFzpPZ2lL9px+L9cqxBJU/nYXcw6PACWDLli1p0KABq1atYtiwYaU+\nIbx9+zZjx46ldevWvP/++4+NaXLVJOUuLqWOX8IyOLZogde775DwfytwaNQY9z69i5zbqFEjFi5c\nyNixY1m8eHGxpvTWhkqlws7GBvHMGQSl0tLhSEiYDM3t29we8hbe49/HpUuXUn/mh4eH8/vvv/Py\nyy/Tpk0bfv31Vxo1avTE6x4VJuHaNZRhoSgCAkoVQ2HY1a6NOuYKYcOHAzkCJXkqk+UNKXGTKBUK\nXx8UvkU3awesXmXGaJ5t7GrVBLkcdXQ0Lk/RO2XQaMi+chXbKv64vfFGidUZrRXX7t2Q2StxLiaB\nepTmzZuzceNG1qxZw7Fjx4iNjS3gXyYIAj4+PgUSurzfv/rqK7RaLQsXLixyHc3duyh8fJA5OICD\nQ5neo7HJK40VRfGxL2pBEBgzZgyjR4/m1KlTNGvW7KnvbTAYGDp0KHq9nk2bNiH/Vx/bg5mz0CUk\nUH1P+RPEkQCv0aPJijzLw9mzsW9Qv9jd8XfffZejR48yZcoUWrZsSfPmzc0YaelRqVS42dsjc3RE\nJtnaSFRgRK0WuZcXsR9OIvn7nVT6ZAZ2NWqU6l716tXj2LFjtGvXjrZt27Jv3z5atmxZ7DWRkZEo\nlUrq1KmDTYMGuHU3bsWWMiSY9CNHqBEQgJ2dXbnucxNMVbogCMJGoAsQJ4pi/ZJc07hxY/GXsSSf\nAAAgAElEQVT06dMmiUdCoiJzrUsXbKsEELCmaHVFfWoqaQcOkHXxIuqLl8iOiUHUavFbuBDXLq+a\nMVrTo0tMRFAokDs7l/gag8FAQkICsbGx3Lt3r8jf4+PjH7tuypQpzJ07t9B7ijodN3r0xMbXl6rr\nvyzTezIVWX//TeyUqVRZsQK76v87sU1LS8PPz4+ePXvydSnUZlesWMF7773H2rVrGTFixGNjol7P\nlWbNcenyKpVnzizrW5CwELrERG70fB2PQYPwHPpWsXOTk5MJDw9Hr9dz9uxZPDw8zBRl6Xjw4AHP\nP/88rmo139epQ63Dhy0dkoSESRENBpJ3fE/ckiUYMjLwHDYM7/fHlfr07c6dO7Rr14579+6xZ8+e\nYqtS2rZtS0ZGRn7fubH7wFN/PcC9ceMI2rmT1kMG4+HhwYESCJpZEkEQzoiiWEDG15QnbpuAFcBm\nE64hISEBuPfvj5An+qDXo7lxIz9Bc2gUjkunThgyMrg/bToyZ2eU9evhMWQwynr1SyTvXZ4wZGVx\n4/VeODRsiN+ihSX+ApDJZPj4+ODj48Nzzz1X5DyNRsP9+/eJjY1FpVLRoUOHIucmf/892Vev4vXe\nu0/9PsyFonJlNHfukLz9O3ynTs1/3dnZmUGDBrFhwwYWLVqE11OYhV+5coVJkybRqVMnhueWpjxK\n9j/XMKSn49CwoVHeg4RlsPHwoPpPP5Zog8TNzY0dO3bQokULhgwZwp49e6xCpKcwEhISaN++PfHx\n8Sxp3x65TJIDkKj4CDIZ7n374Pxye+IWLsKQlVmmf6MBAQEcO3aMl19+mVdffZXvv/+erl27Fpgn\niiKRkZH069cPzbVr3BrwJn6LFuHUqvhTuqdBGZJjTZUdE0NoaCh79+412r3NjiiKJvsFBAEXSzq/\nUaNGooSEROkw6PXizYGDxKiG4eLlkNri5ZDaYlTDcDFu5cqccYNBzL55UzTo9RaO1PTEr14tXg6p\nLSb9sMtiMeiSk8WYZs3FmwMHiQaDwWJxlIS74yeI0Y2biPqMjMdev3DhggiI8+fPL/G9tFqt2KxZ\nM9Hd3V28d+9eoXMSt30nXg6pLWbfulWmuCWsh4wzZ8SknTufOG/ZsmUiIC5atMgMUT09SUlJYnh4\nuKhUKsXDhw+L92fNEmOnTbN0WBISZifvWSHj9Gnx9shRYvbt26W6j0qlEps0aSLK5XJx27ZtBcav\nXbsmAuK6devE5D17xMshtUX1lStliv3fGPR6MfvWLdGg14tLliwRAfHBgwdGXcPYAKfFQnIlaRtJ\nQqKCIMhkKCpXwu311/Gb9wXV9/5MyH9P4T1mTM64IGAbGIjwDOweew4fjkPTpjz4/HOyb9ywSAwJ\nq1ahT0nB9yPrkP8vDvf+/TCkpZG6b99jr9evX58XXniBNWvWYDAYirj6cebPn89ff/3FypUr8fMr\n3Bcw6+xZ5J6eRm0+l7AsiV9v5v4nM8k6d67Yee+99x49evRg8uTJ/PXXX2aKrmSkp6fTuXNnLly4\nwK5du3jppZeoNGMGlT/7zNKhSUiYnbxnBe29e2ScOsX1Lq+RsHo1hqcU2fLw8ODgwYO0bNmS/v37\ns2HDhsfGHxUmUV+6jKBUYltKobWiEGQybKtWzfdygxyBkvKIxZ/gBEEYIQjCaUEQTv+7d0RCQuLp\n8Js3j0off4Rrt27Y1ajxzBobC3I5fvPnIVMoiP3gQ7OrOYo6HZnnzuHWqxfK2rXNunZpsG/UCLta\ntUjcurVQa4Dr16+XqB/g/PnzzJw5kzfeeIO+ffsWOc9z+Nv4zZlt9QmtRMmp/NmnKCpV4u74CeiS\nkoqcJwgCGzdupEqVKvTp04fExEQzRlk0WVlZdO3alVOnTvHdd9/RqVMnS4ckIWEVuHbtSo19e3F6\n8UXily3nRtduZJw48VT3cHFxYf/+/XTo0IG3336bZcuW5Y9FRkZiY2ND/fr1UV++jF1IMEIR1jpl\nIePkSR589jkNGjQAKLcCJRZP3ERRXCeKYmNRFBt7e3tbOhwJCYkKgqJSJSrP/hwbb28MarVZ1xZs\nbAjauhXfqVPMum5pEQQB7/Hj8RoxssBYjx498PX1ZdWq4pVis7OzGThwIB4eHqxatarYpMyuZk2c\n2rQpc9wS1oPcxQX/pUvRJyQQO3kyYjEntG5ubmzfvp3Y2Fjeeusti/u7aTQaevXqxdGjR/n666/p\n2bMnkNMvfO3VLiTt2GHR+CQkLI2iUiWqLFtKwJdfIooG1DExT30PBwcH9uzZQ48ePXj//feZM2cO\nkJO41a9fH1uFAnVUlFH92x4l+59rJH37La56Pf7+/lLiJiEhIWFtOLdvT5XVq8zqFaaOuYI+ORlB\nLs+xACgnOLd9CZeOrxRIuGxtbXn77bf5+eefuXnzZpHXz5w5kwsXLrB+/fpihUzU0dGk/PQzhuxs\nY4UuYSXY16+H70dTyTj2B6lPaP5v2rQpCxYs4Mcff2Tp0qVmirAgOp2O/v37s2/fPtauXcuAAQPy\nx/SpqWiuXUM088aPhIS14tS6FdV//BGPN98EIPWXX0j8+usSm3fb2dmxY8cOBgwYwMcff8zUqVOJ\njIwkPDwcMTs7RxzlpZdMEruydggA2dHRhIaGSqWS/0YQhG3ACSBEEIS7giAMM9VaEhISEkUhCALa\ne/e4+9576FQqk64larXcGz+e2yMLnlyVB3SJicSvWlWg1G3EiBEIgsC6desKvS4iIoL58+czdOhQ\nunTpUuT9RVEkbvFi7n/yiVHjlrAe3Pr2xX/5MlxefbLFyNixY+nevTuTJ0/m1KlTZojucQwGA2+9\n9RY//PADS5cuLaCAqk9OBkDu5mb22CQkrBWZnV1+KWP60d95OPcLbr81FH16Romut7GxYfPmzYwY\nMYIvvviC+Ph4wsPDkdnb4/PBBzi98IJJ4rYLzlGWVMdcITQ0lKioKDRmbqMwBiZL3ERR7CeKYmVR\nFBWiKFYRRXHDk6+SkJCQMD769AzSfz9G7EcfmbQsK+m77WiuX8ervCZuCQkkLP8/Unbteuz1qlWr\n8tprr7F+/Xqy/3VSlpGRweDBgwkICGDJkiXF3j/lhx/IOPYHPuPHI7OzM3r8EpZHEARcOnRAkMnQ\nPnyIrpje9bx+Nz8/P3r37k1SMb1xxkYURUaPHs2WLVuYPXs248aNKzDHkJICgNzV1WxxSUiUJyrP\nnUPluXPJjIzk9rCh6FNTS3SdTCZjzZo1TJgwAblcTuvWrdE+eGDStga5iwsKPz+yo6MJCwtDq9US\nHR1tsvVMhVQqKSEhUeFRhgTjM2kSGb8fI+mbb0yyhi4pifgVK3Bs8TxOJir1MDXK4GAcGjcmadt3\nBXqUxowZQ3x8PLv+ldRNnjyZf/75h6+++gqXYkpStffu8XDuFzg0a4b7gP4miV/CehA1Gm7168+9\niR8UW0bl7u7Ojh07zNrvJooiEyZMYN26dXz00Ud89NFHhc7TSSduEhLFIggCbj264790CerLUdwa\nMgRDZmaJr120aBEJCQmEhoZy/+Np3Oo/4MkXlgG7OnXQp6cRGhoKlE+BEilxk5CQeCZwH9Afp5de\nIm7BQtRRUUa/f8KKlRjS0vCZMqVcqyW69++H9u5dMv7887HX27dvT82aNR8TKTl48CArV65k3Lhx\nvFRMsiqKIvenTwdRpPLs2c+EJcWzjmBri9d775F56hTxK1YUO7dp06bMmzePPXv2sHz5cpPHNn36\ndJYuXcq4ceP4/PPPi5wnd3LCsWVLbCThNAmJYnF5+WUCVq7AqVVrBHv7p7rWzc0NURRzFCXrmFaF\nucryZVRdt47g4GBsbW2lxE1CQkLCWhEEgcpzZiN3cyN++f8Z9d6iwYBOpcK9b5//b++8w6Mq1gb+\nm63pnU7oUi10RAXBig0LoiJeBbuoWC52vXYveK+CBfwEEUQQe72KoGADQVBAkKZI7yGN9GyZ74/Z\nkE0g7CabsEl4f88zz56dOe9535mdPWfeOVOI8I2jr6vEnnUW1pQUMt+ZXSbeYrFw2223sXDhQlat\nWkVWVhYjR46kQ4cO/Pvf/z7iNZVSJF13HY2ffAJH82Y1ab5Qi0i49BLiLx9C+v+9Tu5PPx3x3Lvv\nvpvBgwdz33331eh8t+eee45nn32Wm266ifHjxx+xkyWqZ09aTH0DewX7EQqCUEpM//40vPcelFIU\nbdyIa+/eoGXde/bgycyssRUlSyjZIslut9OlS5c6uUCJCvcyvP707NlT//rrr+E2QxCEekzhhg04\nmjfHEh1d7dfWbneN7D9ztEl75VUKN6yn+YQJZfKTkZFBs2bNGDFiBPn5+cyaNYuff/6Z3r17V3gt\n7fXKG7ZjGG9hIVuuuBJ3WhqtP/0Ee6NGFZ6bkZFBt27dsFgsrFixgoRqHqI4YcIE7rnnHoYPH85b\nb72F9Rjd51IQahLtdrPpwovQHg8tpk0LqrMuZ/58dtx+By1nv0NUt241ZpsnO5ud991HwmVDGP3u\nbObOncvu3btrTF8oKKV+01r3LB8vT1NBEI4pIjp0wBIdjbeggILVf4R8vYKVKynatBmgXjhtACl3\n3E7qq68ekp+kpCSuuuoqpk6dyowZM3jooYcCOm3bbriB9Den1bTJQi3FEhFBswkTiOnfP+D2GElJ\nSbz33nvs2LGD66+/vlrnu02ZMoV77rmHyy67jOnTpwfltO0d9zybh1xebTYIwrGAstlo+vw4PNnZ\nbP3HPyg+wjYyJRSuWQsWCxEdOtSobZaYGPKX/Ur+8t846aST2LNnD/v27atRndWNOG6CIByT7Hni\nCbbdcAOuXbuqfA1dXMyuBx5k5113hX0T4eqkZPhY8bZteMstlzxq1ChcLhfdunXjscceO+J1Mt9+\nm/zFS2Rxh2McZ5vWNB03Fmts7BE35gY4+eSTGTduHJ988gmvvFI9Q5pnzpzJLbfcwnnnncfs2bOx\nBdHBot1uClauxFtQUC02CMKxROSJJ9LyrenowkK2/OMfFG3ceMTzY889lyZPP13je58qqxXnccdR\n5NsSAOreAiXiuAmCcEyScvvt4Haz6eJL2HTRRWy55hrylvwCQPGOHex/fTKZ777Hga+/Jm/xYgrX\nrsWbV3afmoxZ71C8dSsN/nlvnV6Q5HAU/LGGv88dRM7cuWXie/XqxezZs/nss89wOBwVyhdt3sy+\nF8cTM2AA8ZdeUtPmCnUA1969bBl6BbkLFx3xvHvuuYeLLrqIMWPGsGzZspB0fvzxx4wYMYIBAwbw\n0UcfHbHOlqC9XnY/9i8KVqyQFVAFoYpEdOpEyxlvAZA2ceKRz+3QnoQhlx0Ns4jo0IGi9evp1q0b\nEydOpGPHml0QpbqROW6CIByz5C1dSvbnn+PNPoAnK4sGo+8kqlcvcr77jh23jTrk/BZvTiX6lFM4\n8M037HniSbwHDhDVpw+pUybXO8dNe71sOu98rElJtJr9TuVkPR62Dr+Gos2bafPF59gbNqwhK4W6\nhLegwMx3S0+n9SefYG9Ucb0ome9mtVpZvnw5cXFxZGVlkZGRQUZGBunp6Uc8LvnMzMykb9++zJs3\nj5iYmIA2aq3Z+8yzZM6aRcodd9DgjturswgE4ZijePt2bMnJFb5N8xw4QP7SpUT16nVU9kzMmDmL\nvc88Q7sfvj/inNtwU9Ect/oxIUMQBKEKRPfuTfRh5mjFDhxIh5Ur8GQfwJOdhTc7G092Ns5OnQCw\nN2pE7Fln4c3Pp8Gdd9Q7pw1AWSwkDLuKfWPHUbhuHRG+vAdDwe+/U7B6NU3HjhWnTTiIJTKSZhPG\ns/nyoewaM4YW096scF5oUlIS7777Lv3796dp06YUFhYecThyQkICSUlJJCUlkZycTNu2bUlKSqJZ\ns2aMGjUqKKcNAI8Hd3o6SSNHknL7oZ03giBUDkdqKgCe3Dx23n03KaNuI6p794PpBStXsuOOO2kx\n463DPo+rm8gTjieqZ0+8OTlQix23ipA3boIgCMJh8WRn89fpA4gfPJgmTz1ZKdnirVuxt2hRL51a\nITSyPv2U3Q8+RMqo22gwevQRz/3iiy/4+uuvSU5OLuOY+R8nJCQENW8tEN6iIixOJ9rjAYslrHX3\n5V9eZsWeFbxx0RtYLbL6pVD3ce3dx7brrsO1dy+pr00i+uSTAdj/f6+TNmEC7ZctxRobG2Yraw8V\nvXETx00QBEGokF2PPELOt/M57qcfsQSYH6RdLgpWriSqV6+jZJ1QV9n18CMUrl1Lq/ffC1ivjgYZ\ns2aR+c5sWs54C1tyclhtmb16Nld/fDWDOwzms6s+C6stglCduNPS2Hb99RRv207zV18hpl8/doy+\ni8IN62lXbj51TaO1rtUdi7IdgCAIglBpGtx5J22/+jKoxvX+KVPY+o9rKViz5ihYJtRlGj/2KK3e\nnV0rnLasTz5l79PP4GjVCmtcXFhtWbJjCSM/G0n/lv35YOgHAPyZ/iceryesdglCdWBr0IAWM2bg\naNuG7aNuJ+f77ylcu5aITjW78XZ59jz7HFvq6FYf4rgJgiAIFWJv3DioNxCF69axf9JrxF1wAZFd\nuhwFy4S6jCUyEktEBJ7cPPa/PtkMTwwDB+bOY/cjjxB9Sl+avfgCym4Pix0AW7K2cPG7F9M8rjkf\nXfERDquD7dnb6TG5B7d9eVu92nJEOHaxJSbScvp0YgcMwJacgmvHDiI6H13HzRIdReGGDXiLio6q\n3upAHDdBEAThiBTv2MGW4deQt2TJYdN1cTG7HnwIa2ICjR595ChbJ9Rlcn/4nrTx49k/6bWjrjtv\n8WJ2jhlD5Ekn0fzVV7E4nUfdBn/+TP8Tm8XG/67+HylRKQCkxqcyuvdopiyfwph5Y8R5E+oF1rg4\nmr/yMhFdOtNmzlfEX3LxUdUf0bEjeDwU//33UdVbHciqkoIgCMIRsaWkULxxI5mz3z04odyftEmT\nKNqwgeaTJmFLTAyDhUJdJe7888n78Sf2T5pEVK+eh61fNYXzuOOIO28QjR99tMY3/g2Gc9qew9+j\n/ybCFlEm/pkzniGnOIcXl7xInDOOxwc8HiYLBaF6URYLztatj7peZ/sOABSu33DU3/aFirxxEwRB\nEI6IJSKC+CFDyPn2W1x79x2S7mjenMThw4k9Y2AYrBPqMkopGv/rMRytW7NzzH2409JqXGfR5s1o\ntxtbSgrNnn8+7PPaxswbw5TfpgAc4rSBKaMJgyYwousInvjhCT5d/+nRNlEQ6hWOli1QEREUbVgf\nblMqjThugiAIQkASr7oSPB6yPvjgkLSEyy+n8WOPhsEqoT5giY6m2YTxeHNz2fP0MzWqq3DDn2y5\nahh7/z22RvUEy2vLXuOFxS+wJu3IC/pYlIUpF01h4vkTubD9hUfJOkGonyirlaQR1xFx4onhNqXS\nyHYAAZjz1xzGLRrHaS1Oo1+LfvRN7UucM7y9c4IgCOFg2003U7RhA+3mf4uy20mbOBF748YkDBkS\nbtOEekDOgu+I6NgBe9OmNXL94i1b2HLNP1AWCy3fmYWjefMa0RMs3/z9DefNOo9B7Qbx2VWfVWq/\ntj25e1i2cxkXdbioBi0UBCFcyHYAVcTldZHnymPswrEMmjWIxHGJdH+9O2l5ZjhHsac4zBYKgiAc\nHZJvupHkW29Ba03+8hXsf3UiBSt/D7dZQj0h9oyB2Js2RXu9uHburNZru3btYuvI68HjocW0N8Pu\ntK1LW8fQD4bSuUFnZg+ZXelNth+e/zCXvncpn62Xfd4Eoaq409LwFhaG24xKIY5bABL2Dqbjj8u4\ntziLe5K/ZXizf5Fsa0mCw6z4NOrLUbR5qQ3XfnItk3+bzNq0tXi1t1I6itxF7Dywk9/3/M78TfNZ\nm7YWMJsDTl85nZ+3/0xWYVa1500QBKEyRPfuTdLVV4PHw+6HHsLepAkNH3gg3GYJ9Yy9z/2bzVdd\nhXv//mq5ntaaHXeOxpubS4upb+Bs27ZarhsKCzYvINIeyRfDviDWGVtp+ZcGvUTPpj254sMr+HbT\ntzVgoSDUbwr//JP0N6fhyT4QblMqhQyVDMD778P998OOHeC/zcz69dChA9w5ZSafrP+EzNiF5Csz\naf/4xF6svH0pVit8v+V7/kr/i/35+0nLT2N//n76NOvD7b1vx+P1kPx8MtlF2WV03t3nbsYPGs+e\n3D00eaHJwfgmMU3o3KAzo/uMZnCHwbg8LjILM2kQ1aBW7/4uCEL9wVtQwJ8n90UXFdFi+rSjugqg\ncGxQuGEDW664kqgePUh9YwrKEnofc8GqVWi3h6ju3arBwuohqzCLhIiEKstnFGQw8K2BbMzYyLxr\n5nFqi1Or0TpBEMJJRUMlZTuAAFxxhQluN+zcCVu3mtCypUnv7L6Grz+/hn3bNMRshBYL+UNpCm+A\nqCjNhW9dQR5mWKXVE43DncJv3zfl9t5gtVjpH3Ub2bkxxNlTSHQ0IMGRQqvdbQBoGN2Qbwf/zd/Z\na0nT69h4YC1r09ZS4CoAYNXeVfSc0pPkyGQ6N+hMp5ROdG7QmUs6XkLLhJZhKS9BEOo33vx8tNdL\nwtCh4rQJNUJEhw40euRh9vzrcdInTybl1luDknPt3Ytrxw7cGRl4MjPxZGRiTUwk8coriKwFixBo\nrbl37r1c2ulS+rfsH5LTBpAUmcS8a+bRf3p/7vvmPhZdv0g6cQWhniOOW5DYbMZZa1nOH7rtNhO8\nXsXevcexdetx7NwJ0dHg8Xq5M2Eev3yXgjcvmeK8SAoLzbVKSFr+b+bOhmK/qXLNmsFdI80qUi88\n0oY5c9oAFxIXB02bwhc94MqZ0CS2CSObTGBbwVr2Za/lvT0fkF2cyfENj6dlQku+/PNLHpz/IK0T\nWpuQaD4Hth5YLQusuL1udufsZvuB7WzL3sb27O04bU5G9xkNwLM/PktmYSZNY5seDC3jW9YKp9Ll\ncbE5azNZhVllwmktTqNzg84UuArYmLGR1PhU4p3x8jAUBB+25GTaL16MNSY63KYI9ZiEoUPJ/2Up\naS+/QszppxPRqRO5CxeRu2AB7swMPBmZeDIycGdlctz8+SiHg/TXJ5P5zjtlrmNNTCS6T28crVqF\nJyN+jFs0jgm/TCApMon+LftXyzUbxTRi/rXzcVqd8pwShGMAGSpZS/B4oKgICgrA5YLGjU38kiXw\n55+waxfs3m0+k5Ph//7PpJ94IqxeXXIVDdH7OKtfPN/MieC7zd9xy/TxZKvNZFs2U6TzAFh5w3pO\nat6Bqcun8vpvrx906EqcuwGtBuCwOsgqzGJL1paDTtn2A9spdBcyYdAEAAbNHMTcv+eWycdJjU5i\n5a0rATh35rn8uPVHCt2lEz8HthrIgusWAHD+rPMp8hQZpy7GOHbdmnQ7+EBbtG0RxZ5iXF6X+fS4\naJ3Ymq6Nu+L2upm2YhourwuXx3XwnL7N+zKw9UAyCzK5/avbD3HMHu73MHf0voMN+zfQcWLHQ36H\niedPZFSvUfyy4xdOnmreJsQ6YmkR34LU+FQe6/8Yp6SeQlpeGmvS1pAal0rzuOY4bc6Q60BdJLc4\nl82Zm9mdu5tWCa1on9w+3CYJglBP8OTmsWXoUFKnTMHRvBn7p0wh/Y2p2BITsSYlYU1KxJaYRKMH\nH8ASHU3hhg2496WZ+KQkrElJWJy149788bqPGfL+EIYdP4xZl82qESer2FPMPV/fwz9P+SdtEttU\n+/UFQTh6VDRUUhy3Ok5mpnHm/B27Ro1g5EiT3rmzmY+ntYao/ZC4mSv6deO92XbeX/M+d8+YSoFz\nMznWLXhwAbD91jyaN4ri+s+uZ9rKaQd1OawO2ia2Zc2oNSil+Hjdx6TlpR10alLjUomPiD94vtZQ\nVKTJyM9m54Fd7DywC7tyclb7fjid8I+PRrBu31/szd/F3vxduLzFDG55LTOHvEVsLEQ8E0GRp6hM\nfkf1HMXECybi8rhwPOM4pDzuP+V+xp09jpyiHLpP7k5CRAIJEQnEO+OJdyZwVtPLubLHIArceTz/\nxadsWZ9AXnoCB/bFk7UngfTtKaxaHkGB2s+zs75jxeZteGO2UeDYxgG1ncf7TGDYqafx0boPGfrB\n0IN6G8c0JjUulamDp3JCoxNYsmMJn67/lBhHDLGOWGIcMcQ4YjjvuPOIc8aRlpdGZmHmwfgYRwwW\nVfvWCnJ5XGw/sJ3NmZuxWqwMaDUAgLPfPpvf9/xOWn7pZrnDTxjOzMtmorWmx+QeJEUmHawXLeJb\n0LtZb05sFP7hSoIg1B1KVnyzRESgta6Tb5V+3fUr/af1p2vjriy4bsFhN9muDjZmbKTPG32Ic8ax\ncORCmsU1qxE9giDUPDLHrZ6SmGhCly6HT1+71gzD3LlTsW1bA7Zvb0DJFjkXt7uCJ/93BQe2giff\nA7G7IH47L+dF8fzzMKT1Tcx+8gI40AJ1IBWd25DN2sL4Irj3XjjRfhmd+ppraV0aJk40w0dXroTu\n3RWQ4AudAXj7bbjmGripwXROv7zEUg2RGXxucbMgES6+GB4/7iseftCC0nainHaiIuzM/awhS5Kh\nTx8bn/bfzttv2YmLthMXYychxkH8Gjt7T4RGjWKZ0OYvXnsN9uyBtbth71540wMDd0PjxtFY/hjO\njCcgLg6aNDGhfS+IioJolQJrh7J4UtlhrLfGwtUH4IzWZ3B53nxWbt6Go8E2dME2MvO28/28WE74\nB6zeu5oXf56AS5d1PMc2+ZsHbo7jzRVv8uD8B8uk2XQUTyVt4aHRDRi/eDzPz5+CRTuxKycOixOH\n1ckLPb/g3DOdzFw1k1mLF2DHSYTdhCh7FPf3fpw2bcz+g58sXI/HZUV7bHjdNiyeKEZ0v4b+/eHH\nLT/z6rQ9eFy2gwFt5YaBZ3LJJTDi4xv59PcFHFDb0MqsytNSncqsMxdy6qmQGtOGAm8bTottTSNH\nGxLtTUjITmLDBmjdzkWruHas27md33d+Q3rxLjSaa1s/yH/POxFHbDatJrSiUWQqTVbiJ50AACAA\nSURBVKNTaRabSoIzmdObXMigLn0ptmbw2uLp7N9rB68dPHa0x05bxykMPbMdzvhMPvhlESt/s+N1\n2/G6bWhLMc3tJ3DjsEYQu5O3F89j3Z9FeC1FeJUJA1KuZti5bdmYt5wXfvg/0rOLcFMIeLFb7dzW\n+XEG9ezAin2/MPXXmXjddhxWE5w2O9efdCvHNWnEH2mr+GHLT9gsNiJsDhxWJ3aLk7NbDSIhOprd\nubvYvH83yh2BVTmxE4FdRRDvSKJFqnHOd++GrCwzd9blMgGgTx/zuXSpWRCpuLg0PTIShg0z6e+8\nA5s2gcVihl7b7dCwIQwfbtK/+sp06pSk2WzQoAH09f1fV6yAwkJQqjQkJEB73wvT1auNzpK0kntN\nyVDxdevMf91qNTZYreZ/lGIW22XvXhNfkma1gsMBTqeR8x5m4V2lzPmVTS+xz9/WQHi9puw9ntIQ\nGWlsLCyE9HQT539OaqoZAn/ggJnvXJIvm818Nmxo5IuKzDXKp1fD+hpBUVI+JXa73eZ7vK9PraDA\nxJWUZ8lnyYspr7dyZVnTWCJKnZy66LQBTFsxjQbRDfjkyk9qzGkDaJfUjrnXzOWMt87grLfP4u4+\nd9O1cVf6NO+D1ppfd/1qOhOdsQc7FSu7DUFFFHuKSctLo9BdSKG7EI3Goiw0j2tOnDOOfFc+u3N2\no5TCoiwozGeD6AZE2CIodBeSXZiNUgqH1SFTFAShAsRxOwZwOKB1axP8cTphzRrzoM/KsrJtWyrb\ntqUebJz1b92XUQPNsX8Dr3t3E5eYCPfdd2jDqUcP871pU3j22dIGTEno6es/aN8e3nijJF5htSaX\nSR/a4wwst0JOTtkQF2ce4Dq7Ob/9UBpf4mCdMdC8dczJMQ2sJk3MkNIS5ywy0px3773G/qiow5fb\n+PHw4oumgb17t3EAc3JMWlJkEt0SziA9HXb/YdKysuDV9nDnP+CmHjcx896b+HFRMdjzwJkDjlze\na9mcB26GwR0GM/m/zdm6JxflzEU5c1DOHH5U0Tw02rzBy/n7ePKKCsFaBLYisOYwabGNc8+EDfs3\nMG/jPLyqJK0ItIXc/z3OzJkw+4/ZvL3t7bIZykvBsd44bi8u+Q+fWT4FJyYAlpwW9Fq1lUsugVh7\nIjnrToaMqyGjDWS2ZmtGO763wqmnwuPdX6fVZYeWmXMC3HWXg6dOeJ8TrvZFWlwQu4sZHienvwQX\nX+XmzIbD+eib7WyI3w7xv0BEFq+ObcQHY/rSsf8uHv3pn4de/LOpHJfcjvgu67nlh8NsOjvrPc45\n9QpyG67loSXXH5L87os9ObVTW3Y7dvO/P7/gQKYT3BGgLWBx8/W/8tj8M2zK2cTM32dRUOQCq8vY\nb/Hy/Mgh7F/biO+3fM9dX991mAqzheK0aKavnM4jCx45JNkxYT9Fmck8uuBRxv7wIp6iCHA7wWsD\nr52kd9aTvs/B2IVjGffFh2Sl2w+m4Yqk5aIvGTYMXl36Ks/8vJC9u23gtRr7i+I4cddLDB8Or//6\nOv/6cBX79lhMmrZAfgqneh9h4UKYunwqD4zbRPp+Bdr3581tzKDk25kzx8jf9/ROU9dL0rNbMLTd\nDbz/PkxcOpF/Pp5GUYHf9TPaMrL3lbz5Jryx/A1uviMP7bGWSR994dm89BLMWD6bETe4zLW1BVCQ\n0ZZHRvThmWfgzSUfcOONmHitfOntGHvPiYy5z8PUn77ilpvLNejSj2PiUx244eYipsyfz513+uJL\n7E/vwIyX2jDkqnwmfvkD9z/oAmtxadhxMp9M6Ujfs/by4HvTmT6jXPqq4XwzowfNTlrH7bP+y3ff\nOsDjAI/TfK6+miVfdKZRhy088tb/eOdtX7rbl76tH2uWNiSl5T6ee30DkybaUF47StuwYMOS3Zb1\nqyNJbJjHuJeyefUlOxZlzrEqG8rrZP06RXw8PPkkvPJKWafT64XcXHMvve02eP31ssUTGQn5+eb4\npptg1qyy6Q0bGmcb4NJL4fPPzXGJU9eunRm5AXDeefD992UdvxNPhEWLTPpZZ5mOAf9nRt++8Jlv\nu7F+/eCvv0qvD3DmmTBzpjnu2dPcb/0ZPBhee80cd+li7sP+zuWVV8K4cea4fXtTJv7PpREj4JFH\njNN60kkcwqhRcPfdsH9/aeeGP/ffb8pt2zZja3meeMJ0mqxbB5dccmj62HGv8MgNj7B9fSP6XX1o\n+sSJcPbZplxvuqlsmlLw1lvGrjlzYMyYsg63UqYz9PjjTRk/+2xPmid/yZ89LuTWL2+lyZZ7WfSv\nPjRolkfvN3ofojt14xP8+sLjWGL2033CmWSnxWJ1x6K0Ha+liCY7b2HpW5exu+gv+r12IVl5hXgt\nJaGAThsns2rGCH7b9RunvHnKIdfvuel9lr01lEXbFnHOzHMOST9tyxx+mjaIr/76iiHvDynNt7Zi\ncyVx9v4v+PL1Pvy09Sduf2MaufuScXhMcHpS6Jl0Jq9NiKPQXcijj1jZvNEOmHYNQKdOph0CcPvt\npkOspJMZTHvliSfM8XXXQVpa2Y7o/v1N3QHToZyTUzb9ggtM/QBzrtdb2mllscCQIUZvUZGpx/6/\nncVi6u7w4ab9YO57ZbnmGlOndu+G0aMPTb/pJjjnHNiypdSOknoDcMcd5j+3fr25d5RPv/de859b\nsQL+8x9jv3948kk44QT48UcYO/bQ9EmToGNH+OKLw6d/+KFpe86YAc8/f2j6woVmatCkSea+ZrOV\n7XCcOxdiY+HNN+Hjjw9Nnz7d3Pfeftv8f/xxOErvG5Mnm6lH/sTFwYQJh5ZpbUccNwGlSt/c+T/U\nYmPhhRcqlktOhueeqzi9USN4+OGK0xs3hhtuqDi9XTs40hZRl1xS9iFZXFzq2AFcdZUJFREbxNY5\n/mXTuXPZtIcfLpu/ggLTG1/CF1+A1+vAanVgsSQedFwBOjXoxN+fdqpQ77AThjHk9WEUF5sbfnGx\nCQ7f6NCnz3iawbFPU1Dgn65p6XOmJl80mRuavAJWNza7B6vdjdXupaNvd4mXz3uJJwc8iUe7cXvd\nuDwuEiIS6NLQpL9y0X94xecbaV3aQCx5a9C8uWnoud2lb408ntI3Lm3awOLFJWl23O6WuFzmAZAc\nlcxrF73KJQ4j53YbGcdtmp49ITWlE5tuyuK3lS4sNhfKZj4bDk7hhOMAxwn8dO1ScvKMU6UtbuwW\nB21v6USLFCjWp7L6xk3kZUVg0U6U14nF60RfaKNtWzg+8gL+GLmbDRvK2u453TRgh7UaRmfvMH77\nrfSNi8vtxfOgIjoabu5xM80yr2LxLy6KPS68liK0pYiGY5qgFFzR5Qr03hPYuLkQr6UIjyrEowo5\n4wVT4U5NPZUr2xSxP6sQjyoC5UYrN4+8Z27H8c54urZrRFErN15ceLQbmzWfT1805bQ9ezvxHVYS\nfZwLj/bi9XqJdybyg+9t2887fsbd4SsSjvPi1SY0jmzJ5EGm9fH+2vfJ6rIAi99+k62jTuLJ028H\nYNrKaeR2XwqUDqPvGHUq/+xv/qwTl02k6OR1ZerriZHnccOpVwLw5A9Pos/dUSb9JNsVXNz3bADu\n+uY2uLTsFijduZ4z+5vXjTd/cxVcUfaV28ncxWmnTaDIU8Qt3w+Gco3f/jxKr15Pk12UzZ1LLoDh\nZdPP9I7lhBMeYHfObu7//XwYVjb9MudEjj++I7tzdzN9x4Nwhom34cSm7Nx4aW+OP74H24tzWeea\nS9zJxbh1abjtkj60bt2Zpfv+4J2sO6Fcv8J1LKBhw4Z8u3kBL+UMg2vLpg/LXUpMTC9mrZ7F0/m3\nQLnG++V71mG3d2T84vE8xRj0HWXTby3cBjTjqR+eYkqTp7A8UTb9iYQMII77v7mfd9uPx/qEcYgV\n5vPhhHxAMXrOaOb2eBNHt9J0OzHcGGM2wx715SiWnPIZjt42FMbpjPI24tL4HwEYM28MG09fTMTJ\nNpS2orSNGE8q/RpMBcym0XvPXIXzNNBoQBPnbkfv1JcBuOOrO9h/3p/gLql7miTXSZzUzjyIbvz8\nRnIv3oX2WNAolLaQXNSL9u0fNeXwv1txDc72dQiYToGUgr6kpt4GwAML7oHzi1FYjX1YSSnoS7Nm\nxmH4769PEDnIUiY9ubAXjRsPoNhTzLT1rxLr8z1M2UFy4ck0atSX3OJc3t08legzS9NzHH/SIWMM\nKcmtaBrblAMxpR2T/iQmms+EBOjt51uVOAclz6q4OOOIlLxVLfkseVvqdJrncpLuR9Nl+3DZMrBo\np3njbXXwUMsv+GJuLh5rDm6r+Uwo6I9S4NVeUuytyNW5eBwZeC3FWLwRKJsZDhDtiCbV1gMynChv\nBMoTgcUTgS3jeJSCtkltucz+Oit/jUB5nb7y0VjyzVzxLg27cCkzWLXaC2hQXlAah+t4ALo27so5\nrkls/NuLthThsqXjsqej8xsBsDNnJ5vUPPIbpaOtpfPmUzauB+KYtGwSL0T9E0unWJTXDkqj0dh3\nbwAa8vQPT/N68jh0kolHmfrXLD0DiOLeufcyo/VL0BqUtoC2orSdXgWmt/bur+9mzgnvgLe0btjc\nCVyofj9Y91ecPM9X9wCtsBc34lLv1wDcM280i7r8DFqZO6tWOPLaMCj7XXP9b2/lq6a/l3Y2Ac6c\nTpyfYf47d86/jq8a/1Wm3kRk9mBIxisA3DL/cn5qtAu01We7lci00xieaby1fy68lh+ic1C+dLSV\nyH39uD7L/Dee+vUOvlNelMWOxWIz/92MU8jPvxitNW9v+g9/xNqwYMeiTcdSbG5XtO6Jy+Ni8YFP\nyWmhUUqDxYuyaOILTsRmO57c4lyWez8koq9GKS9YzHkNi/pitx9PRkEGy22ziTxd4/VqXB5NofaS\nmH42VmsXduXs4rO0WayM1ni8Gq/Xi6dQE7HlUiyWTmzJ2sLUDbNZuceXN6+5/yTsuwhoydasrbz/\n1wJWbCktG7xWmhYPBFIO/UPWcmSOmyAIglAptDaNnxLHEEzDEOBA0QHcXvfBNK/24rA6SIpMAmBL\n1hY8Xs9Bea01cc44msSaXoU1+9ag0Qd1aK1JiUqhWVwzvNrL8t3LD7GnSUwTmsU1w+VxsWLPioM2\nlpAan0rT2KYUugtZuWclDqsDh9WB3WLHYXWQEpVCrDMWj9dDkacIh9WBVVmDHqpVMveq2FNMdmE2\nxZ7iMqF1YmtiHDHszd3LH/v+wOV14faWdpqc1eYsEiMTWZe2jh+3/mjifee4PC5u6XkLSZFJLNq2\niK83fn2I/vtOvY84ZxwLNi9gweYFh6Q/2v9RImwRfPXXVyzctrBM2Wo0z5/9PAAfrv2QxdsXl0lz\nWB0H099Y/ga/7PgFt6/Dx+11E+eI4/WLzCu+xxY8xpKdSw6mub1umsc154OhHwBw8xc389vu3w46\nPUopOqZ05O1LzeiAEZ+OYEP6BuMy+sq+W+NuvHr+qwBc/v7lbMvedrBeaTT9WvTj5fOM49dvWj/2\n5e0rU/cuan/RwfTWL7UmpygHj/bg8XrwaA8ju47k1fNfRWuN5alDx7Pee/K9vHDuC+QU5RA39tDV\nmB8//XGeGPAEu3J20ezFsnPKLMrCaxe8xs09bj5i/REqR74rn/T8dPbn76dzg844bU5+2fEL8/6e\nR3pBOm6v+2AdeuaMZ4hzxjHnrzl8u+lblFIH0xSKp894GofVwZd/fsniHYsB48SW3KNK6v7MVTNZ\ntG1RmboTYYvg/y40K8W98PMLLNxe+t8CSI5M5s2L3wTg8e8e59fdpn1bck5qXCqTL5oMwL1z7+WP\nfX+UyWeH5A68cr5xzG79361sytxUJr1r464H7Rvx6Qh25uw8aJvH6+G0Fqcx9qyxAAx8ayDp+ell\n7L+4w8X895z/AtBqQivyXHll7ks397iZl897ucI1BR449QHGnjWWjIIMkp9PPiT96YFP82j/R9ma\ntZVWL7U6JH3CuRO46+S7WLNvDce/dvwh6VMHT+X6btezZMcS+k499FX4+5e/z9AuQ/nm728O+zZ3\nzvA5DGo36ODCQOVZOHJhrd77UBYnEQRBEARBqACtTWdCSePWq71YLVYibGZhlJzinIPnleC0OYmw\nReDVXrILzZvkkoa73WIn1hnE0A5BqMVorSlwFxxcwbvEsYtxxJAYmYjb62b9/vUH5y2WzGNMiUoh\nKTIJt9fN9uztB9NKzouPiCfGEYPb6yajIOOgQ10yBzLKHoXT5sTj9VDgLihzfYXCZrFhtVjxai/F\nnuKDDqnb68bj9RDnjMNpc5Lvymdf3r4yTq1He2id0JpoR+3d1kYcN0EQBEEQBEEQhFpORY5bja5z\npZQapJTaoJTaqJR6MLCEIAiCIAiCIAiCUJ4ac9yUUlZgInAeZh34YUqpzkeWEgRBEARBEARBEMpT\nk2/cegMbtdabtNbFwLvAxTWoTxAEQRAEQRAEoV5Sk45bM2C73/cdvjhBEARBEARBEAShEtToHLdg\nUErdrJT6VSn1a1paWrjNEQRBEARBEARBqHXUpOO2E0j1+97cF1cGrfVkrXVPrXXPBg0a1KA5giAI\ngiAIgiAIdZOadNyWAccppVorpRzAVcDnNahPEARBEARBEAShXmKrqQtrrd1KqTuAuYAVeFNrvaam\n9AmCIAiCIAiCINRXasxxA9BafwV8VZM6BEEQBEEQBEEQ6jthX5xEEARBEARBEARBODJKax1uGw6i\nlEoDtobbjsOQAuwPg6zoFt2iu+blRbfoFt31V3eo8qJbdIvu+qu7OuRripZa60NXbdRaSwgQgF/D\nISu6Rbfort+2i27RLbprt7zoFt2iu/7qrg75ox1kqKQgCIIgCIIgCEItRxw3QRAEQRAEQRCEWo44\nbsExOUyyolt0i+6alxfdolt011/docqLbtEtuuuv7uqQP6rUqsVJBEEQBEEQBEEQhEORN26CIAiC\nIAiCIAi1HHHcBEEQBEEQBEEQajniuAmCIAiCIAiCINRybOE2QDAopWzADcClQFNf9E7gM2Cq1tpV\nk/J1lWM138KxiVKqEdDM93Wn1npvJWQV0NtfHliqg5joHIpsddgeinyottfVcqsO3b7rJAForTMq\nKReW39tPvkp2hyofznyHqtt3jbDkO5y6pa5VXv5Y/r3DjSxOUo0opeKBh4BLgIaABvZhnIixWuus\nI8jOBrKAt4AdvujmwHVAktb6ygC6Q5UPxfYqy1aD7irnO1S7y12rzjUqQ7E7VHlpTFdOt1KqK/B/\nQLxPBkw9zwJGaa2XB5A/B5gE/FVOvp1Pfl5NyFaT7VWWrwbb62S5VYPuFsDzwJk+fQqIAxYAD2qt\nt9SE3aHKh2J3qPJhzneousOZ77pa5lLXjqHfu1YRzt2/a2PA/KBjgfVABpAOrPPFJQSQnQs8ADT2\ni2vsi5sXQPbPqqRVo3wotldZNpzlFqrdvvO7Akt8deRbX1jvi+seQPYcYCMwB3jDF772xZ0ThO4q\ny4didx3Pd6i6w5JvYCXQ5zDxJwO/B2H3OqDVYeJbA+tqSraabK+yfDXYXifLrRp0LwauBKx+cVbg\nKmBJLf69q2x3Hc93qLrDme+6WuZS146h37s2hbAbUNsCoTkRG6qS5ktfAgwFLH5xFl8F/yUIu0OV\nD8X2KsuGs9xCtdt3Xl1tVEpjumq6w5Jv4K8jpG0Mwu6/ANth4h2B5EORrS7bqypfHbbXxXKrYd0V\nptWG37uqdtfjfNfl37uulrnUtXqW79oUZI7bobTSWo/zj9Ba7wHGKaWuDyC7VSl1P/CW9g2d8g2p\nGgFsDyB7FTAOmKiUKhmilwB850sLRIn8JKVUJub1c3wl5EOxPRTZUOVDKbdQ7QaI1lr/Uj5Sa71E\nKRUdQNZG6fBOf3YC9iB0hyIfit2hyocz36HqDle+5yilvgRmUFo3U4FrMW/tAvEmsEwp9W45+auA\nqTUoWx22hyIfqu2Hk2+B6RiqzeUWqu7flFKTMEPQ/eWvA1bUoN2hyodid6jy4cx3qLrDme+6WuZS\n146t37vWIHPcyqGUmocZ/nS4xvzZWuuzjiCbCDwIXAw0wsyZ2gt8DozTASZgKqX6+GT+BjoCfYG1\nWuuvKpmHZN/hS1rra4KUqbLt1ZDvUHQ7gGHALmA5MAg4FVgDTNZHWJzET+9gn14qY7fvGi8DbTn8\njWCz1vqOI8g+BFwBHK5x9b7W+t8BdFdZPhS7Q5WvoXyXNKYD5TtU3eHM93mY/4j//LjPg70/KKU6\nVSC/NgjZzpj/SaVlffLnVyAfrO1Vznso+Q5Vvhp0V7ncQvnNfPfVGw5nO2bRp6IA8qHW1SrJV4Pd\ndTLf1SAbtnzX1TKXunZs/d61CXHcylHOiWjoiy5pzI/VWmcGkO+Imey4RGud6xc/SGtdoUevlHoc\nOA/TK/8NZgGD74Gzgbla62cD6P38MNFnYCZ8orUefCT5w1yvn8+G1TrwRPY+wHqtdbZSKgpTft0x\nztNzWuvsAPKjgU+01sG+5fKXnYUps0ggG4gGPsFMfFVa6+sCyLcFLsM0oD3ABuAdrfWBStgQzkZl\nKI0zaUxXTXdYGtNCKUqphlrrfWHSnay1Tg+HbkEQBOEYJ9xjNetSAEYGSB+Nafh/CmwBLvZLWx5A\ndjVmgmYUcACI88VHAquCsG05MBMYAJzu+9ztOz49CPmlfsc3Yl5ZPw4swqz0cyTZNfjmUwCTgfHA\naT75j4PQnY15Y/YTcBuQUonfZJXv04ZxsK2+7ypQufl+r3nAo8DPwETgWWAtMCDc9a2uBaBhGHUn\nhzv/NZy/kkWT1lHJRZOCuPacAOlxwL+Bt4Fh5dImBXH9xsBrvv9XMvAEsAp4H2gShHzSYcIWIBGz\ncuyRZAeVK8M3fLrfARoFoXtsyf0I6AFswswf2xrovuq7Jz8KtKni79ILM+R7JqZj6RvM6mfLgG4B\nZGOAp3z35mwgDTMfeESQum3ALZiFdFb5whzgVsAeQl2bHMQ5Vp/up4FTyqU9GkA2CrgfuA+IwAy/\n+hyzil1MFW0OuLiX77wT/Y7tvt/+c+A5ICoI+Tv86lpb4EcgE/gFOCGA7MfA8BDy2AYzvPZpX92Z\nAvwBfMBh5uWWk7UAI4H/Ab/76v27BPkMlbomde0o1rUae44ezRB2A+pSALYFSF9dUpmBVsCvwF2+\n7ysCyK443LHv+8ogbLMA92Ae7l19cZsqkTd//cuABr7jaMxbtyPJrvM7Xl4uLRjbV/jsPwcz/yIN\nM974OiA2gOwfmAn3iUAOvoac70YaaMGH1ZQ6elHA977jFoF+L79r1MiNgACNad85VW5QI43pOtWY\npuJFkx4kuJVbu1cQegC7A8h+5CvzSzCNg48AZ0l5BqH7a+BOn62rfPlI9cV9FoS8F9hcLrh8n0e8\nx/nb56tnzwAtMffKT4PQvdrv+Dugl++4PfBrANnNwH+BbcBSn86mlahrSzGjMIZhhtZe7os/E1gc\nQPYzzPD+5sC9wGPAcZh5Jc8FoXs25v5wsu8azX3HrwHvBZA93L0hCXOf2RGE7jcw94K7gd+AFw/3\ne1Yg+z7wAmYrhPnAq0A/4D/A20HozsF0nOb4BU9JfCXq2gvAdEzH6XhgRhC61/gdfwlc6jseACwK\nILsT+BDzDHofs6+poxJ17UdMp+mDmGfqGMx/9AZgQQDZaZjnx2nABMw97mzMlJM7pa5JXatFdS2k\n52htCWE3oLYFSnt8yofVQFEA2TXlvsdgGi0vEsCBwfR0RPmO/VdIjA90Ayl3neaYnotXCeBolpP7\nHdPoTqZcg4TATucH+N5G+v5YPX3H7YFlQegu7+zZMcPJZgNpAWTvwTTct2LeoM3H9OCsBh4PILua\n0gZoon++gT+CLLcq3wgIoTHtk69ygxppTNepxjShr9zqwQyb/u4woSCA7Mpy3x/BvIlPDlTPfOf7\ndwptO9K1K5D/p6++nuAXtznI32t5RbqC1L2O0tEES8qlBerQ8tfdD9PA2+Mr85tDLLdA9+Tfy31f\n5vu0YIa1B9IdyjYrHsw92f/eUPK9OAjdq/yObZhRHB8DziDyvdL3qXxlrfy+BzNy5WXM/NVGfnHB\n1jX/32slvrdFldC9we94Wbm0QKNHVvg+44B/AF9hOoamEdw2J6HUtVXlvi/xfToJbqVeqWtS145W\nXQt5JfHaEMJuQG0LmOF2XTENSf/QCtgVQHYBvrddfnE235/TE0DWWUF8CgFeXVcgdwFB9Kz6nb/F\n74a3Cd9bF4zzGcjpjMf0+PyNcUBdvmv8AJwUhO4K/6wE99q/Kb7GN2ZFycuB3kHI3YVxWqZg9uIq\ncT4bAD8GWW6hbGVQ5ca0T77KDeoAN09pTFet3GqsMY0Z0ns/ZR/yjTAO97dB2P0HcFwFaduDKG9L\nubgRmDeHW4PQ/bvf8TOV+b38zivpkHoRiCXI0QSYVTzv9dXXTfgaV760YBo4d/rK/gxMT+9LmJ7t\nJwnQq364/yBmaNYgYFoQuhdjRiEMxXRMXeKLP53AHRQ/A6f5jgdj5kmXpAXj6IeyzcpfQIuq1DXf\nOYf8Fygdth9oufCVfsdvVlQPA1yjB+a+PNqX52Dr2ibMfOkhlGtEBqMbM0x/OmYo2cOYt0At8Q0N\nq0JdS8YMNzziWwzfub9hOr56A/sp7XxtF+h/4pNt6zvujt+zE7O4mtS1mqlrl9bxutYrDHUtpOdo\nbQlhN6C2BcxQvdMqSHsngGxz/N68lEs7Ndx5q2J5RAGtgzw3DjjJdzMKOOTNT659GPPXBePodayi\nfJVvBITQmPadU+UGNdKY9o+r9Y1pzBvhcZgOhkzMMJV1vrgjDk31yV8OdKgg7ZIAss8DZx0mfhDB\n7Vf0FIeZD4F5UH8YTJ3xkxmMaejtCfL8x8uFkiHgjQliSJHv3AHAe5gh3asxPcw3E2D+DfBuZfJ2\nGPmTMG/052BWGX4JMyx3DeXm41Qgu9RXVxaW/PaYTqnRQehu5cvzPuBPX9jn+3JJYgAABkBJREFU\nizvi8wC4nQo67AhuONNM/IZT+8XfCLgCyL5RQV1rCyysRNlbMI3pnwjQYesnM61caORX1+YHeY0R\nmM7P/Zghc2sx85biA8gF1dF4BPkzMfPz12GGoX2EcYr24TdXvwLZMzAjGP7CdPz28atrz1eirqX5\n6lmJXqlrFctMr4a6NrIW1rVAz6KSurbRV9dOrmRdC+k5WltC2A2QIKEuh3I3goxyN4LEALJVbkz7\nzqlyg5r625g+ZNPhcnLhbEyfSNnGdHtffLCN6Y7AWeV/Nw7T8DiC/JlVkT+C7Hk1rbu8PGbBpuOr\nwfZwlluwujuFoLtTiPWlD+YNTDJmi5UxwPlByvamdAhzZ0xHTVCyocpXIHsBfh1ElZDvB/yrErr7\nVGO+u2A6t45Wmfcpp7syv3ffUHT7XSfZF2ZWVtbvGkE9Q2pCvkQ22LpWTrYJkB7GfAecl1eDuv9H\nuU7oI5yr8FvALsTfq5/vPxZwiGdtCrIdgCDUEEqpkVrraUdbNhy6lVKRmCEMfxxL+T5aun1bZtyO\n6RToiln06DNf2nKtdfcA16+yvFLqTswqZFXVHap8OG0Pt+5RmE6hyuqusqzvnMep4vY0h5HtgxmK\nHOzWNlWWrwHdoeS7slv61KYyP5r5rvJWRoeRVcDAYGRDla8B3VD1fActWwO661K+l2qte/uOb8Tc\n2z/FjKL5Qms99kjytYZwe44SJNTXQCUWh6lOWdFd/3QTwoq1ocqHU3ddtr0e6K7S9jShyIruY1J3\nlbcywoy4CGUbpCrLV4PucOb7WC3zKq+eXpuCDUEQqoxSalVFSZi5bjUiK7qPOd0WrXUugNZ6i1Jq\nAPChUqqlTz4QociHU3ddtr0u63ZrrT1AvlLqb631Ad+1CpRS3hqUFd3Hnu6emIXCHgHu01qvVEoV\naK1/CEK2RwiyocqHqjuc+T5Wy9yilErEzCtUWus0AK11nlLKHeQ1wo44boIQGo2AczHzlvxRmMUo\nakpWdB9buvcqpbpqrVcCaK1zlVIXYjYyPSEIu0ORD6fuumx7XdZdrJSK0lrnYxpLACil4jFbgdSU\nrOg+xnRrrb3AeKXUB77PvQTZNg1FVnQfe7oxK6D/hnnmaqVUE631bqVUDMF1aNUOQnldJ0HCsR4I\nbRXSKsuK7mNLNyGuWBuKfDh112Xb67juKm9PE4qs6D72dB9GrlJbGVWXrOg+9nT7XSfo1dNrQ5DF\nSQRBEARBEARBEGo5lnAbIAiCIAiCIAiCIBwZcdwEQRAEQRAEQRBqOeK4CYIgCHUWpVSu77OVUurq\nar72w+W+B7OAjSAIgiDUCOK4CYIgCPWBVkClHDelVKAVyco4blrrUyppkyAIgiBUG+K4CYIgCPWB\nsUA/pdRKpdQ9SimrUuo/SqllSqlVSqlbAJRSA5RSPymlPgfW+uI+VUr9ppRao5S62Rc3Foj0XW+W\nL67k7Z7yXfsPpdRqpdSVftf+Xin1oVJqvVJqllKq7iwzLQiCINRqZB83QRAEoT7wIDBGa30hgM8B\ny9Za91JKOYFFSql5vnO7A8drrTf7vl+vtc5QSkUCy5RSH2mtH1RK3aG17noYXZcBXYGTMMueL1NK\n/ehL6wZ0AXYBi4BTgYXVn11BEAThWEPeuAmCIAj1kXOAa5VSK4FfgGTgOF/aUj+nDWC0Uup3YAmQ\n6ndeRZwGzNZae7TWe4EfgF5+196hzWaxKzFDOAVBEAQhZOSNmyAIglAfUcCdWuu5ZSKVGgDklft+\nFtBXa52vlPoeiAhBb5HfsQd5zgqCIAjVhLxxEwRBEOoDOUCs3/e5wG1KKTuAUqq9Uir6MHLxQKbP\naesInOyX5iqRL8dPwJW+eXQNgP7A0mrJhSAIgiBUgPQECoIgCPWBVYDHN+RxOvASZpjict8CIWnA\nJYeR+xq4VSm1DtiAGS5ZwmRglVJqudZ6uF/8J0Bf4HdAA/drrff4HD9BEARBqBGU1jrcNgiCIAiC\nIAiCIAhHQIZKCoIgCIIgCIIg1HLEcRMEQRAEQRAEQajliOMmCIIgCIIgCIJQyxHHTRAEQRAEQRAE\noZYjjpsgCIIgCIIgCEItRxw3QRAEQRAEQRCEWo44boIgCIIgCIIgCLUccdwEQRAEQRAEQRBqOf8P\nqYJf9zvrgRsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq-YzbJRqOMH",
        "colab_type": "text"
      },
      "source": [
        "## Generate Pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB2PSr3uEFw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMF87KVVEFuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "# fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "# fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "# gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "def sample_images(image_grid_rows=2, image_grid_columns=5):\n",
        "\n",
        "    # Sample random noise\n",
        "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # Get image labels 0-9\n",
        "    fake_labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "    fake_labels_category = to_categorical(fake_labels, num_classes=num_classes)\n",
        "\n",
        "    # Generate images from random noise\n",
        "    gen_imgs = generator.predict([z, fake_labels_category])\n",
        "\n",
        "    # Rescale image pixel values to [0, 1]\n",
        "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    # gen_imgs = (gen_imgs+1) * 255/2\n",
        "\n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows,\n",
        "                            image_grid_columns,\n",
        "                            figsize=(10, 4),\n",
        "                            sharey=True,\n",
        "                            sharex=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output a grid of images\n",
        "            axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')\n",
        "            axs[i, j].set_title(\"Class: \" + str(d_name[fake_labels[cnt]]))\n",
        "            cnt += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx4Ocr7DLE0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_images()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKcTHSEFEFq8",
        "colab_type": "code",
        "outputId": "f4cc8bb5-ec3d-480b-cdd4-b5c9eba5221d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "d_name = {0:\"airplane\", 1:\"automobile\", 2:\"bird\", 3:\"cat\", 4:\"deer\", 5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9:\"truck\"}\t\t\n",
        "i = 1\t\t\t\t\n",
        "plt.imshow(x_test[i],cmap='binary')\n",
        "print(y_test[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngfg/+Mxi6J4mGTIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgIT1giiqQU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images_labels_prediction(images,labels,prediction,idx,num=10):\n",
        "    fig=plt.gcf()\n",
        "    fig.set_size_inches(12,14)\n",
        "    if num>25: num=25\n",
        "    for i in range(0,num):\n",
        "        ax = plt.subplot(5,5,i+1)\n",
        "        ax.imshow(images[idx],cmap='binary') \n",
        "        title= str(i)+' '+label_dict[labels[i][0]]   #显示数字对应的类别\n",
        "        if len(prediction)>0:\n",
        "            title+= '=>'+label_dict[prediction[i]]   #显示数字对应的类别\n",
        "        ax.set_title(title,fontsize=10)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        idx+=1\n",
        "    plt.show()\n",
        "\n",
        "plot_images_labels_prediction(x_train,y_train,[],0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZNbvo9zqSZB",
        "colab_type": "text"
      },
      "source": [
        "# Pseudo Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYa-jKsACxRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pseudo_model = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "pseudo_model.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjVlrccQCxOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pseudo_train(iterations, batch_size, save_interval, alpha_f, t1, t2, iter_epochs):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # Get unlabeled examples and pseudo labels\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "        pseudo_label = pseudo_model.predict(imgs_unlabeled)\n",
        "\n",
        "        # -------------------------\n",
        "        #  Supervised Training\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs_labeled, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Train on labeled examples\n",
        "        alpha = 1\n",
        "        # loss_labeled, acc_labeled = pseudo_model.train_on_batch(imgs_labeled, labels)\n",
        "        datagen.fit(imgs_labeled)\n",
        "        pseudo_model.fit_generator(datagen.flow(imgs_labeled, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        loss_labeled, acc_labeled = history.losses[-1], history.accs[-1]\n",
        "\n",
        "\n",
        "        loss_unlabeled = -1\n",
        "        acc_unlabeled = -1\n",
        "\n",
        "        # -------------------------\n",
        "        #  Supervised Training\n",
        "        # -------------------------\n",
        "\n",
        "        # Set alpha\n",
        "        if iteration < t1: alpha = 0\n",
        "        else:\n",
        "            if t1 <= iteration < t2: alpha = (iteration - t1)/(t2 - t1) * alpha_f\n",
        "            else: alpha = alpha_f\n",
        "\n",
        "            # Train on unlabeled examples\n",
        "            loss_unlabeled, acc_unlabeled = pseudo_model.train_on_batch(imgs_unlabeled, pseudo_label)\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "          # Save losses\n",
        "          losses_pseudo_labeled.append(loss_labeled)\n",
        "          losses_pseudo_unlabeled.append(loss_unlabeled)\n",
        "          losses_pseudo.append(loss_labeled + alpha * loss_unlabeled)\n",
        "          accs_pseudo_labeled.append(acc_labeled)\n",
        "          accs_pseudo_unlabeled.append(acc_unlabeled)\n",
        "          accs_pseudo.append((acc_labeled + alpha*acc_unlabeled)/(1 + alpha))\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [supervised loss: %.4f, acc: %.2f%%] [unsupervised loss: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, loss_labeled, 100 * acc_labeled, \n",
        "                  loss_unlabeled, 100 * acc_unlabeled))\n",
        "          \n",
        "          pseudo_model.save(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\" + str(iteration+1) + \".h5\")\n",
        "          file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_supervised_losses.json\"\n",
        "          file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_unsupervised_losses.json\"\n",
        "          file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_losses.json\"\n",
        "          with open(file1, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo_labeled), json_file)\n",
        "          with open(file2, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo_unlabeled), json_file)\n",
        "          with open(file3, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GnU0n4JCxMB",
        "colab_type": "code",
        "outputId": "6b8aac6d-fc1f-482a-ef98-6aea6eafac81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 50 # 30\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "alpha_f = 3\n",
        "t1 = 2 # 500\n",
        "t2 = 4 # 1000\n",
        "iter_epochs = 10\n",
        "\n",
        "losses_pseudo_labeled = []\n",
        "losses_pseudo_unlabeled = []\n",
        "losses_pseudo = []\n",
        "accs_pseudo_labeled = []\n",
        "accs_pseudo_unlabeled = []\n",
        "accs_pseudo = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "pseudo_model = load_model(\"./models/cifar10_model.037.h5\")\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the SGGAN for the specified number of iterations\n",
        "pseudo_train(iterations, batch_size, save_interval, alpha_f, t1, t2, iter_epochs)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 17s 17s/step - loss: 0.3863 - acc: 0.9062 - val_loss: 0.5980 - val_acc: 0.8652\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86540\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3433 - acc: 1.0000 - val_loss: 0.6056 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86540\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3471 - acc: 0.9688 - val_loss: 0.6014 - val_acc: 0.8625\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86540\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4042 - acc: 0.8750 - val_loss: 0.5985 - val_acc: 0.8635\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86540\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2573 - acc: 1.0000 - val_loss: 0.5946 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86540\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2650 - acc: 1.0000 - val_loss: 0.5941 - val_acc: 0.8659\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.86540 to 0.86590, saving model to /content/models/cifar10_model.006.h5\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2566 - acc: 1.0000 - val_loss: 0.5930 - val_acc: 0.8652\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2772 - acc: 1.0000 - val_loss: 0.5957 - val_acc: 0.8648\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2318 - acc: 1.0000 - val_loss: 0.5978 - val_acc: 0.8638\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2242 - acc: 1.0000 - val_loss: 0.6000 - val_acc: 0.8634\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "1 [supervised loss: 0.2242, acc: 100.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4102 - acc: 0.9062 - val_loss: 0.6049 - val_acc: 0.8632\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3408 - acc: 0.9688 - val_loss: 0.6060 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3711 - acc: 0.9688 - val_loss: 0.6047 - val_acc: 0.8630\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3452 - acc: 0.9688 - val_loss: 0.6053 - val_acc: 0.8629\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2556 - acc: 1.0000 - val_loss: 0.6064 - val_acc: 0.8634\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2623 - acc: 1.0000 - val_loss: 0.6093 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2564 - acc: 1.0000 - val_loss: 0.6126 - val_acc: 0.8607\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2465 - acc: 1.0000 - val_loss: 0.6181 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2259 - acc: 1.0000 - val_loss: 0.6237 - val_acc: 0.8571\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2329 - acc: 1.0000 - val_loss: 0.6295 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "2 [supervised loss: 0.2329, acc: 100.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2812 - acc: 0.9688 - val_loss: 0.6351 - val_acc: 0.8530\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2791 - acc: 1.0000 - val_loss: 0.6403 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3125 - acc: 0.9688 - val_loss: 0.6453 - val_acc: 0.8517\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2989 - acc: 0.9375 - val_loss: 0.6495 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2759 - acc: 0.9688 - val_loss: 0.6536 - val_acc: 0.8485\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2391 - acc: 1.0000 - val_loss: 0.6586 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2226 - acc: 1.0000 - val_loss: 0.6627 - val_acc: 0.8460\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2187 - acc: 1.0000 - val_loss: 0.6656 - val_acc: 0.8457\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2387 - acc: 1.0000 - val_loss: 0.6686 - val_acc: 0.8460\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2116 - acc: 1.0000 - val_loss: 0.6715 - val_acc: 0.8445\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "3 [supervised loss: 0.2116, acc: 100.00%] [unsupervised loss: 0.5881, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2652 - acc: 1.0000 - val_loss: 0.6691 - val_acc: 0.8458\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 0.6673 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2350 - acc: 1.0000 - val_loss: 0.6662 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2553 - acc: 1.0000 - val_loss: 0.6639 - val_acc: 0.8456\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2295 - acc: 1.0000 - val_loss: 0.6626 - val_acc: 0.8461\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2225 - acc: 1.0000 - val_loss: 0.6624 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2147 - acc: 1.0000 - val_loss: 0.6631 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2246 - acc: 1.0000 - val_loss: 0.6634 - val_acc: 0.8481\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2152 - acc: 1.0000 - val_loss: 0.6639 - val_acc: 0.8492\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.6657 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "4 [supervised loss: 0.2105, acc: 100.00%] [unsupervised loss: 0.8170, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4842 - acc: 0.9062 - val_loss: 0.6690 - val_acc: 0.8473\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5064 - acc: 0.8438 - val_loss: 0.6672 - val_acc: 0.8476\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2853 - acc: 1.0000 - val_loss: 0.6654 - val_acc: 0.8488\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2639 - acc: 1.0000 - val_loss: 0.6648 - val_acc: 0.8492\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2442 - acc: 1.0000 - val_loss: 0.6666 - val_acc: 0.8484\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2222 - acc: 1.0000 - val_loss: 0.6694 - val_acc: 0.8473\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2347 - acc: 1.0000 - val_loss: 0.6735 - val_acc: 0.8457\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2106 - acc: 1.0000 - val_loss: 0.6785 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2215 - acc: 1.0000 - val_loss: 0.6846 - val_acc: 0.8441\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2179 - acc: 1.0000 - val_loss: 0.6903 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "5 [supervised loss: 0.2179, acc: 100.00%] [unsupervised loss: 0.4710, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2813 - acc: 0.9688 - val_loss: 0.6918 - val_acc: 0.8413\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3661 - acc: 0.9375 - val_loss: 0.6913 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2703 - acc: 1.0000 - val_loss: 0.6912 - val_acc: 0.8407\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2545 - acc: 1.0000 - val_loss: 0.6925 - val_acc: 0.8411\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2491 - acc: 1.0000 - val_loss: 0.6952 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2204 - acc: 1.0000 - val_loss: 0.6995 - val_acc: 0.8399\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2218 - acc: 1.0000 - val_loss: 0.7047 - val_acc: 0.8379\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2140 - acc: 1.0000 - val_loss: 0.7107 - val_acc: 0.8352\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2142 - acc: 1.0000 - val_loss: 0.7177 - val_acc: 0.8341\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2110 - acc: 1.0000 - val_loss: 0.7242 - val_acc: 0.8326\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "6 [supervised loss: 0.2110, acc: 100.00%] [unsupervised loss: 0.7174, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3937 - acc: 0.9062 - val_loss: 0.7332 - val_acc: 0.8285\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3563 - acc: 0.9688 - val_loss: 0.7315 - val_acc: 0.8293\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3050 - acc: 0.9688 - val_loss: 0.7291 - val_acc: 0.8313\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3045 - acc: 0.9688 - val_loss: 0.7276 - val_acc: 0.8326\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3034 - acc: 0.9375 - val_loss: 0.7297 - val_acc: 0.8329\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3854 - acc: 0.9375 - val_loss: 0.7361 - val_acc: 0.8324\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2571 - acc: 1.0000 - val_loss: 0.7463 - val_acc: 0.8294\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2324 - acc: 1.0000 - val_loss: 0.7585 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2159 - acc: 1.0000 - val_loss: 0.7721 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2183 - acc: 1.0000 - val_loss: 0.7862 - val_acc: 0.8207\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "7 [supervised loss: 0.2183, acc: 100.00%] [unsupervised loss: 0.5661, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2308 - acc: 1.0000 - val_loss: 0.8137 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2480 - acc: 1.0000 - val_loss: 0.8273 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2774 - acc: 0.9688 - val_loss: 0.8364 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2892 - acc: 0.9688 - val_loss: 0.8417 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2182 - acc: 1.0000 - val_loss: 0.8464 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2200 - acc: 1.0000 - val_loss: 0.8511 - val_acc: 0.8053\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2549 - acc: 0.9688 - val_loss: 0.8546 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 0.8579 - val_acc: 0.8028\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 0.8605 - val_acc: 0.8023\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2143 - acc: 1.0000 - val_loss: 0.8629 - val_acc: 0.8012\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "8 [supervised loss: 0.2143, acc: 100.00%] [unsupervised loss: 0.8016, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2503 - acc: 0.9688 - val_loss: 0.8754 - val_acc: 0.7983\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - acc: 1.0000 - val_loss: 0.8816 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2358 - acc: 1.0000 - val_loss: 0.8856 - val_acc: 0.7939\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2290 - acc: 1.0000 - val_loss: 0.8868 - val_acc: 0.7934\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 0.8871 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2338 - acc: 1.0000 - val_loss: 0.8860 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2168 - acc: 1.0000 - val_loss: 0.8839 - val_acc: 0.7944\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2174 - acc: 1.0000 - val_loss: 0.8816 - val_acc: 0.7945\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2115 - acc: 1.0000 - val_loss: 0.8785 - val_acc: 0.7956\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2154 - acc: 1.0000 - val_loss: 0.8748 - val_acc: 0.7976\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "9 [supervised loss: 0.2154, acc: 100.00%] [unsupervised loss: 0.6819, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2525 - acc: 1.0000 - val_loss: 0.8730 - val_acc: 0.7981\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2463 - acc: 1.0000 - val_loss: 0.8699 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2740 - acc: 0.9688 - val_loss: 0.8644 - val_acc: 0.8001\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2553 - acc: 0.9688 - val_loss: 0.8578 - val_acc: 0.8012\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2294 - acc: 1.0000 - val_loss: 0.8515 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2155 - acc: 1.0000 - val_loss: 0.8469 - val_acc: 0.8036\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2225 - acc: 1.0000 - val_loss: 0.8430 - val_acc: 0.8053\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2104 - acc: 1.0000 - val_loss: 0.8403 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2155 - acc: 1.0000 - val_loss: 0.8380 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2123 - acc: 1.0000 - val_loss: 0.8365 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "10 [supervised loss: 0.2123, acc: 100.00%] [unsupervised loss: 0.6380, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4252 - acc: 0.8750 - val_loss: 0.8420 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3430 - acc: 0.9062 - val_loss: 0.8454 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3078 - acc: 0.9688 - val_loss: 0.8493 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2962 - acc: 0.9688 - val_loss: 0.8550 - val_acc: 0.8011\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3297 - acc: 0.9375 - val_loss: 0.8604 - val_acc: 0.7993\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2510 - acc: 1.0000 - val_loss: 0.8670 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2363 - acc: 1.0000 - val_loss: 0.8734 - val_acc: 0.7967\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2160 - acc: 1.0000 - val_loss: 0.8794 - val_acc: 0.7953\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 0.8856 - val_acc: 0.7939\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2130 - acc: 1.0000 - val_loss: 0.8915 - val_acc: 0.7932\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "11 [supervised loss: 0.2130, acc: 100.00%] [unsupervised loss: 0.6975, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3537 - acc: 0.9375 - val_loss: 0.9047 - val_acc: 0.7899\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3138 - acc: 0.9375 - val_loss: 0.9114 - val_acc: 0.7887\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3124 - acc: 0.9375 - val_loss: 0.9108 - val_acc: 0.7884\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3064 - acc: 0.9688 - val_loss: 0.9037 - val_acc: 0.7898\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2588 - acc: 1.0000 - val_loss: 0.8955 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2437 - acc: 1.0000 - val_loss: 0.8892 - val_acc: 0.7900\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2550 - acc: 1.0000 - val_loss: 0.8838 - val_acc: 0.7900\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2462 - acc: 1.0000 - val_loss: 0.8788 - val_acc: 0.7923\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2151 - acc: 1.0000 - val_loss: 0.8748 - val_acc: 0.7935\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2145 - acc: 1.0000 - val_loss: 0.8723 - val_acc: 0.7941\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "12 [supervised loss: 0.2145, acc: 100.00%] [unsupervised loss: 0.6879, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2918 - acc: 0.9688 - val_loss: 0.8765 - val_acc: 0.7945\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3290 - acc: 0.9688 - val_loss: 0.8801 - val_acc: 0.7940\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2750 - acc: 0.9688 - val_loss: 0.8833 - val_acc: 0.7938\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2328 - acc: 1.0000 - val_loss: 0.8875 - val_acc: 0.7931\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2224 - acc: 1.0000 - val_loss: 0.8908 - val_acc: 0.7921\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2157 - acc: 1.0000 - val_loss: 0.8950 - val_acc: 0.7898\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2170 - acc: 1.0000 - val_loss: 0.8996 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2080 - acc: 1.0000 - val_loss: 0.9032 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2120 - acc: 1.0000 - val_loss: 0.9070 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2109 - acc: 1.0000 - val_loss: 0.9103 - val_acc: 0.7839\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "13 [supervised loss: 0.2109, acc: 100.00%] [unsupervised loss: 0.7711, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2843 - acc: 0.9375 - val_loss: 0.9269 - val_acc: 0.7787\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2713 - acc: 0.9688 - val_loss: 0.9264 - val_acc: 0.7803\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2352 - acc: 1.0000 - val_loss: 0.9245 - val_acc: 0.7808\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2391 - acc: 1.0000 - val_loss: 0.9230 - val_acc: 0.7817\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2946 - acc: 0.9375 - val_loss: 0.9229 - val_acc: 0.7824\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2258 - acc: 1.0000 - val_loss: 0.9212 - val_acc: 0.7835\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.9216 - val_acc: 0.7830\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2111 - acc: 1.0000 - val_loss: 0.9212 - val_acc: 0.7829\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2099 - acc: 1.0000 - val_loss: 0.9218 - val_acc: 0.7835\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2072 - acc: 1.0000 - val_loss: 0.9227 - val_acc: 0.7832\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "14 [supervised loss: 0.2072, acc: 100.00%] [unsupervised loss: 0.8685, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2055 - acc: 1.0000 - val_loss: 0.9215 - val_acc: 0.7827\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2071 - acc: 1.0000 - val_loss: 0.9210 - val_acc: 0.7830\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 0.9206 - val_acc: 0.7846\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2175 - acc: 1.0000 - val_loss: 0.9176 - val_acc: 0.7852\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2077 - acc: 1.0000 - val_loss: 0.9143 - val_acc: 0.7859\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2272 - acc: 1.0000 - val_loss: 0.9085 - val_acc: 0.7876\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2252 - acc: 1.0000 - val_loss: 0.9018 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2022 - acc: 1.0000 - val_loss: 0.8953 - val_acc: 0.7902\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2032 - acc: 1.0000 - val_loss: 0.8894 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2015 - acc: 1.0000 - val_loss: 0.8840 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "15 [supervised loss: 0.2015, acc: 100.00%] [unsupervised loss: 0.7075, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 0.8808 - val_acc: 0.7937\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2080 - acc: 1.0000 - val_loss: 0.8781 - val_acc: 0.7941\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2155 - acc: 1.0000 - val_loss: 0.8726 - val_acc: 0.7955\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2118 - acc: 1.0000 - val_loss: 0.8671 - val_acc: 0.7968\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2055 - acc: 1.0000 - val_loss: 0.8612 - val_acc: 0.7987\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2042 - acc: 1.0000 - val_loss: 0.8556 - val_acc: 0.7998\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2048 - acc: 1.0000 - val_loss: 0.8499 - val_acc: 0.8005\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 0.8430 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 0.8371 - val_acc: 0.8036\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2062 - acc: 1.0000 - val_loss: 0.8307 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "16 [supervised loss: 0.2062, acc: 100.00%] [unsupervised loss: 0.5367, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2340 - acc: 1.0000 - val_loss: 0.8237 - val_acc: 0.8072\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2409 - acc: 1.0000 - val_loss: 0.8212 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2158 - acc: 1.0000 - val_loss: 0.8190 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2264 - acc: 1.0000 - val_loss: 0.8174 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2210 - acc: 1.0000 - val_loss: 0.8159 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2182 - acc: 1.0000 - val_loss: 0.8142 - val_acc: 0.8097\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2192 - acc: 1.0000 - val_loss: 0.8128 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2129 - acc: 1.0000 - val_loss: 0.8116 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2073 - acc: 1.0000 - val_loss: 0.8106 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2121 - acc: 1.0000 - val_loss: 0.8089 - val_acc: 0.8116\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "17 [supervised loss: 0.2121, acc: 100.00%] [unsupervised loss: 0.7278, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2107 - acc: 1.0000 - val_loss: 0.8120 - val_acc: 0.8096\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 0.8138 - val_acc: 0.8091\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2453 - acc: 0.9688 - val_loss: 0.8113 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2200 - acc: 1.0000 - val_loss: 0.8080 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2055 - acc: 1.0000 - val_loss: 0.8053 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2047 - acc: 1.0000 - val_loss: 0.8032 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2055 - acc: 1.0000 - val_loss: 0.8013 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2103 - acc: 1.0000 - val_loss: 0.7996 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2054 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2041 - acc: 1.0000 - val_loss: 0.7969 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "18 [supervised loss: 0.2041, acc: 100.00%] [unsupervised loss: 0.6255, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2143 - acc: 1.0000 - val_loss: 0.8061 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2171 - acc: 1.0000 - val_loss: 0.8116 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2084 - acc: 1.0000 - val_loss: 0.8162 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2164 - acc: 1.0000 - val_loss: 0.8205 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2080 - acc: 1.0000 - val_loss: 0.8250 - val_acc: 0.8044\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2062 - acc: 1.0000 - val_loss: 0.8289 - val_acc: 0.8038\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2077 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.8037\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2076 - acc: 1.0000 - val_loss: 0.8352 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2003 - acc: 1.0000 - val_loss: 0.8379 - val_acc: 0.8031\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2078 - acc: 1.0000 - val_loss: 0.8404 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "19 [supervised loss: 0.2078, acc: 100.00%] [unsupervised loss: 0.8913, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2078 - acc: 1.0000 - val_loss: 0.8655 - val_acc: 0.7966\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2107 - acc: 1.0000 - val_loss: 0.8779 - val_acc: 0.7935\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2097 - acc: 1.0000 - val_loss: 0.8889 - val_acc: 0.7916\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2092 - acc: 1.0000 - val_loss: 0.9001 - val_acc: 0.7891\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2025 - acc: 1.0000 - val_loss: 0.9104 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2044 - acc: 1.0000 - val_loss: 0.9202 - val_acc: 0.7841\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2024 - acc: 1.0000 - val_loss: 0.9291 - val_acc: 0.7817\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2030 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 0.7797\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2011 - acc: 1.0000 - val_loss: 0.9445 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2037 - acc: 1.0000 - val_loss: 0.9510 - val_acc: 0.7766\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "20 [supervised loss: 0.2037, acc: 100.00%] [unsupervised loss: 0.7241, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2048 - acc: 1.0000 - val_loss: 0.9749 - val_acc: 0.7725\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2072 - acc: 1.0000 - val_loss: 0.9846 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2052 - acc: 1.0000 - val_loss: 0.9926 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2120 - acc: 1.0000 - val_loss: 0.9995 - val_acc: 0.7670\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2094 - acc: 1.0000 - val_loss: 1.0047 - val_acc: 0.7661\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 1.0070 - val_acc: 0.7654\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2046 - acc: 1.0000 - val_loss: 1.0082 - val_acc: 0.7657\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 1.0000 - val_loss: 1.0086 - val_acc: 0.7668\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 1.0068 - val_acc: 0.7674\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2028 - acc: 1.0000 - val_loss: 1.0054 - val_acc: 0.7674\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "21 [supervised loss: 0.2028, acc: 100.00%] [unsupervised loss: 0.8058, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2267 - acc: 1.0000 - val_loss: 1.0510 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2179 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.7528\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2161 - acc: 1.0000 - val_loss: 1.0762 - val_acc: 0.7499\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2116 - acc: 1.0000 - val_loss: 1.0836 - val_acc: 0.7493\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2080 - acc: 1.0000 - val_loss: 1.0873 - val_acc: 0.7480\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2099 - acc: 1.0000 - val_loss: 1.0882 - val_acc: 0.7486\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2077 - acc: 1.0000 - val_loss: 1.0864 - val_acc: 0.7494\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2073 - acc: 1.0000 - val_loss: 1.0829 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2216 - acc: 1.0000 - val_loss: 1.0766 - val_acc: 0.7512\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2044 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.7531\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "22 [supervised loss: 0.2044, acc: 100.00%] [unsupervised loss: 0.9709, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2050 - acc: 1.0000 - val_loss: 1.1207 - val_acc: 0.7434\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2104 - acc: 1.0000 - val_loss: 1.1435 - val_acc: 0.7395\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2250 - acc: 1.0000 - val_loss: 1.1545 - val_acc: 0.7380\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2098 - acc: 1.0000 - val_loss: 1.1628 - val_acc: 0.7385\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 1.1585 - val_acc: 0.7399\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2058 - acc: 1.0000 - val_loss: 1.1549 - val_acc: 0.7409\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2046 - acc: 1.0000 - val_loss: 1.1532 - val_acc: 0.7412\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2051 - acc: 1.0000 - val_loss: 1.1511 - val_acc: 0.7410\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1999 - acc: 1.0000 - val_loss: 1.1492 - val_acc: 0.7413\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2046 - acc: 1.0000 - val_loss: 1.1474 - val_acc: 0.7411\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "23 [supervised loss: 0.2046, acc: 100.00%] [unsupervised loss: 0.7639, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2495 - acc: 0.9688 - val_loss: 1.2298 - val_acc: 0.7255\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2018 - acc: 1.0000 - val_loss: 1.2593 - val_acc: 0.7198\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2094 - acc: 1.0000 - val_loss: 1.2792 - val_acc: 0.7166\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 1.2966 - val_acc: 0.7138\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2044 - acc: 1.0000 - val_loss: 1.3110 - val_acc: 0.7120\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 1.3241 - val_acc: 0.7092\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 1.3338 - val_acc: 0.7084\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2021 - acc: 1.0000 - val_loss: 1.3419 - val_acc: 0.7068\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2022 - acc: 1.0000 - val_loss: 1.3498 - val_acc: 0.7047\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2017 - acc: 1.0000 - val_loss: 1.3554 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "24 [supervised loss: 0.2017, acc: 100.00%] [unsupervised loss: 0.8762, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2154 - acc: 1.0000 - val_loss: 1.4710 - val_acc: 0.6858\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2577 - acc: 0.9688 - val_loss: 1.4966 - val_acc: 0.6836\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2767 - acc: 0.9688 - val_loss: 1.4988 - val_acc: 0.6838\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2883 - acc: 0.9688 - val_loss: 1.4712 - val_acc: 0.6897\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2445 - acc: 0.9688 - val_loss: 1.4247 - val_acc: 0.6983\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2037 - acc: 1.0000 - val_loss: 1.3818 - val_acc: 0.7056\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2062 - acc: 1.0000 - val_loss: 1.3472 - val_acc: 0.7119\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2035 - acc: 1.0000 - val_loss: 1.3173 - val_acc: 0.7171\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2058 - acc: 1.0000 - val_loss: 1.2915 - val_acc: 0.7220\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1998 - acc: 1.0000 - val_loss: 1.2688 - val_acc: 0.7263\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "25 [supervised loss: 0.1998, acc: 100.00%] [unsupervised loss: 0.7082, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2132 - acc: 1.0000 - val_loss: 1.2844 - val_acc: 0.7248\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2036 - acc: 1.0000 - val_loss: 1.2880 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2065 - acc: 1.0000 - val_loss: 1.2917 - val_acc: 0.7247\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2076 - acc: 1.0000 - val_loss: 1.2917 - val_acc: 0.7247\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2044 - acc: 1.0000 - val_loss: 1.2896 - val_acc: 0.7248\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2084 - acc: 1.0000 - val_loss: 1.2853 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2060 - acc: 1.0000 - val_loss: 1.2809 - val_acc: 0.7278\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2011 - acc: 1.0000 - val_loss: 1.2757 - val_acc: 0.7293\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2097 - acc: 1.0000 - val_loss: 1.2673 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2033 - acc: 1.0000 - val_loss: 1.2596 - val_acc: 0.7324\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "26 [supervised loss: 0.2033, acc: 100.00%] [unsupervised loss: 0.7602, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2455 - acc: 0.9688 - val_loss: 1.2792 - val_acc: 0.7304\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2059 - acc: 1.0000 - val_loss: 1.2803 - val_acc: 0.7301\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 1.2801 - val_acc: 0.7301\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2175 - acc: 1.0000 - val_loss: 1.2727 - val_acc: 0.7322\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2061 - acc: 1.0000 - val_loss: 1.2647 - val_acc: 0.7338\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2051 - acc: 1.0000 - val_loss: 1.2563 - val_acc: 0.7356\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 1.2487 - val_acc: 0.7378\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2039 - acc: 1.0000 - val_loss: 1.2406 - val_acc: 0.7385\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2111 - acc: 1.0000 - val_loss: 1.2305 - val_acc: 0.7390\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2002 - acc: 1.0000 - val_loss: 1.2204 - val_acc: 0.7409\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "27 [supervised loss: 0.2002, acc: 100.00%] [unsupervised loss: 0.6367, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2197 - acc: 1.0000 - val_loss: 1.2167 - val_acc: 0.7430\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2375 - acc: 1.0000 - val_loss: 1.2084 - val_acc: 0.7434\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2509 - acc: 0.9688 - val_loss: 1.1975 - val_acc: 0.7453\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2159 - acc: 1.0000 - val_loss: 1.1882 - val_acc: 0.7457\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 1.1802 - val_acc: 0.7469\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2113 - acc: 1.0000 - val_loss: 1.1715 - val_acc: 0.7482\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2069 - acc: 1.0000 - val_loss: 1.1638 - val_acc: 0.7493\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2062 - acc: 1.0000 - val_loss: 1.1567 - val_acc: 0.7506\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2027 - acc: 1.0000 - val_loss: 1.1501 - val_acc: 0.7514\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2027 - acc: 1.0000 - val_loss: 1.1426 - val_acc: 0.7522\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "28 [supervised loss: 0.2027, acc: 100.00%] [unsupervised loss: 0.9944, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2143 - acc: 1.0000 - val_loss: 1.1692 - val_acc: 0.7457\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2078 - acc: 1.0000 - val_loss: 1.1781 - val_acc: 0.7437\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2188 - acc: 1.0000 - val_loss: 1.1826 - val_acc: 0.7421\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2106 - acc: 1.0000 - val_loss: 1.1860 - val_acc: 0.7409\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2219 - acc: 1.0000 - val_loss: 1.1771 - val_acc: 0.7428\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2066 - acc: 1.0000 - val_loss: 1.1686 - val_acc: 0.7436\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2038 - acc: 1.0000 - val_loss: 1.1614 - val_acc: 0.7432\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2051 - acc: 1.0000 - val_loss: 1.1536 - val_acc: 0.7456\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2078 - acc: 1.0000 - val_loss: 1.1450 - val_acc: 0.7469\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 1.1358 - val_acc: 0.7490\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "29 [supervised loss: 0.2067, acc: 100.00%] [unsupervised loss: 0.9705, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2081 - acc: 1.0000 - val_loss: 1.2398 - val_acc: 0.7288\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 1.2923 - val_acc: 0.7186\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2111 - acc: 1.0000 - val_loss: 1.3419 - val_acc: 0.7071\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2111 - acc: 1.0000 - val_loss: 1.3841 - val_acc: 0.6992\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 1.0000 - val_loss: 1.4194 - val_acc: 0.6937\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2212 - acc: 1.0000 - val_loss: 1.4371 - val_acc: 0.6899\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2062 - acc: 1.0000 - val_loss: 1.4471 - val_acc: 0.6883\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2229 - acc: 1.0000 - val_loss: 1.4431 - val_acc: 0.6893\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2099 - acc: 1.0000 - val_loss: 1.4309 - val_acc: 0.6913\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 1.4164 - val_acc: 0.6946\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "30 [supervised loss: 0.2049, acc: 100.00%] [unsupervised loss: 0.7869, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2500 - acc: 1.0000 - val_loss: 1.4367 - val_acc: 0.6915\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2273 - acc: 1.0000 - val_loss: 1.4358 - val_acc: 0.6923\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2186 - acc: 1.0000 - val_loss: 1.4298 - val_acc: 0.6942\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2046 - acc: 1.0000 - val_loss: 1.4217 - val_acc: 0.6953\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2024 - acc: 1.0000 - val_loss: 1.4125 - val_acc: 0.6966\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 1.4012 - val_acc: 0.6986\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2011 - acc: 1.0000 - val_loss: 1.3902 - val_acc: 0.7017\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2000 - acc: 1.0000 - val_loss: 1.3792 - val_acc: 0.7032\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2030 - acc: 1.0000 - val_loss: 1.3669 - val_acc: 0.7053\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2013 - acc: 1.0000 - val_loss: 1.3564 - val_acc: 0.7071\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "31 [supervised loss: 0.2013, acc: 100.00%] [unsupervised loss: 0.8429, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2014 - acc: 1.0000 - val_loss: 1.4682 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2050 - acc: 1.0000 - val_loss: 1.5232 - val_acc: 0.6795\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1997 - acc: 1.0000 - val_loss: 1.5743 - val_acc: 0.6716\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2002 - acc: 1.0000 - val_loss: 1.6229 - val_acc: 0.6642\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1989 - acc: 1.0000 - val_loss: 1.6645 - val_acc: 0.6574\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1980 - acc: 1.0000 - val_loss: 1.7041 - val_acc: 0.6521\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 1.7372 - val_acc: 0.6482\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1995 - acc: 1.0000 - val_loss: 1.7669 - val_acc: 0.6444\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 1.7866 - val_acc: 0.6414\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2026 - acc: 1.0000 - val_loss: 1.7983 - val_acc: 0.6400\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "32 [supervised loss: 0.2026, acc: 100.00%] [unsupervised loss: 0.9424, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2507 - acc: 1.0000 - val_loss: 1.9607 - val_acc: 0.6171\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2379 - acc: 1.0000 - val_loss: 1.9854 - val_acc: 0.6137\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2552 - acc: 0.9688 - val_loss: 1.9757 - val_acc: 0.6142\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2364 - acc: 1.0000 - val_loss: 1.9308 - val_acc: 0.6186\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2462 - acc: 0.9688 - val_loss: 1.8400 - val_acc: 0.6283\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2031 - acc: 1.0000 - val_loss: 1.7597 - val_acc: 0.6383\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2199 - acc: 1.0000 - val_loss: 1.6814 - val_acc: 0.6471\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2118 - acc: 1.0000 - val_loss: 1.6073 - val_acc: 0.6569\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2016 - acc: 1.0000 - val_loss: 1.5473 - val_acc: 0.6679\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2012 - acc: 1.0000 - val_loss: 1.4969 - val_acc: 0.6756\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "33 [supervised loss: 0.2012, acc: 100.00%] [unsupervised loss: 1.5216, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2226 - acc: 1.0000 - val_loss: 1.8449 - val_acc: 0.6229\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1984 - acc: 1.0000 - val_loss: 2.0367 - val_acc: 0.5994\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2120 - acc: 1.0000 - val_loss: 2.2139 - val_acc: 0.5805\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2004 - acc: 1.0000 - val_loss: 2.3917 - val_acc: 0.5602\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2047 - acc: 1.0000 - val_loss: 2.5466 - val_acc: 0.5434\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2031 - acc: 1.0000 - val_loss: 2.6829 - val_acc: 0.5306\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2041 - acc: 1.0000 - val_loss: 2.7940 - val_acc: 0.5207\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1984 - acc: 1.0000 - val_loss: 2.8931 - val_acc: 0.5103\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2023 - acc: 1.0000 - val_loss: 2.9653 - val_acc: 0.5036\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2070 - acc: 1.0000 - val_loss: 3.0280 - val_acc: 0.4994\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "34 [supervised loss: 0.2070, acc: 100.00%] [unsupervised loss: 0.7494, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2249 - acc: 1.0000 - val_loss: 3.8321 - val_acc: 0.4419\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2313 - acc: 1.0000 - val_loss: 4.0891 - val_acc: 0.4276\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2476 - acc: 1.0000 - val_loss: 4.2609 - val_acc: 0.4180\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2294 - acc: 1.0000 - val_loss: 4.3774 - val_acc: 0.4123\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2761 - acc: 0.9688 - val_loss: 4.4215 - val_acc: 0.4102\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2160 - acc: 1.0000 - val_loss: 4.4397 - val_acc: 0.4108\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2460 - acc: 0.9688 - val_loss: 4.3601 - val_acc: 0.4166\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2280 - acc: 1.0000 - val_loss: 4.1858 - val_acc: 0.4279\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2132 - acc: 1.0000 - val_loss: 4.0001 - val_acc: 0.4409\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2032 - acc: 1.0000 - val_loss: 3.8199 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "35 [supervised loss: 0.2032, acc: 100.00%] [unsupervised loss: 2.5141, acc: 40.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2750 - acc: 0.9688 - val_loss: 5.1958 - val_acc: 0.3721\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2887 - acc: 0.9688 - val_loss: 5.4826 - val_acc: 0.3604\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2145 - acc: 1.0000 - val_loss: 5.6837 - val_acc: 0.3538\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2661 - acc: 0.9688 - val_loss: 5.6672 - val_acc: 0.3547\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2857 - acc: 0.9375 - val_loss: 5.5064 - val_acc: 0.3618\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2603 - acc: 0.9688 - val_loss: 5.1675 - val_acc: 0.3796\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2101 - acc: 1.0000 - val_loss: 4.8724 - val_acc: 0.3981\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2108 - acc: 1.0000 - val_loss: 4.5917 - val_acc: 0.4152\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2216 - acc: 1.0000 - val_loss: 4.3179 - val_acc: 0.4326\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2153 - acc: 1.0000 - val_loss: 4.0716 - val_acc: 0.4506\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "36 [supervised loss: 0.2153, acc: 100.00%] [unsupervised loss: 1.9286, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8912 - acc: 0.9062 - val_loss: 3.9965 - val_acc: 0.4544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5669 - acc: 0.9375 - val_loss: 3.5579 - val_acc: 0.4748\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5984 - acc: 0.9062 - val_loss: 2.9764 - val_acc: 0.5078\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2434 - acc: 1.0000 - val_loss: 2.5103 - val_acc: 0.5401\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2408 - acc: 1.0000 - val_loss: 2.1838 - val_acc: 0.5705\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2343 - acc: 0.9688 - val_loss: 1.9373 - val_acc: 0.5961\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2097 - acc: 1.0000 - val_loss: 1.7795 - val_acc: 0.6143\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2138 - acc: 1.0000 - val_loss: 1.6802 - val_acc: 0.6262\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2075 - acc: 1.0000 - val_loss: 1.6153 - val_acc: 0.6367\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 1.5736 - val_acc: 0.6458\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "37 [supervised loss: 0.2067, acc: 100.00%] [unsupervised loss: 2.2769, acc: 50.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2345 - acc: 1.0000 - val_loss: 1.7094 - val_acc: 0.6184\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3304 - acc: 0.9375 - val_loss: 1.7582 - val_acc: 0.6082\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2645 - acc: 1.0000 - val_loss: 1.8020 - val_acc: 0.6012\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 1.8488 - val_acc: 0.5947\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2473 - acc: 1.0000 - val_loss: 1.8920 - val_acc: 0.5877\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2212 - acc: 1.0000 - val_loss: 1.9323 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2362 - acc: 1.0000 - val_loss: 1.9663 - val_acc: 0.5782\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2312 - acc: 1.0000 - val_loss: 1.9953 - val_acc: 0.5721\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2245 - acc: 1.0000 - val_loss: 2.0155 - val_acc: 0.5656\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2184 - acc: 1.0000 - val_loss: 2.0230 - val_acc: 0.5634\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "38 [supervised loss: 0.2184, acc: 100.00%] [unsupervised loss: 0.9966, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2480 - acc: 1.0000 - val_loss: 1.9631 - val_acc: 0.5736\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3173 - acc: 0.9375 - val_loss: 1.8542 - val_acc: 0.5916\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2470 - acc: 1.0000 - val_loss: 1.7471 - val_acc: 0.6081\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2275 - acc: 1.0000 - val_loss: 1.6600 - val_acc: 0.6242\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2183 - acc: 1.0000 - val_loss: 1.5953 - val_acc: 0.6387\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2142 - acc: 1.0000 - val_loss: 1.5479 - val_acc: 0.6487\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2143 - acc: 1.0000 - val_loss: 1.5138 - val_acc: 0.6573\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2054 - acc: 1.0000 - val_loss: 1.4906 - val_acc: 0.6598\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2079 - acc: 1.0000 - val_loss: 1.4733 - val_acc: 0.6609\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2007 - acc: 1.0000 - val_loss: 1.4615 - val_acc: 0.6635\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "39 [supervised loss: 0.2007, acc: 100.00%] [unsupervised loss: 2.1522, acc: 53.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2664 - acc: 0.9688 - val_loss: 1.5611 - val_acc: 0.6468\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2490 - acc: 1.0000 - val_loss: 1.6153 - val_acc: 0.6382\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2395 - acc: 1.0000 - val_loss: 1.6706 - val_acc: 0.6279\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2195 - acc: 1.0000 - val_loss: 1.7245 - val_acc: 0.6201\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2369 - acc: 1.0000 - val_loss: 1.7767 - val_acc: 0.6119\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2405 - acc: 1.0000 - val_loss: 1.8255 - val_acc: 0.6038\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2453 - acc: 0.9688 - val_loss: 1.8696 - val_acc: 0.5953\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2098 - acc: 1.0000 - val_loss: 1.9107 - val_acc: 0.5899\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2121 - acc: 1.0000 - val_loss: 1.9468 - val_acc: 0.5839\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2140 - acc: 1.0000 - val_loss: 1.9780 - val_acc: 0.5804\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "40 [supervised loss: 0.2140, acc: 100.00%] [unsupervised loss: 1.1751, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2054 - acc: 1.0000 - val_loss: 2.1465 - val_acc: 0.5576\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2674 - acc: 0.9688 - val_loss: 2.2152 - val_acc: 0.5463\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2133 - acc: 1.0000 - val_loss: 2.2767 - val_acc: 0.5382\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2030 - acc: 1.0000 - val_loss: 2.3340 - val_acc: 0.5306\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2191 - acc: 1.0000 - val_loss: 2.3755 - val_acc: 0.5249\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2313 - acc: 1.0000 - val_loss: 2.4169 - val_acc: 0.5204\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2068 - acc: 1.0000 - val_loss: 2.4496 - val_acc: 0.5155\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2042 - acc: 1.0000 - val_loss: 2.4744 - val_acc: 0.5120\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 2.4873 - val_acc: 0.5100\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2026 - acc: 1.0000 - val_loss: 2.4932 - val_acc: 0.5095\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "41 [supervised loss: 0.2026, acc: 100.00%] [unsupervised loss: 1.3729, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2635 - acc: 0.9688 - val_loss: 2.6325 - val_acc: 0.4951\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2568 - acc: 1.0000 - val_loss: 2.6451 - val_acc: 0.4934\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2632 - acc: 0.9688 - val_loss: 2.6308 - val_acc: 0.4945\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2167 - acc: 1.0000 - val_loss: 2.6087 - val_acc: 0.4969\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2699 - acc: 0.9375 - val_loss: 2.5907 - val_acc: 0.4980\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2261 - acc: 1.0000 - val_loss: 2.5622 - val_acc: 0.5016\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2161 - acc: 1.0000 - val_loss: 2.5239 - val_acc: 0.5060\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2077 - acc: 1.0000 - val_loss: 2.4864 - val_acc: 0.5106\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2014 - acc: 1.0000 - val_loss: 2.4516 - val_acc: 0.5144\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2018 - acc: 1.0000 - val_loss: 2.4149 - val_acc: 0.5185\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "42 [supervised loss: 0.2018, acc: 100.00%] [unsupervised loss: 2.0262, acc: 50.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3245 - acc: 0.9688 - val_loss: 2.7709 - val_acc: 0.4828\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3198 - acc: 0.9375 - val_loss: 2.8798 - val_acc: 0.4701\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2638 - acc: 1.0000 - val_loss: 2.9593 - val_acc: 0.4619\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2701 - acc: 0.9688 - val_loss: 3.0056 - val_acc: 0.4559\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2250 - acc: 1.0000 - val_loss: 3.0295 - val_acc: 0.4530\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2350 - acc: 1.0000 - val_loss: 3.0445 - val_acc: 0.4515\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2337 - acc: 1.0000 - val_loss: 3.0473 - val_acc: 0.4507\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2496 - acc: 0.9688 - val_loss: 3.0545 - val_acc: 0.4507\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2140 - acc: 1.0000 - val_loss: 3.0548 - val_acc: 0.4504\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2069 - acc: 1.0000 - val_loss: 3.0444 - val_acc: 0.4530\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "43 [supervised loss: 0.2069, acc: 100.00%] [unsupervised loss: 1.6148, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2028 - acc: 1.0000 - val_loss: 3.3730 - val_acc: 0.4172\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2483 - acc: 1.0000 - val_loss: 3.5126 - val_acc: 0.4033\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2175 - acc: 1.0000 - val_loss: 3.6324 - val_acc: 0.3942\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2143 - acc: 1.0000 - val_loss: 3.7299 - val_acc: 0.3874\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2153 - acc: 1.0000 - val_loss: 3.7959 - val_acc: 0.3815\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2177 - acc: 1.0000 - val_loss: 3.8121 - val_acc: 0.3810\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2220 - acc: 1.0000 - val_loss: 3.8100 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2157 - acc: 1.0000 - val_loss: 3.8012 - val_acc: 0.3830\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 3.7957 - val_acc: 0.3837\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2087 - acc: 1.0000 - val_loss: 3.7720 - val_acc: 0.3857\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "44 [supervised loss: 0.2087, acc: 100.00%] [unsupervised loss: 2.6320, acc: 50.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5402 - acc: 0.8438 - val_loss: 4.8845 - val_acc: 0.2996\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2605 - acc: 1.0000 - val_loss: 5.2794 - val_acc: 0.2757\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2389 - acc: 1.0000 - val_loss: 5.5738 - val_acc: 0.2622\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2304 - acc: 1.0000 - val_loss: 5.7872 - val_acc: 0.2530\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2556 - acc: 0.9688 - val_loss: 5.9406 - val_acc: 0.2481\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2185 - acc: 1.0000 - val_loss: 6.0484 - val_acc: 0.2447\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2133 - acc: 1.0000 - val_loss: 6.1144 - val_acc: 0.2428\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2114 - acc: 1.0000 - val_loss: 6.1524 - val_acc: 0.2432\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2245 - acc: 1.0000 - val_loss: 6.1682 - val_acc: 0.2455\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2101 - acc: 1.0000 - val_loss: 6.1595 - val_acc: 0.2461\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "45 [supervised loss: 0.2101, acc: 100.00%] [unsupervised loss: 1.7234, acc: 59.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6936 - acc: 0.8438 - val_loss: 5.9456 - val_acc: 0.2503\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3772 - acc: 0.9062 - val_loss: 5.4343 - val_acc: 0.2674\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3175 - acc: 0.9688 - val_loss: 4.9711 - val_acc: 0.2856\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2608 - acc: 0.9688 - val_loss: 4.5032 - val_acc: 0.3097\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2537 - acc: 1.0000 - val_loss: 4.1316 - val_acc: 0.3324\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2209 - acc: 1.0000 - val_loss: 3.8310 - val_acc: 0.3535\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 3.5895 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2324 - acc: 1.0000 - val_loss: 3.3863 - val_acc: 0.3890\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2065 - acc: 1.0000 - val_loss: 3.2191 - val_acc: 0.4047\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2102 - acc: 1.0000 - val_loss: 3.0782 - val_acc: 0.4185\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "46 [supervised loss: 0.2102, acc: 100.00%] [unsupervised loss: 3.0449, acc: 34.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3205 - acc: 0.9688 - val_loss: 3.1247 - val_acc: 0.4035\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3520 - acc: 0.9688 - val_loss: 3.0834 - val_acc: 0.4028\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2946 - acc: 0.9688 - val_loss: 3.0406 - val_acc: 0.4019\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3701 - acc: 0.9375 - val_loss: 2.9557 - val_acc: 0.4065\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3143 - acc: 0.9375 - val_loss: 2.8595 - val_acc: 0.4139\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2338 - acc: 1.0000 - val_loss: 2.7691 - val_acc: 0.4224\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2885 - acc: 0.9688 - val_loss: 2.6874 - val_acc: 0.4291\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2245 - acc: 1.0000 - val_loss: 2.6032 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2149 - acc: 1.0000 - val_loss: 2.5260 - val_acc: 0.4500\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2363 - acc: 0.9688 - val_loss: 2.4423 - val_acc: 0.4634\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "47 [supervised loss: 0.2363, acc: 96.88%] [unsupervised loss: 1.3443, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2331 - acc: 1.0000 - val_loss: 2.4536 - val_acc: 0.4657\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2655 - acc: 1.0000 - val_loss: 2.4632 - val_acc: 0.4674\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2153 - acc: 1.0000 - val_loss: 2.4755 - val_acc: 0.4675\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2169 - acc: 1.0000 - val_loss: 2.4942 - val_acc: 0.4658\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 2.5121 - val_acc: 0.4654\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2122 - acc: 1.0000 - val_loss: 2.5319 - val_acc: 0.4636\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2121 - acc: 1.0000 - val_loss: 2.5497 - val_acc: 0.4610\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2169 - acc: 1.0000 - val_loss: 2.5632 - val_acc: 0.4589\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2018 - acc: 1.0000 - val_loss: 2.5712 - val_acc: 0.4563\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2019 - acc: 1.0000 - val_loss: 2.5765 - val_acc: 0.4575\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "48 [supervised loss: 0.2019, acc: 100.00%] [unsupervised loss: 1.1876, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2919 - acc: 0.9375 - val_loss: 2.6913 - val_acc: 0.4419\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2855 - acc: 0.9688 - val_loss: 2.7319 - val_acc: 0.4364\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 2.7653 - val_acc: 0.4321\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2910 - acc: 0.9688 - val_loss: 2.7854 - val_acc: 0.4320\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2093 - acc: 1.0000 - val_loss: 2.7989 - val_acc: 0.4311\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2263 - acc: 1.0000 - val_loss: 2.7932 - val_acc: 0.4352\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2182 - acc: 1.0000 - val_loss: 2.7860 - val_acc: 0.4366\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1992 - acc: 1.0000 - val_loss: 2.7767 - val_acc: 0.4388\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2321 - acc: 1.0000 - val_loss: 2.7471 - val_acc: 0.4449\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2046 - acc: 1.0000 - val_loss: 2.7197 - val_acc: 0.4477\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "49 [supervised loss: 0.2046, acc: 100.00%] [unsupervised loss: 1.4220, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2313 - acc: 1.0000 - val_loss: 2.9427 - val_acc: 0.4240\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3028 - acc: 0.9688 - val_loss: 3.0102 - val_acc: 0.4184\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2498 - acc: 0.9688 - val_loss: 3.0530 - val_acc: 0.4168\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2414 - acc: 0.9688 - val_loss: 3.0834 - val_acc: 0.4154\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2341 - acc: 1.0000 - val_loss: 3.0881 - val_acc: 0.4165\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2309 - acc: 1.0000 - val_loss: 3.0792 - val_acc: 0.4191\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2507 - acc: 1.0000 - val_loss: 3.0509 - val_acc: 0.4236\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2129 - acc: 1.0000 - val_loss: 3.0157 - val_acc: 0.4281\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2167 - acc: 1.0000 - val_loss: 2.9776 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2282 - acc: 1.0000 - val_loss: 2.9371 - val_acc: 0.4399\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "50 [supervised loss: 0.2282, acc: 100.00%] [unsupervised loss: 1.1243, acc: 68.75%]\n",
            "Training time: 1690.3439s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbIxExOyCxJy",
        "colab_type": "code",
        "outputId": "03aa14c8-47b2-421f-eda6-abf1d16158ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "plot_pseudo_supervised_losses = np.array(losses_pseudo_labeled)\n",
        "plot_pseudo_unsupervised_losses = np.array(losses_pseudo_unlabeled)\n",
        "plot_pseudo_all_losses = np.array(losses_pseudo)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_all_losses, label=\"All loss\", color='black')\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_supervised_losses, label=\"Supervised loss\", color='tab:blue', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_unsupervised_losses, label=\"Unsupervised loss\", color='tab:green', linestyle='dashed')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Pseudo Label's Supervised and Unsupervised Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc8febd0ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAFPCAYAAAA1Pp3dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njpuIiIiIiEiaU8dNREREREQkzf0/cLarPUidtLYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-4ZGValCxGj",
        "colab_type": "code",
        "outputId": "475a1cbb-8a96-4869-9d7e-fb4a3a5dd660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "# tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-8000.h5\")\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-30.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 25s 505us/step\n",
            "Training Accuracy: 75.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jD1xtbCw7H",
        "colab_type": "code",
        "outputId": "a5fcf47f-880b-47d5-b261-abad697b5333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "# tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-6000.h5\")\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-30.h5\", by_name=False)\n",
        "\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 459us/step\n",
            "Test Accuracy: 74.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeX3mD5qC5TQ",
        "colab_type": "code",
        "outputId": "ad516769-aeae-44d3-f332-ac78190827aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "accs = []\n",
        "# tx = [x for x in range(1,31,1)]\n",
        "tx = [x for x in range(1, len(iteration_checkpoints)+1, 1)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  # tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\"+ str(e) +\".h5\")\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"Pseudo Label's accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 405us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 425us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 406us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 405us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 421us/step\n",
            "10000/10000 [==============================] - 4s 424us/step\n",
            "10000/10000 [==============================] - 4s 430us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 408us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 408us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 407us/step\n",
            "10000/10000 [==============================] - 4s 406us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 420us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "0.8634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc8fd642898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFPCAYAAAAfjmxyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4PM4STC5Q3",
        "colab_type": "code",
        "outputId": "6abae66c-7903-4dca-96ed-d7222c50b24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(max(accs[15:]))\n",
        "# for acc in accs:\n",
        "#   print(acc)\n",
        "pseudo_accs = accs\n",
        "print(pseudo_accs)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8112\n",
            "[0.8634, 0.8549, 0.8464, 0.8475, 0.8423, 0.8309, 0.8178, 0.7995, 0.7981, 0.8066, 0.7921, 0.7936, 0.7801, 0.7829, 0.793, 0.8068, 0.8112, 0.81, 0.8002, 0.7746, 0.7616, 0.7478, 0.7321, 0.6938, 0.7257, 0.7305, 0.7414, 0.75, 0.7388, 0.692, 0.6977, 0.6248, 0.6487, 0.4684, 0.4003, 0.441, 0.6315, 0.5675, 0.6554, 0.5682, 0.4988, 0.4995, 0.4328, 0.3352, 0.241, 0.4082, 0.4646, 0.4485, 0.4369, 0.443]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9jhBOIcqT-h",
        "colab_type": "text"
      },
      "source": [
        "# Mean Teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxPsJI6lMwsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "teacher = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "student.compile(loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'], \n",
        "                optimizer=Adam())\n",
        "teacher.compile(loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'], \n",
        "                optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKFkigJMwpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_teacher_train(iterations, batch_size, save_interval, alpha, iter_epochs):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # -------------------------\n",
        "        #  Train the model\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs_labeled, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Get unlabeled examples\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "        # Train on labeled examples\n",
        "        # loss_labeled_classification, acc_labeled_classification = student.train_on_batch(imgs_labeled, labels)\n",
        "        datagen.fit(imgs_labeled)\n",
        "        student.fit_generator(datagen.flow(imgs_labeled, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        loss_labeled_classification, acc_labeled_classification = history.losses[-1], history.accs[-1]\n",
        "        pred_teacher_labeled = teacher.predict(imgs_labeled)\n",
        "        loss_labeled_consistency, acc_labeled_consistency = student.train_on_batch(imgs_labeled, pred_teacher_labeled)\n",
        "\n",
        "        # Train on unlabeled examples\n",
        "        pred_teacher_unlabeled = teacher.predict(imgs_unlabeled)\n",
        "        loss_unlabeled_consistency, acc_unlabeled_consistency = student.train_on_batch(imgs_unlabeled, pred_teacher_unlabeled)\n",
        "\n",
        "        # Update teacher model\n",
        "        teacher_weights_this = teacher.get_weights()\n",
        "        student_weights_this = student.get_weights()\n",
        "        for i in range(len(teacher_weights_this)):\n",
        "          teacher_weights_this[i] = alpha * teacher_weights_this[i] + (1-alpha) * student_weights_this[i]\n",
        "        # teacher_weights_this = alpha * teacher_weights_this + (1-alpha) * student_weights_this\n",
        "        teacher_weights_last = teacher_weights_this\n",
        "        teacher.set_weights(teacher_weights_this)\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "          # Save losses\n",
        "          supervised_losses.append(loss_labeled_classification)\n",
        "          unsupervised_losses.append(loss_labeled_consistency + loss_unlabeled_consistency)\n",
        "          labeled_consistency_costs.append(loss_labeled_consistency)\n",
        "          unlabeled_consistency_costs.append(loss_unlabeled_consistency)\n",
        "          accs_supervised.append(acc_labeled_classification)\n",
        "          accs_unsupervised.append((acc_labeled_consistency + acc_unlabeled_consistency)/2.0)\n",
        "          accs_labeled_consistency.append(acc_labeled_consistency)\n",
        "          accs_unlabeled_consistency.append(acc_unlabeled_consistency)\n",
        "\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [supervised loss: %.4f, acc: %.2f%%] [unsupervised loss: %.4f, acc: %.2f%%] [labeled consistency loss: %.4f, acc:acc: %.2f%%] [unlabeled consistency loss: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, loss_labeled_classification, 100 * acc_labeled_classification, \n",
        "                 loss_labeled_consistency + loss_unlabeled_consistency, 100 * ((acc_labeled_consistency + acc_unlabeled_consistency)/2.0), \n",
        "                 loss_labeled_consistency, 100 * acc_labeled_consistency, \n",
        "                  loss_unlabeled_consistency, 100 * acc_unlabeled_consistency))\n",
        "          \n",
        "          student.save(\"./models/models-label-\" + str(num_labeled) + \"/student-\" + str(iteration+1) + \".h5\")\n",
        "          teacher.save(\"./models/models-label-\" + str(num_labeled) + \"/teacher-\" + str(iteration+1) + \".h5\")\n",
        "          file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_supervised_losses.json\"\n",
        "          file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_unsupervised_losses.json\"\n",
        "          file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_labeled_consistency_costs.json\"\n",
        "          file4 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_unlabeled_consistency_costs.json\"\n",
        "          with open(file1, 'w') as json_file:\n",
        "                json.dump(str(supervised_losses), json_file)\n",
        "          with open(file2, 'w') as json_file:\n",
        "                json.dump(str(unsupervised_losses), json_file)\n",
        "          with open(file3, 'w') as json_file:\n",
        "                json.dump(str(labeled_consistency_costs), json_file)\n",
        "          with open(file4, 'w') as json_file:\n",
        "                json.dump(str(unlabeled_consistency_costs), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS6N6-dRMwnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a37f5b0-7808-43fb-c5ef-160342e99ae4"
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 50 # 30\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "alpha = 0.5\n",
        "iter_epochs = 10\n",
        "\n",
        "supervised_losses = [] # classification cost\n",
        "unsupervised_losses = [] # consistency cost\n",
        "labeled_consistency_costs = []\n",
        "unlabeled_consistency_costs = []\n",
        "accs_supervised = []\n",
        "accs_unsupervised = []\n",
        "accs_labeled_consistency = []\n",
        "accs_unlabeled_consistency = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "student = load_model(\"./models/cifar10_model.037.h5\")\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the mean teacher for the specified number of iterations\n",
        "mean_teacher_train(iterations, batch_size, save_interval, alpha, iter_epochs)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 17s 17s/step - loss: 0.4723 - acc: 0.9062 - val_loss: 0.6941 - val_acc: 0.8453\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5345 - acc: 0.9062 - val_loss: 0.6744 - val_acc: 0.8482\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5150 - acc: 0.8750 - val_loss: 0.6693 - val_acc: 0.8484\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5425 - acc: 0.9062 - val_loss: 0.6587 - val_acc: 0.8513\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5422 - acc: 0.9062 - val_loss: 0.6489 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3827 - acc: 0.9375 - val_loss: 0.6381 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2826 - acc: 1.0000 - val_loss: 0.6341 - val_acc: 0.8563\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2757 - acc: 1.0000 - val_loss: 0.6341 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2473 - acc: 1.0000 - val_loss: 0.6358 - val_acc: 0.8534\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2720 - acc: 1.0000 - val_loss: 0.6384 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "1 [supervised loss: 0.2720, acc: 100.00%] [unsupervised loss: 2.9591, acc: 87.50%] [labeled consistency loss: 1.9378, acc:acc: 87.50%] [unlabeled consistency loss: 1.0213, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3001 - acc: 0.9688 - val_loss: 0.6255 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3367 - acc: 0.9062 - val_loss: 0.6263 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3256 - acc: 0.9375 - val_loss: 0.6276 - val_acc: 0.8535\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2848 - acc: 0.9375 - val_loss: 0.6299 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2459 - acc: 1.0000 - val_loss: 0.6339 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2274 - acc: 1.0000 - val_loss: 0.6377 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2520 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.8498\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2192 - acc: 1.0000 - val_loss: 0.6480 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2186 - acc: 1.0000 - val_loss: 0.6522 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2164 - acc: 1.0000 - val_loss: 0.6572 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "2 [supervised loss: 0.2164, acc: 100.00%] [unsupervised loss: 1.3210, acc: 87.50%] [labeled consistency loss: 0.6407, acc:acc: 93.75%] [unlabeled consistency loss: 0.6803, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3303 - acc: 0.9688 - val_loss: 0.6676 - val_acc: 0.8437\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2947 - acc: 0.9688 - val_loss: 0.6739 - val_acc: 0.8407\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3039 - acc: 0.9688 - val_loss: 0.6780 - val_acc: 0.8397\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2940 - acc: 0.9688 - val_loss: 0.6823 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2665 - acc: 1.0000 - val_loss: 0.6850 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2724 - acc: 1.0000 - val_loss: 0.6863 - val_acc: 0.8370\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2426 - acc: 1.0000 - val_loss: 0.6881 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2257 - acc: 1.0000 - val_loss: 0.6896 - val_acc: 0.8361\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2236 - acc: 1.0000 - val_loss: 0.6913 - val_acc: 0.8363\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2255 - acc: 1.0000 - val_loss: 0.6928 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "3 [supervised loss: 0.2255, acc: 100.00%] [unsupervised loss: 1.0495, acc: 95.31%] [labeled consistency loss: 0.4135, acc:acc: 100.00%] [unlabeled consistency loss: 0.6359, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2332 - acc: 1.0000 - val_loss: 0.6991 - val_acc: 0.8347\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2327 - acc: 1.0000 - val_loss: 0.7059 - val_acc: 0.8336\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4898 - acc: 0.9062 - val_loss: 0.7124 - val_acc: 0.8324\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2320 - acc: 1.0000 - val_loss: 0.7191 - val_acc: 0.8313\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2483 - acc: 1.0000 - val_loss: 0.7259 - val_acc: 0.8306\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2475 - acc: 1.0000 - val_loss: 0.7324 - val_acc: 0.8292\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2246 - acc: 1.0000 - val_loss: 0.7382 - val_acc: 0.8276\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2123 - acc: 1.0000 - val_loss: 0.7442 - val_acc: 0.8261\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2110 - acc: 1.0000 - val_loss: 0.7504 - val_acc: 0.8245\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2119 - acc: 1.0000 - val_loss: 0.7550 - val_acc: 0.8226\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "4 [supervised loss: 0.2119, acc: 100.00%] [unsupervised loss: 0.9849, acc: 95.31%] [labeled consistency loss: 0.2437, acc:acc: 100.00%] [unlabeled consistency loss: 0.7412, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2928 - acc: 0.9688 - val_loss: 0.7619 - val_acc: 0.8226\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2630 - acc: 0.9688 - val_loss: 0.7680 - val_acc: 0.8219\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2581 - acc: 0.9688 - val_loss: 0.7748 - val_acc: 0.8205\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2197 - acc: 1.0000 - val_loss: 0.7825 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2293 - acc: 1.0000 - val_loss: 0.7906 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2263 - acc: 1.0000 - val_loss: 0.7973 - val_acc: 0.8155\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2234 - acc: 1.0000 - val_loss: 0.8047 - val_acc: 0.8132\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2170 - acc: 1.0000 - val_loss: 0.8108 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2192 - acc: 1.0000 - val_loss: 0.8174 - val_acc: 0.8093\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2177 - acc: 1.0000 - val_loss: 0.8228 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "5 [supervised loss: 0.2177, acc: 100.00%] [unsupervised loss: 1.2041, acc: 96.88%] [labeled consistency loss: 0.5604, acc:acc: 96.88%] [unlabeled consistency loss: 0.6437, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3127 - acc: 0.9688 - val_loss: 0.8231 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3293 - acc: 0.9375 - val_loss: 0.8236 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2752 - acc: 0.9688 - val_loss: 0.8221 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2581 - acc: 1.0000 - val_loss: 0.8185 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2559 - acc: 1.0000 - val_loss: 0.8168 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2273 - acc: 1.0000 - val_loss: 0.8147 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2314 - acc: 1.0000 - val_loss: 0.8128 - val_acc: 0.8106\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2240 - acc: 1.0000 - val_loss: 0.8117 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2211 - acc: 1.0000 - val_loss: 0.8105 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2364 - acc: 1.0000 - val_loss: 0.8098 - val_acc: 0.8138\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "6 [supervised loss: 0.2364, acc: 100.00%] [unsupervised loss: 1.1544, acc: 95.31%] [labeled consistency loss: 0.4915, acc:acc: 93.75%] [unlabeled consistency loss: 0.6628, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5044 - acc: 0.9062 - val_loss: 0.8309 - val_acc: 0.8089\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3430 - acc: 0.9375 - val_loss: 0.8437 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3281 - acc: 0.9688 - val_loss: 0.8591 - val_acc: 0.8020\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2675 - acc: 1.0000 - val_loss: 0.8755 - val_acc: 0.7967\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2976 - acc: 0.9688 - val_loss: 0.8931 - val_acc: 0.7947\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2601 - acc: 0.9688 - val_loss: 0.9095 - val_acc: 0.7896\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2400 - acc: 1.0000 - val_loss: 0.9264 - val_acc: 0.7850\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2644 - acc: 1.0000 - val_loss: 0.9429 - val_acc: 0.7820\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2540 - acc: 0.9688 - val_loss: 0.9606 - val_acc: 0.7777\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2559 - acc: 1.0000 - val_loss: 0.9803 - val_acc: 0.7742\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "7 [supervised loss: 0.2559, acc: 100.00%] [unsupervised loss: 1.4026, acc: 89.06%] [labeled consistency loss: 0.7509, acc:acc: 84.38%] [unlabeled consistency loss: 0.6517, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4128 - acc: 0.9062 - val_loss: 0.9902 - val_acc: 0.7722\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3870 - acc: 0.9062 - val_loss: 0.9707 - val_acc: 0.7754\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3399 - acc: 0.9062 - val_loss: 0.9506 - val_acc: 0.7788\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3083 - acc: 0.9688 - val_loss: 0.9283 - val_acc: 0.7848\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2640 - acc: 1.0000 - val_loss: 0.9107 - val_acc: 0.7870\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2732 - acc: 0.9688 - val_loss: 0.9005 - val_acc: 0.7878\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3023 - acc: 0.9688 - val_loss: 0.8931 - val_acc: 0.7906\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2511 - acc: 0.9688 - val_loss: 0.8890 - val_acc: 0.7934\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2223 - acc: 1.0000 - val_loss: 0.8879 - val_acc: 0.7945\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2270 - acc: 1.0000 - val_loss: 0.8874 - val_acc: 0.7957\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "8 [supervised loss: 0.2270, acc: 100.00%] [unsupervised loss: 1.6894, acc: 90.62%] [labeled consistency loss: 1.1102, acc:acc: 87.50%] [unlabeled consistency loss: 0.5793, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2237 - acc: 1.0000 - val_loss: 0.9079 - val_acc: 0.7919\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2407 - acc: 1.0000 - val_loss: 0.9139 - val_acc: 0.7903\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2321 - acc: 1.0000 - val_loss: 0.9167 - val_acc: 0.7897\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2721 - acc: 0.9688 - val_loss: 0.9171 - val_acc: 0.7903\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2543 - acc: 1.0000 - val_loss: 0.9143 - val_acc: 0.7907\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2359 - acc: 1.0000 - val_loss: 0.9131 - val_acc: 0.7916\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2245 - acc: 1.0000 - val_loss: 0.9101 - val_acc: 0.7934\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2340 - acc: 1.0000 - val_loss: 0.9068 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2196 - acc: 1.0000 - val_loss: 0.9038 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2138 - acc: 1.0000 - val_loss: 0.8993 - val_acc: 0.7972\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "9 [supervised loss: 0.2138, acc: 100.00%] [unsupervised loss: 1.2877, acc: 90.62%] [labeled consistency loss: 0.6737, acc:acc: 90.62%] [unlabeled consistency loss: 0.6140, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4500 - acc: 0.9062 - val_loss: 0.9340 - val_acc: 0.7903\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4721 - acc: 0.8750 - val_loss: 0.9374 - val_acc: 0.7890\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3045 - acc: 0.9688 - val_loss: 0.9367 - val_acc: 0.7899\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2789 - acc: 1.0000 - val_loss: 0.9283 - val_acc: 0.7922\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2569 - acc: 1.0000 - val_loss: 0.9161 - val_acc: 0.7929\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2333 - acc: 1.0000 - val_loss: 0.9038 - val_acc: 0.7942\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2273 - acc: 1.0000 - val_loss: 0.8921 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2175 - acc: 1.0000 - val_loss: 0.8826 - val_acc: 0.7971\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - acc: 1.0000 - val_loss: 0.8731 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2166 - acc: 1.0000 - val_loss: 0.8654 - val_acc: 0.8005\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "10 [supervised loss: 0.2166, acc: 100.00%] [unsupervised loss: 1.6249, acc: 92.19%] [labeled consistency loss: 1.0314, acc:acc: 90.62%] [unlabeled consistency loss: 0.5935, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2451 - acc: 1.0000 - val_loss: 0.8640 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2631 - acc: 1.0000 - val_loss: 0.8584 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2791 - acc: 0.9688 - val_loss: 0.8526 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2764 - acc: 1.0000 - val_loss: 0.8435 - val_acc: 0.8026\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3138 - acc: 0.9688 - val_loss: 0.8296 - val_acc: 0.8069\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2461 - acc: 1.0000 - val_loss: 0.8215 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2115 - acc: 1.0000 - val_loss: 0.8167 - val_acc: 0.8084\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2421 - acc: 0.9688 - val_loss: 0.8137 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2136 - acc: 1.0000 - val_loss: 0.8136 - val_acc: 0.8084\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2125 - acc: 1.0000 - val_loss: 0.8162 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "11 [supervised loss: 0.2125, acc: 100.00%] [unsupervised loss: 1.5780, acc: 87.50%] [labeled consistency loss: 0.5123, acc:acc: 96.88%] [unlabeled consistency loss: 1.0656, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2864 - acc: 0.9688 - val_loss: 0.8604 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2674 - acc: 0.9688 - val_loss: 0.8736 - val_acc: 0.7935\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2573 - acc: 1.0000 - val_loss: 0.8840 - val_acc: 0.7920\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2216 - acc: 1.0000 - val_loss: 0.8939 - val_acc: 0.7903\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2218 - acc: 1.0000 - val_loss: 0.9030 - val_acc: 0.7877\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2171 - acc: 1.0000 - val_loss: 0.9121 - val_acc: 0.7863\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2184 - acc: 1.0000 - val_loss: 0.9208 - val_acc: 0.7843\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2232 - acc: 1.0000 - val_loss: 0.9283 - val_acc: 0.7831\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2114 - acc: 1.0000 - val_loss: 0.9350 - val_acc: 0.7819\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2179 - acc: 1.0000 - val_loss: 0.9413 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "12 [supervised loss: 0.2179, acc: 100.00%] [unsupervised loss: 1.4654, acc: 85.94%] [labeled consistency loss: 0.7035, acc:acc: 90.62%] [unlabeled consistency loss: 0.7618, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2198 - acc: 1.0000 - val_loss: 0.9751 - val_acc: 0.7737\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2173 - acc: 1.0000 - val_loss: 0.9874 - val_acc: 0.7694\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2221 - acc: 1.0000 - val_loss: 0.9988 - val_acc: 0.7676\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2162 - acc: 1.0000 - val_loss: 1.0093 - val_acc: 0.7653\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2133 - acc: 1.0000 - val_loss: 1.0189 - val_acc: 0.7634\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2183 - acc: 1.0000 - val_loss: 1.0258 - val_acc: 0.7626\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2346 - acc: 0.9688 - val_loss: 1.0281 - val_acc: 0.7612\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2095 - acc: 1.0000 - val_loss: 1.0295 - val_acc: 0.7605\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2154 - acc: 1.0000 - val_loss: 1.0297 - val_acc: 0.7606\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2081 - acc: 1.0000 - val_loss: 1.0289 - val_acc: 0.7599\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "13 [supervised loss: 0.2081, acc: 100.00%] [unsupervised loss: 1.1204, acc: 89.06%] [labeled consistency loss: 0.4386, acc:acc: 96.88%] [unlabeled consistency loss: 0.6817, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2492 - acc: 1.0000 - val_loss: 1.0189 - val_acc: 0.7611\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2171 - acc: 1.0000 - val_loss: 1.0141 - val_acc: 0.7611\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2266 - acc: 1.0000 - val_loss: 1.0076 - val_acc: 0.7626\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2211 - acc: 1.0000 - val_loss: 0.9987 - val_acc: 0.7640\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2234 - acc: 1.0000 - val_loss: 0.9888 - val_acc: 0.7664\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2113 - acc: 1.0000 - val_loss: 0.9793 - val_acc: 0.7688\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2218 - acc: 1.0000 - val_loss: 0.9683 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.9572 - val_acc: 0.7737\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2091 - acc: 1.0000 - val_loss: 0.9475 - val_acc: 0.7758\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2102 - acc: 1.0000 - val_loss: 0.9381 - val_acc: 0.7777\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "14 [supervised loss: 0.2102, acc: 100.00%] [unsupervised loss: 1.2986, acc: 84.38%] [labeled consistency loss: 0.4581, acc:acc: 90.62%] [unlabeled consistency loss: 0.8405, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2544 - acc: 0.9688 - val_loss: 0.9437 - val_acc: 0.7774\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2362 - acc: 1.0000 - val_loss: 0.9476 - val_acc: 0.7746\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2298 - acc: 1.0000 - val_loss: 0.9514 - val_acc: 0.7744\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2390 - acc: 1.0000 - val_loss: 0.9545 - val_acc: 0.7737\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2262 - acc: 1.0000 - val_loss: 0.9573 - val_acc: 0.7733\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2259 - acc: 1.0000 - val_loss: 0.9602 - val_acc: 0.7728\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2152 - acc: 1.0000 - val_loss: 0.9625 - val_acc: 0.7724\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2210 - acc: 1.0000 - val_loss: 0.9645 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2094 - acc: 1.0000 - val_loss: 0.9661 - val_acc: 0.7691\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2154 - acc: 1.0000 - val_loss: 0.9672 - val_acc: 0.7687\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "15 [supervised loss: 0.2154, acc: 100.00%] [unsupervised loss: 1.1958, acc: 92.19%] [labeled consistency loss: 0.4900, acc:acc: 93.75%] [unlabeled consistency loss: 0.7058, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3048 - acc: 0.9688 - val_loss: 0.9719 - val_acc: 0.7665\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3442 - acc: 0.9375 - val_loss: 0.9670 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2366 - acc: 1.0000 - val_loss: 0.9594 - val_acc: 0.7694\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2486 - acc: 0.9688 - val_loss: 0.9501 - val_acc: 0.7720\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2345 - acc: 1.0000 - val_loss: 0.9406 - val_acc: 0.7737\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2192 - acc: 1.0000 - val_loss: 0.9317 - val_acc: 0.7755\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2152 - acc: 1.0000 - val_loss: 0.9237 - val_acc: 0.7778\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2241 - acc: 1.0000 - val_loss: 0.9153 - val_acc: 0.7784\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2138 - acc: 1.0000 - val_loss: 0.9080 - val_acc: 0.7810\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 0.9014 - val_acc: 0.7819\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "16 [supervised loss: 0.2190, acc: 100.00%] [unsupervised loss: 1.3921, acc: 92.19%] [labeled consistency loss: 0.6286, acc:acc: 93.75%] [unlabeled consistency loss: 0.7635, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2205 - acc: 1.0000 - val_loss: 0.9067 - val_acc: 0.7795\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2151 - acc: 1.0000 - val_loss: 0.9127 - val_acc: 0.7780\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2418 - acc: 1.0000 - val_loss: 0.9167 - val_acc: 0.7762\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2339 - acc: 1.0000 - val_loss: 0.9189 - val_acc: 0.7751\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2188 - acc: 1.0000 - val_loss: 0.9191 - val_acc: 0.7758\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2212 - acc: 1.0000 - val_loss: 0.9184 - val_acc: 0.7746\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2088 - acc: 1.0000 - val_loss: 0.9180 - val_acc: 0.7755\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2114 - acc: 1.0000 - val_loss: 0.9176 - val_acc: 0.7753\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2249 - acc: 1.0000 - val_loss: 0.9146 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2220 - acc: 1.0000 - val_loss: 0.9125 - val_acc: 0.7791\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "17 [supervised loss: 0.2220, acc: 100.00%] [unsupervised loss: 1.1415, acc: 92.19%] [labeled consistency loss: 0.3908, acc:acc: 96.88%] [unlabeled consistency loss: 0.7507, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2396 - acc: 1.0000 - val_loss: 0.9282 - val_acc: 0.7747\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2626 - acc: 0.9688 - val_loss: 0.9295 - val_acc: 0.7741\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3350 - acc: 0.9375 - val_loss: 0.9226 - val_acc: 0.7767\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2336 - acc: 1.0000 - val_loss: 0.9178 - val_acc: 0.7774\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - acc: 1.0000 - val_loss: 0.9155 - val_acc: 0.7785\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2203 - acc: 1.0000 - val_loss: 0.9148 - val_acc: 0.7796\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2151 - acc: 1.0000 - val_loss: 0.9148 - val_acc: 0.7806\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2075 - acc: 1.0000 - val_loss: 0.9163 - val_acc: 0.7806\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 1.0000 - val_loss: 0.9199 - val_acc: 0.7791\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 0.7783\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "18 [supervised loss: 0.2045, acc: 100.00%] [unsupervised loss: 1.1824, acc: 90.62%] [labeled consistency loss: 0.5166, acc:acc: 90.62%] [unlabeled consistency loss: 0.6658, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2586 - acc: 0.9688 - val_loss: 0.9497 - val_acc: 0.7726\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2225 - acc: 1.0000 - val_loss: 0.9553 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2280 - acc: 1.0000 - val_loss: 0.9595 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2318 - acc: 1.0000 - val_loss: 0.9619 - val_acc: 0.7701\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2578 - acc: 0.9688 - val_loss: 0.9601 - val_acc: 0.7698\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2209 - acc: 1.0000 - val_loss: 0.9590 - val_acc: 0.7690\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2113 - acc: 1.0000 - val_loss: 0.9599 - val_acc: 0.7702\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2219 - acc: 1.0000 - val_loss: 0.9605 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2110 - acc: 1.0000 - val_loss: 0.9616 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2175 - acc: 1.0000 - val_loss: 0.9625 - val_acc: 0.7701\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "19 [supervised loss: 0.2175, acc: 100.00%] [unsupervised loss: 0.8765, acc: 95.31%] [labeled consistency loss: 0.2974, acc:acc: 96.88%] [unlabeled consistency loss: 0.5792, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2292 - acc: 1.0000 - val_loss: 0.9653 - val_acc: 0.7694\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2120 - acc: 1.0000 - val_loss: 0.9657 - val_acc: 0.7693\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2375 - acc: 1.0000 - val_loss: 0.9642 - val_acc: 0.7694\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2080 - acc: 1.0000 - val_loss: 0.9619 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2061 - acc: 1.0000 - val_loss: 0.9599 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2156 - acc: 1.0000 - val_loss: 0.9564 - val_acc: 0.7723\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2138 - acc: 1.0000 - val_loss: 0.9532 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2079 - acc: 1.0000 - val_loss: 0.9501 - val_acc: 0.7742\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2061 - acc: 1.0000 - val_loss: 0.9466 - val_acc: 0.7756\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2039 - acc: 1.0000 - val_loss: 0.9428 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "20 [supervised loss: 0.2039, acc: 100.00%] [unsupervised loss: 0.9553, acc: 93.75%] [labeled consistency loss: 0.4759, acc:acc: 93.75%] [unlabeled consistency loss: 0.4794, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2073 - acc: 1.0000 - val_loss: 0.9358 - val_acc: 0.7774\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2089 - acc: 1.0000 - val_loss: 0.9346 - val_acc: 0.7773\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2166 - acc: 1.0000 - val_loss: 0.9333 - val_acc: 0.7764\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 0.9310 - val_acc: 0.7772\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2096 - acc: 1.0000 - val_loss: 0.9283 - val_acc: 0.7775\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2042 - acc: 1.0000 - val_loss: 0.9259 - val_acc: 0.7781\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2144 - acc: 1.0000 - val_loss: 0.9211 - val_acc: 0.7793\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2076 - acc: 1.0000 - val_loss: 0.9161 - val_acc: 0.7808\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2036 - acc: 1.0000 - val_loss: 0.9120 - val_acc: 0.7816\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2008 - acc: 1.0000 - val_loss: 0.9080 - val_acc: 0.7822\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "21 [supervised loss: 0.2008, acc: 100.00%] [unsupervised loss: 0.8899, acc: 92.19%] [labeled consistency loss: 0.2546, acc:acc: 100.00%] [unlabeled consistency loss: 0.6353, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2666 - acc: 0.9688 - val_loss: 0.9155 - val_acc: 0.7809\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2803 - acc: 1.0000 - val_loss: 0.9165 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2379 - acc: 1.0000 - val_loss: 0.9130 - val_acc: 0.7803\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2399 - acc: 1.0000 - val_loss: 0.9074 - val_acc: 0.7825\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2441 - acc: 1.0000 - val_loss: 0.8972 - val_acc: 0.7848\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2245 - acc: 1.0000 - val_loss: 0.8860 - val_acc: 0.7879\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2279 - acc: 1.0000 - val_loss: 0.8727 - val_acc: 0.7916\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.8613 - val_acc: 0.7942\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2098 - acc: 1.0000 - val_loss: 0.8510 - val_acc: 0.7963\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2099 - acc: 1.0000 - val_loss: 0.8422 - val_acc: 0.7982\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "22 [supervised loss: 0.2099, acc: 100.00%] [unsupervised loss: 1.5036, acc: 90.62%] [labeled consistency loss: 0.5406, acc:acc: 90.62%] [unlabeled consistency loss: 0.9629, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2192 - acc: 1.0000 - val_loss: 0.8531 - val_acc: 0.7962\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2350 - acc: 1.0000 - val_loss: 0.8615 - val_acc: 0.7935\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2066 - acc: 1.0000 - val_loss: 0.8691 - val_acc: 0.7921\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2031 - acc: 1.0000 - val_loss: 0.8765 - val_acc: 0.7896\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2134 - acc: 1.0000 - val_loss: 0.8822 - val_acc: 0.7880\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2063 - acc: 1.0000 - val_loss: 0.8875 - val_acc: 0.7871\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2055 - acc: 1.0000 - val_loss: 0.8930 - val_acc: 0.7863\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2086 - acc: 1.0000 - val_loss: 0.8970 - val_acc: 0.7853\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2186 - acc: 1.0000 - val_loss: 0.8988 - val_acc: 0.7851\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2058 - acc: 1.0000 - val_loss: 0.9005 - val_acc: 0.7847\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "23 [supervised loss: 0.2058, acc: 100.00%] [unsupervised loss: 0.9018, acc: 96.88%] [labeled consistency loss: 0.3088, acc:acc: 100.00%] [unlabeled consistency loss: 0.5930, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2012 - acc: 1.0000 - val_loss: 0.9219 - val_acc: 0.7788\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 0.9321 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2200 - acc: 1.0000 - val_loss: 0.9408 - val_acc: 0.7751\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2014 - acc: 1.0000 - val_loss: 0.9478 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2014 - acc: 1.0000 - val_loss: 0.9543 - val_acc: 0.7712\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 0.9596 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2012 - acc: 1.0000 - val_loss: 0.9646 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2017 - acc: 1.0000 - val_loss: 0.9686 - val_acc: 0.7674\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2027 - acc: 1.0000 - val_loss: 0.9724 - val_acc: 0.7665\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2089 - acc: 1.0000 - val_loss: 0.9745 - val_acc: 0.7665\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "24 [supervised loss: 0.2089, acc: 100.00%] [unsupervised loss: 0.8730, acc: 95.31%] [labeled consistency loss: 0.2608, acc:acc: 96.88%] [unlabeled consistency loss: 0.6122, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2456 - acc: 0.9688 - val_loss: 0.9946 - val_acc: 0.7629\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2287 - acc: 1.0000 - val_loss: 1.0025 - val_acc: 0.7596\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2586 - acc: 0.9688 - val_loss: 1.0051 - val_acc: 0.7591\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2480 - acc: 0.9688 - val_loss: 1.0038 - val_acc: 0.7595\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2049 - acc: 1.0000 - val_loss: 1.0021 - val_acc: 0.7585\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2128 - acc: 1.0000 - val_loss: 1.0016 - val_acc: 0.7582\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2025 - acc: 1.0000 - val_loss: 1.0014 - val_acc: 0.7574\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2023 - acc: 1.0000 - val_loss: 1.0018 - val_acc: 0.7572\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1994 - acc: 1.0000 - val_loss: 1.0017 - val_acc: 0.7567\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2137 - acc: 1.0000 - val_loss: 1.0034 - val_acc: 0.7577\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "25 [supervised loss: 0.2137, acc: 100.00%] [unsupervised loss: 1.6000, acc: 82.81%] [labeled consistency loss: 0.5256, acc:acc: 93.75%] [unlabeled consistency loss: 1.0744, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2119 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.7405\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2194 - acc: 1.0000 - val_loss: 1.1038 - val_acc: 0.7341\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2158 - acc: 1.0000 - val_loss: 1.1260 - val_acc: 0.7297\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2500 - acc: 1.0000 - val_loss: 1.1392 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2284 - acc: 0.9688 - val_loss: 1.1436 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2059 - acc: 1.0000 - val_loss: 1.1462 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2076 - acc: 1.0000 - val_loss: 1.1476 - val_acc: 0.7259\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 1.1489 - val_acc: 0.7255\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2020 - acc: 1.0000 - val_loss: 1.1506 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2031 - acc: 1.0000 - val_loss: 1.1509 - val_acc: 0.7271\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "26 [supervised loss: 0.2031, acc: 100.00%] [unsupervised loss: 1.1701, acc: 96.88%] [labeled consistency loss: 0.4080, acc:acc: 96.88%] [unlabeled consistency loss: 0.7621, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2478 - acc: 1.0000 - val_loss: 1.1982 - val_acc: 0.7173\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2682 - acc: 0.9688 - val_loss: 1.2107 - val_acc: 0.7150\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2542 - acc: 0.9688 - val_loss: 1.2131 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2123 - acc: 1.0000 - val_loss: 1.2138 - val_acc: 0.7136\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2254 - acc: 1.0000 - val_loss: 1.2102 - val_acc: 0.7151\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2253 - acc: 1.0000 - val_loss: 1.2022 - val_acc: 0.7159\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2286 - acc: 1.0000 - val_loss: 1.1902 - val_acc: 0.7183\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2194 - acc: 1.0000 - val_loss: 1.1769 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2140 - acc: 1.0000 - val_loss: 1.1633 - val_acc: 0.7229\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2110 - acc: 1.0000 - val_loss: 1.1503 - val_acc: 0.7255\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "27 [supervised loss: 0.2110, acc: 100.00%] [unsupervised loss: 1.5279, acc: 92.19%] [labeled consistency loss: 0.6862, acc:acc: 93.75%] [unlabeled consistency loss: 0.8417, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3306 - acc: 0.9688 - val_loss: 1.1530 - val_acc: 0.7249\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2315 - acc: 1.0000 - val_loss: 1.1457 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2061 - acc: 1.0000 - val_loss: 1.1408 - val_acc: 0.7271\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2253 - acc: 1.0000 - val_loss: 1.1224 - val_acc: 0.7316\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2197 - acc: 1.0000 - val_loss: 1.1012 - val_acc: 0.7357\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2311 - acc: 0.9688 - val_loss: 1.0829 - val_acc: 0.7383\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2186 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.7427\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2047 - acc: 1.0000 - val_loss: 1.0475 - val_acc: 0.7469\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1993 - acc: 1.0000 - val_loss: 1.0333 - val_acc: 0.7518\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2022 - acc: 1.0000 - val_loss: 1.0201 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "28 [supervised loss: 0.2022, acc: 100.00%] [unsupervised loss: 1.2038, acc: 89.06%] [labeled consistency loss: 0.4941, acc:acc: 90.62%] [unlabeled consistency loss: 0.7097, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2123 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.7415\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 1.0970 - val_acc: 0.7353\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2118 - acc: 1.0000 - val_loss: 1.1195 - val_acc: 0.7306\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2165 - acc: 1.0000 - val_loss: 1.1405 - val_acc: 0.7268\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2051 - acc: 1.0000 - val_loss: 1.1584 - val_acc: 0.7238\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 1.1749 - val_acc: 0.7200\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2081 - acc: 1.0000 - val_loss: 1.1885 - val_acc: 0.7170\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2059 - acc: 1.0000 - val_loss: 1.1980 - val_acc: 0.7145\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1993 - acc: 1.0000 - val_loss: 1.2058 - val_acc: 0.7133\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2030 - acc: 1.0000 - val_loss: 1.2114 - val_acc: 0.7119\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "29 [supervised loss: 0.2030, acc: 100.00%] [unsupervised loss: 1.1251, acc: 95.31%] [labeled consistency loss: 0.3363, acc:acc: 100.00%] [unlabeled consistency loss: 0.7888, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2541 - acc: 0.9688 - val_loss: 1.2324 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2358 - acc: 0.9688 - val_loss: 1.2277 - val_acc: 0.7090\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2021 - acc: 1.0000 - val_loss: 1.2220 - val_acc: 0.7098\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 1.2162 - val_acc: 0.7116\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2013 - acc: 1.0000 - val_loss: 1.2106 - val_acc: 0.7124\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2051 - acc: 1.0000 - val_loss: 1.2033 - val_acc: 0.7146\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1994 - acc: 1.0000 - val_loss: 1.1971 - val_acc: 0.7160\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2086 - acc: 1.0000 - val_loss: 1.1898 - val_acc: 0.7171\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1996 - acc: 1.0000 - val_loss: 1.1830 - val_acc: 0.7187\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2044 - acc: 1.0000 - val_loss: 1.1758 - val_acc: 0.7202\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "30 [supervised loss: 0.2044, acc: 100.00%] [unsupervised loss: 1.6035, acc: 85.94%] [labeled consistency loss: 0.6527, acc:acc: 96.88%] [unlabeled consistency loss: 0.9509, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2062 - acc: 1.0000 - val_loss: 1.3002 - val_acc: 0.6954\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2036 - acc: 1.0000 - val_loss: 1.3563 - val_acc: 0.6812\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2008 - acc: 1.0000 - val_loss: 1.4091 - val_acc: 0.6691\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2015 - acc: 1.0000 - val_loss: 1.4585 - val_acc: 0.6585\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2061 - acc: 1.0000 - val_loss: 1.5014 - val_acc: 0.6499\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2039 - acc: 1.0000 - val_loss: 1.5386 - val_acc: 0.6423\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2077 - acc: 1.0000 - val_loss: 1.5663 - val_acc: 0.6378\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2085 - acc: 1.0000 - val_loss: 1.5861 - val_acc: 0.6340\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2064 - acc: 1.0000 - val_loss: 1.5994 - val_acc: 0.6313\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2080 - acc: 1.0000 - val_loss: 1.6077 - val_acc: 0.6294\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "31 [supervised loss: 0.2080, acc: 100.00%] [unsupervised loss: 1.2796, acc: 92.19%] [labeled consistency loss: 0.5120, acc:acc: 96.88%] [unlabeled consistency loss: 0.7676, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2524 - acc: 1.0000 - val_loss: 1.6381 - val_acc: 0.6227\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2323 - acc: 1.0000 - val_loss: 1.6189 - val_acc: 0.6270\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3468 - acc: 0.9688 - val_loss: 1.5770 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2748 - acc: 0.9688 - val_loss: 1.5264 - val_acc: 0.6444\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2159 - acc: 1.0000 - val_loss: 1.4741 - val_acc: 0.6559\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2072 - acc: 1.0000 - val_loss: 1.4288 - val_acc: 0.6669\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2108 - acc: 1.0000 - val_loss: 1.3868 - val_acc: 0.6754\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2119 - acc: 1.0000 - val_loss: 1.3490 - val_acc: 0.6819\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2144 - acc: 1.0000 - val_loss: 1.3130 - val_acc: 0.6913\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2039 - acc: 1.0000 - val_loss: 1.2814 - val_acc: 0.6979\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "32 [supervised loss: 0.2039, acc: 100.00%] [unsupervised loss: 2.0318, acc: 89.06%] [labeled consistency loss: 0.7511, acc:acc: 93.75%] [unlabeled consistency loss: 1.2806, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2218 - acc: 1.0000 - val_loss: 1.3827 - val_acc: 0.6788\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2316 - acc: 1.0000 - val_loss: 1.4273 - val_acc: 0.6709\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2753 - acc: 0.9688 - val_loss: 1.4505 - val_acc: 0.6656\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2322 - acc: 1.0000 - val_loss: 1.4631 - val_acc: 0.6631\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2332 - acc: 1.0000 - val_loss: 1.4679 - val_acc: 0.6612\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2086 - acc: 1.0000 - val_loss: 1.4709 - val_acc: 0.6597\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2144 - acc: 1.0000 - val_loss: 1.4692 - val_acc: 0.6614\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2079 - acc: 1.0000 - val_loss: 1.4616 - val_acc: 0.6617\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2039 - acc: 1.0000 - val_loss: 1.4548 - val_acc: 0.6635\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2021 - acc: 1.0000 - val_loss: 1.4458 - val_acc: 0.6647\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "33 [supervised loss: 0.2021, acc: 100.00%] [unsupervised loss: 1.4257, acc: 93.75%] [labeled consistency loss: 0.6103, acc:acc: 93.75%] [unlabeled consistency loss: 0.8155, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2331 - acc: 1.0000 - val_loss: 1.5366 - val_acc: 0.6491\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2588 - acc: 1.0000 - val_loss: 1.5502 - val_acc: 0.6459\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2608 - acc: 0.9688 - val_loss: 1.5394 - val_acc: 0.6467\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2286 - acc: 1.0000 - val_loss: 1.5148 - val_acc: 0.6509\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3250 - acc: 0.9688 - val_loss: 1.4660 - val_acc: 0.6604\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2293 - acc: 1.0000 - val_loss: 1.4124 - val_acc: 0.6704\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2057 - acc: 1.0000 - val_loss: 1.3635 - val_acc: 0.6824\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2102 - acc: 1.0000 - val_loss: 1.3200 - val_acc: 0.6902\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2110 - acc: 1.0000 - val_loss: 1.2742 - val_acc: 0.7009\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2117 - acc: 1.0000 - val_loss: 1.2299 - val_acc: 0.7111\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "34 [supervised loss: 0.2117, acc: 100.00%] [unsupervised loss: 1.5818, acc: 89.06%] [labeled consistency loss: 0.7200, acc:acc: 93.75%] [unlabeled consistency loss: 0.8618, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2287 - acc: 1.0000 - val_loss: 1.2919 - val_acc: 0.6960\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2318 - acc: 1.0000 - val_loss: 1.3176 - val_acc: 0.6904\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2534 - acc: 1.0000 - val_loss: 1.3275 - val_acc: 0.6867\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2191 - acc: 1.0000 - val_loss: 1.3310 - val_acc: 0.6850\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2162 - acc: 1.0000 - val_loss: 1.3303 - val_acc: 0.6848\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2097 - acc: 1.0000 - val_loss: 1.3253 - val_acc: 0.6866\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2150 - acc: 1.0000 - val_loss: 1.3136 - val_acc: 0.6900\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2345 - acc: 1.0000 - val_loss: 1.2924 - val_acc: 0.6947\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2066 - acc: 1.0000 - val_loss: 1.2717 - val_acc: 0.6978\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 1.0000 - val_loss: 1.2522 - val_acc: 0.7029\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "35 [supervised loss: 0.2040, acc: 100.00%] [unsupervised loss: 1.3805, acc: 87.50%] [labeled consistency loss: 0.4848, acc:acc: 93.75%] [unlabeled consistency loss: 0.8957, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3354 - acc: 0.9062 - val_loss: 1.2549 - val_acc: 0.7027\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2479 - acc: 1.0000 - val_loss: 1.2355 - val_acc: 0.7078\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2356 - acc: 1.0000 - val_loss: 1.2159 - val_acc: 0.7132\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2415 - acc: 1.0000 - val_loss: 1.1980 - val_acc: 0.7182\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2238 - acc: 1.0000 - val_loss: 1.1818 - val_acc: 0.7225\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2191 - acc: 1.0000 - val_loss: 1.1676 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2199 - acc: 1.0000 - val_loss: 1.1559 - val_acc: 0.7303\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2170 - acc: 1.0000 - val_loss: 1.1471 - val_acc: 0.7320\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2051 - acc: 1.0000 - val_loss: 1.1397 - val_acc: 0.7342\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2054 - acc: 1.0000 - val_loss: 1.1344 - val_acc: 0.7355\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "36 [supervised loss: 0.2054, acc: 100.00%] [unsupervised loss: 2.2561, acc: 81.25%] [labeled consistency loss: 1.0693, acc:acc: 90.62%] [unlabeled consistency loss: 1.1868, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2583 - acc: 0.9688 - val_loss: 1.2308 - val_acc: 0.7076\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2272 - acc: 1.0000 - val_loss: 1.2502 - val_acc: 0.7007\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2142 - acc: 1.0000 - val_loss: 1.2674 - val_acc: 0.6940\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2259 - acc: 1.0000 - val_loss: 1.2721 - val_acc: 0.6915\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2147 - acc: 1.0000 - val_loss: 1.2709 - val_acc: 0.6904\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2121 - acc: 1.0000 - val_loss: 1.2647 - val_acc: 0.6894\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 1.2564 - val_acc: 0.6924\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 1.2459 - val_acc: 0.6930\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2036 - acc: 1.0000 - val_loss: 1.2349 - val_acc: 0.6951\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2070 - acc: 1.0000 - val_loss: 1.2244 - val_acc: 0.6976\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "37 [supervised loss: 0.2070, acc: 100.00%] [unsupervised loss: 1.8012, acc: 81.25%] [labeled consistency loss: 0.7302, acc:acc: 78.12%] [unlabeled consistency loss: 1.0711, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2197 - acc: 1.0000 - val_loss: 1.4172 - val_acc: 0.6575\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2084 - acc: 1.0000 - val_loss: 1.4737 - val_acc: 0.6465\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2193 - acc: 1.0000 - val_loss: 1.5228 - val_acc: 0.6358\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2163 - acc: 1.0000 - val_loss: 1.5618 - val_acc: 0.6288\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2214 - acc: 1.0000 - val_loss: 1.5876 - val_acc: 0.6257\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2052 - acc: 1.0000 - val_loss: 1.6051 - val_acc: 0.6226\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2172 - acc: 1.0000 - val_loss: 1.6111 - val_acc: 0.6230\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2083 - acc: 1.0000 - val_loss: 1.6126 - val_acc: 0.6226\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2043 - acc: 1.0000 - val_loss: 1.6104 - val_acc: 0.6221\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2845 - acc: 0.9688 - val_loss: 1.5851 - val_acc: 0.6276\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "38 [supervised loss: 0.2845, acc: 96.88%] [unsupervised loss: 1.1752, acc: 89.06%] [labeled consistency loss: 0.5797, acc:acc: 87.50%] [unlabeled consistency loss: 0.5955, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3602 - acc: 0.9688 - val_loss: 1.7380 - val_acc: 0.6057\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3442 - acc: 0.9688 - val_loss: 1.7590 - val_acc: 0.6049\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3469 - acc: 0.9375 - val_loss: 1.7234 - val_acc: 0.6106\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2324 - acc: 1.0000 - val_loss: 1.6741 - val_acc: 0.6174\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3135 - acc: 0.9688 - val_loss: 1.6044 - val_acc: 0.6280\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2694 - acc: 0.9688 - val_loss: 1.5349 - val_acc: 0.6418\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2251 - acc: 1.0000 - val_loss: 1.4712 - val_acc: 0.6539\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2179 - acc: 1.0000 - val_loss: 1.4099 - val_acc: 0.6662\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2054 - acc: 1.0000 - val_loss: 1.3586 - val_acc: 0.6761\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2073 - acc: 1.0000 - val_loss: 1.3133 - val_acc: 0.6885\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "39 [supervised loss: 0.2073, acc: 100.00%] [unsupervised loss: 2.3797, acc: 78.12%] [labeled consistency loss: 1.4620, acc:acc: 71.88%] [unlabeled consistency loss: 0.9178, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2400 - acc: 0.9688 - val_loss: 1.4375 - val_acc: 0.6598\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2173 - acc: 1.0000 - val_loss: 1.4652 - val_acc: 0.6539\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2058 - acc: 1.0000 - val_loss: 1.4868 - val_acc: 0.6494\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2191 - acc: 1.0000 - val_loss: 1.4948 - val_acc: 0.6469\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2142 - acc: 1.0000 - val_loss: 1.4942 - val_acc: 0.6470\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2064 - acc: 1.0000 - val_loss: 1.4888 - val_acc: 0.6479\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2322 - acc: 1.0000 - val_loss: 1.4778 - val_acc: 0.6496\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2048 - acc: 1.0000 - val_loss: 1.4639 - val_acc: 0.6516\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2070 - acc: 1.0000 - val_loss: 1.4453 - val_acc: 0.6539\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1982 - acc: 1.0000 - val_loss: 1.4282 - val_acc: 0.6569\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "40 [supervised loss: 0.1982, acc: 100.00%] [unsupervised loss: 1.5576, acc: 84.38%] [labeled consistency loss: 0.7255, acc:acc: 84.38%] [unlabeled consistency loss: 0.8321, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7509 - acc: 0.9375 - val_loss: 1.6492 - val_acc: 0.6175\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4857 - acc: 0.9688 - val_loss: 1.6092 - val_acc: 0.6192\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6705 - acc: 0.8750 - val_loss: 1.4916 - val_acc: 0.6371\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2849 - acc: 0.9688 - val_loss: 1.3904 - val_acc: 0.6544\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2366 - acc: 1.0000 - val_loss: 1.3239 - val_acc: 0.6718\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2141 - acc: 1.0000 - val_loss: 1.2853 - val_acc: 0.6812\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2109 - acc: 1.0000 - val_loss: 1.2651 - val_acc: 0.6869\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2113 - acc: 1.0000 - val_loss: 1.2565 - val_acc: 0.6907\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2044 - acc: 1.0000 - val_loss: 1.2554 - val_acc: 0.6913\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2184 - acc: 1.0000 - val_loss: 1.2580 - val_acc: 0.6906\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "41 [supervised loss: 0.2184, acc: 100.00%] [unsupervised loss: 3.0290, acc: 84.38%] [labeled consistency loss: 1.8515, acc:acc: 87.50%] [unlabeled consistency loss: 1.1775, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2016 - acc: 1.0000 - val_loss: 1.2727 - val_acc: 0.6849\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2148 - acc: 1.0000 - val_loss: 1.2815 - val_acc: 0.6809\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2283 - acc: 1.0000 - val_loss: 1.2853 - val_acc: 0.6780\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 1.2877 - val_acc: 0.6755\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2085 - acc: 1.0000 - val_loss: 1.2893 - val_acc: 0.6749\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2214 - acc: 1.0000 - val_loss: 1.2877 - val_acc: 0.6753\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2120 - acc: 1.0000 - val_loss: 1.2837 - val_acc: 0.6747\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2220 - acc: 1.0000 - val_loss: 1.2764 - val_acc: 0.6765\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2072 - acc: 1.0000 - val_loss: 1.2680 - val_acc: 0.6798\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3676 - acc: 0.9688 - val_loss: 1.2555 - val_acc: 0.6835\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "42 [supervised loss: 0.3676, acc: 96.88%] [unsupervised loss: 1.2555, acc: 89.06%] [labeled consistency loss: 0.4084, acc:acc: 90.62%] [unlabeled consistency loss: 0.8472, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2316 - acc: 1.0000 - val_loss: 1.2550 - val_acc: 0.6850\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2613 - acc: 1.0000 - val_loss: 1.2545 - val_acc: 0.6848\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2433 - acc: 0.9688 - val_loss: 1.2534 - val_acc: 0.6856\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2424 - acc: 1.0000 - val_loss: 1.2508 - val_acc: 0.6862\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2084 - acc: 1.0000 - val_loss: 1.2475 - val_acc: 0.6871\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2074 - acc: 1.0000 - val_loss: 1.2453 - val_acc: 0.6878\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2041 - acc: 1.0000 - val_loss: 1.2424 - val_acc: 0.6906\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2199 - acc: 1.0000 - val_loss: 1.2370 - val_acc: 0.6920\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1998 - acc: 1.0000 - val_loss: 1.2318 - val_acc: 0.6925\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2028 - acc: 1.0000 - val_loss: 1.2273 - val_acc: 0.6943\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "43 [supervised loss: 0.2028, acc: 100.00%] [unsupervised loss: 1.7129, acc: 84.38%] [labeled consistency loss: 0.9017, acc:acc: 78.12%] [unlabeled consistency loss: 0.8112, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 1.2651 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2916 - acc: 1.0000 - val_loss: 1.2704 - val_acc: 0.6852\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2839 - acc: 0.9688 - val_loss: 1.2641 - val_acc: 0.6859\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2488 - acc: 1.0000 - val_loss: 1.2544 - val_acc: 0.6898\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2408 - acc: 1.0000 - val_loss: 1.2437 - val_acc: 0.6945\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2169 - acc: 1.0000 - val_loss: 1.2340 - val_acc: 0.6990\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2132 - acc: 1.0000 - val_loss: 1.2251 - val_acc: 0.7029\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2131 - acc: 1.0000 - val_loss: 1.2154 - val_acc: 0.7061\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2547 - acc: 0.9688 - val_loss: 1.2004 - val_acc: 0.7107\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2045 - acc: 1.0000 - val_loss: 1.1871 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "44 [supervised loss: 0.2045, acc: 100.00%] [unsupervised loss: 2.0487, acc: 81.25%] [labeled consistency loss: 1.0562, acc:acc: 81.25%] [unlabeled consistency loss: 0.9925, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 1.2038 - val_acc: 0.7074\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2004 - acc: 1.0000 - val_loss: 1.2185 - val_acc: 0.7036\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2130 - acc: 1.0000 - val_loss: 1.2285 - val_acc: 0.7022\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2019 - acc: 1.0000 - val_loss: 1.2382 - val_acc: 0.7005\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2073 - acc: 1.0000 - val_loss: 1.2449 - val_acc: 0.7000\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2675 - acc: 0.9688 - val_loss: 1.2431 - val_acc: 0.7013\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2026 - acc: 1.0000 - val_loss: 1.2407 - val_acc: 0.7022\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2075 - acc: 1.0000 - val_loss: 1.2350 - val_acc: 0.7027\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2009 - acc: 1.0000 - val_loss: 1.2301 - val_acc: 0.7037\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1978 - acc: 1.0000 - val_loss: 1.2269 - val_acc: 0.7050\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "45 [supervised loss: 0.1978, acc: 100.00%] [unsupervised loss: 1.7007, acc: 81.25%] [labeled consistency loss: 0.5178, acc:acc: 93.75%] [unlabeled consistency loss: 1.1830, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2057 - acc: 1.0000 - val_loss: 1.2953 - val_acc: 0.6917\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2236 - acc: 1.0000 - val_loss: 1.3268 - val_acc: 0.6844\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2351 - acc: 1.0000 - val_loss: 1.3605 - val_acc: 0.6785\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2200 - acc: 1.0000 - val_loss: 1.3942 - val_acc: 0.6700\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2481 - acc: 0.9688 - val_loss: 1.4253 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2152 - acc: 1.0000 - val_loss: 1.4546 - val_acc: 0.6586\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2118 - acc: 1.0000 - val_loss: 1.4821 - val_acc: 0.6545\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2166 - acc: 1.0000 - val_loss: 1.5075 - val_acc: 0.6499\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2022 - acc: 1.0000 - val_loss: 1.5315 - val_acc: 0.6466\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2158 - acc: 1.0000 - val_loss: 1.5519 - val_acc: 0.6456\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "46 [supervised loss: 0.2158, acc: 100.00%] [unsupervised loss: 1.4174, acc: 96.88%] [labeled consistency loss: 0.5832, acc:acc: 93.75%] [unlabeled consistency loss: 0.8342, acc: 100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2440 - acc: 1.0000 - val_loss: 1.5932 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2664 - acc: 0.9688 - val_loss: 1.5967 - val_acc: 0.6416\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2388 - acc: 1.0000 - val_loss: 1.5947 - val_acc: 0.6421\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2245 - acc: 1.0000 - val_loss: 1.5982 - val_acc: 0.6441\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2122 - acc: 1.0000 - val_loss: 1.6050 - val_acc: 0.6457\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1985 - acc: 1.0000 - val_loss: 1.6126 - val_acc: 0.6462\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2038 - acc: 1.0000 - val_loss: 1.6195 - val_acc: 0.6480\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2004 - acc: 1.0000 - val_loss: 1.6289 - val_acc: 0.6477\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1983 - acc: 1.0000 - val_loss: 1.6382 - val_acc: 0.6489\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1998 - acc: 1.0000 - val_loss: 1.6459 - val_acc: 0.6489\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "47 [supervised loss: 0.1998, acc: 100.00%] [unsupervised loss: 1.3481, acc: 92.19%] [labeled consistency loss: 0.5873, acc:acc: 93.75%] [unlabeled consistency loss: 0.7608, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2092 - acc: 1.0000 - val_loss: 1.8132 - val_acc: 0.6284\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2302 - acc: 1.0000 - val_loss: 1.8672 - val_acc: 0.6207\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 1.0000 - val_loss: 1.9110 - val_acc: 0.6157\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2059 - acc: 1.0000 - val_loss: 1.9487 - val_acc: 0.6119\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2002 - acc: 1.0000 - val_loss: 1.9790 - val_acc: 0.6088\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2023 - acc: 1.0000 - val_loss: 2.0033 - val_acc: 0.6065\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2026 - acc: 1.0000 - val_loss: 2.0198 - val_acc: 0.6043\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2024 - acc: 1.0000 - val_loss: 2.0332 - val_acc: 0.6020\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2085 - acc: 1.0000 - val_loss: 2.0322 - val_acc: 0.6029\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2171 - acc: 1.0000 - val_loss: 2.0309 - val_acc: 0.6031\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "48 [supervised loss: 0.2171, acc: 100.00%] [unsupervised loss: 1.8820, acc: 76.56%] [labeled consistency loss: 0.7228, acc:acc: 90.62%] [unlabeled consistency loss: 1.1592, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2143 - acc: 1.0000 - val_loss: 2.3677 - val_acc: 0.5668\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2156 - acc: 1.0000 - val_loss: 2.4671 - val_acc: 0.5538\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2037 - acc: 1.0000 - val_loss: 2.5506 - val_acc: 0.5406\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2142 - acc: 1.0000 - val_loss: 2.6014 - val_acc: 0.5340\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2035 - acc: 1.0000 - val_loss: 2.6417 - val_acc: 0.5281\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2025 - acc: 1.0000 - val_loss: 2.6757 - val_acc: 0.5228\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2140 - acc: 1.0000 - val_loss: 2.6792 - val_acc: 0.5210\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2219 - acc: 1.0000 - val_loss: 2.6544 - val_acc: 0.5219\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2123 - acc: 1.0000 - val_loss: 2.6159 - val_acc: 0.5247\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2083 - acc: 1.0000 - val_loss: 2.5762 - val_acc: 0.5290\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "49 [supervised loss: 0.2083, acc: 100.00%] [unsupervised loss: 1.7218, acc: 85.94%] [labeled consistency loss: 0.6576, acc:acc: 96.88%] [unlabeled consistency loss: 1.0643, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2018 - acc: 1.0000 - val_loss: 2.8050 - val_acc: 0.5034\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.86590\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2271 - acc: 1.0000 - val_loss: 2.9133 - val_acc: 0.4933\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86590\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2040 - acc: 1.0000 - val_loss: 3.0083 - val_acc: 0.4835\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86590\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2078 - acc: 1.0000 - val_loss: 3.0910 - val_acc: 0.4771\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.86590\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2037 - acc: 1.0000 - val_loss: 3.1572 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.86590\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2144 - acc: 1.0000 - val_loss: 3.1999 - val_acc: 0.4672\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.86590\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2168 - acc: 1.0000 - val_loss: 3.2249 - val_acc: 0.4652\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.86590\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2177 - acc: 1.0000 - val_loss: 3.2163 - val_acc: 0.4669\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.86590\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2079 - acc: 1.0000 - val_loss: 3.1990 - val_acc: 0.4687\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.86590\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2019 - acc: 1.0000 - val_loss: 3.1771 - val_acc: 0.4712\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.86590\n",
            "50 [supervised loss: 0.2019, acc: 100.00%] [unsupervised loss: 1.6363, acc: 84.38%] [labeled consistency loss: 0.7438, acc:acc: 84.38%] [unlabeled consistency loss: 0.8925, acc: 84.38%]\n",
            "Training time: 1800.0745s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ndAup4NMwkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "39f3d9b7-703e-45bf-ad7d-6eef74644b53"
      },
      "source": [
        "plot_supervised_losses = np.array(supervised_losses)\n",
        "plot_unsupervised_losses = np.array(unsupervised_losses)\n",
        "plot_labeled_consistency_costs = np.array(labeled_consistency_costs)\n",
        "plot_unlabeled_consistency_costs = np.array(unlabeled_consistency_costs)\n",
        "plot_all_losses = np.array(supervised_losses)+np.array(unsupervised_losses)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, plot_all_losses, label=\"All loss\", color='black')\n",
        "plt.plot(iteration_checkpoints, plot_supervised_losses, label=\"Supervised loss\", color='tab:blue')\n",
        "plt.plot(iteration_checkpoints, plot_unsupervised_losses, label=\"Unsupervised loss\", color='tab:green')\n",
        "plt.plot(iteration_checkpoints, plot_labeled_consistency_costs, label=\"Labeled consistency loss\", color='tab:red', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, plot_unlabeled_consistency_costs, label=\"Unlabeled consistency loss\", color='tab:orange', linestyle='dashed')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Mean Teacher's Supervised and Unsupervised Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc8fb1bc9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFPCAYAAAAfjmxyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omcpkiBMwiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-50.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftgecc1jMwfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-50.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDJmNbdMwce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68eb5e66-3367-436d-e556-b44cc4820f93"
      },
      "source": [
        "accs = []\n",
        "# tx = [x for x in range(1,51,1)]\n",
        "tx = [x for x in range(1, len(iteration_checkpoints)+1, 1)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"Mean Teacher's accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 421us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 420us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 424us/step\n",
            "10000/10000 [==============================] - 4s 431us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 420us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 408us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 405us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 404us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 407us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 407us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "0.8533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc8fb0f0ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFPCAYAAAASkBw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2feQLXtMwY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6e15b48d-daa1-4345-a0c6-a2c993c2e263"
      },
      "source": [
        "print(max(accs[2:]))\n",
        "# for acc in accs:\n",
        "#   print(acc)\n",
        "mt_accs = accs\n",
        "print(mt_accs)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8474\n",
            "[0.844, 0.8533, 0.8474, 0.8394, 0.8303, 0.8231, 0.8054, 0.8058, 0.7995, 0.8036, 0.8111, 0.7992, 0.7824, 0.7816, 0.7803, 0.7868, 0.7843, 0.788, 0.7839, 0.7829, 0.7835, 0.7941, 0.7895, 0.7783, 0.7664, 0.7472, 0.7378, 0.7458, 0.7307, 0.7201, 0.6772, 0.6863, 0.6744, 0.6901, 0.6975, 0.7185, 0.6978, 0.6552, 0.6668, 0.6506, 0.6972, 0.6987, 0.6956, 0.7102, 0.7097, 0.6856, 0.6655, 0.6315, 0.5788, 0.5099]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UrCrrObhruT",
        "colab_type": "text"
      },
      "source": [
        "# To be deleted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N7bCb6u7h7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-eb3VpX7mRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'useless_models')\n",
        "model_name = 'cifar10_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.accs = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.accs.append(logs.get('acc'))\n",
        "\n",
        "history = LossHistory()\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0bnB6CI_zts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik5uq5X3Rbi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations, batch_size, iter_epochs, save_interval):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        imgs, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Compute quantities required for featurewise normalization (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(imgs)\n",
        "        model.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        \n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "            losses.append(history.losses[-1])\n",
        "            accs.append(history.accs[-1])\n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "            discriminator_supervised.save(\"./models/model-\" + str(iteration+1) + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr0Q7EaWOar9",
        "colab_type": "code",
        "outputId": "db7014bf-2598-4046-ab57-4e80f44640dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iterations = 5\n",
        "iter_epochs = 10\n",
        "save_interval = 10\n",
        "batch_size = 32\n",
        "num_labeled = 100\n",
        "losses = []\n",
        "accs = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "dataset = Dataset_CIFAR10(num_labeled)\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "train(iterations, batch_size, iter_epochs, save_interval)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Use time:\" + str(endtime-starttime) + \"s\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2716 - acc: 1.0000 - val_loss: 1.5633 - val_acc: 0.7275\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73420\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2799 - acc: 1.0000 - val_loss: 1.5599 - val_acc: 0.7273\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73420\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2757 - acc: 1.0000 - val_loss: 1.5565 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73420\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2784 - acc: 1.0000 - val_loss: 1.5532 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73420\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2789 - acc: 1.0000 - val_loss: 1.5482 - val_acc: 0.7278\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73420\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2742 - acc: 1.0000 - val_loss: 1.5430 - val_acc: 0.7291\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73420\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2996 - acc: 1.0000 - val_loss: 1.5316 - val_acc: 0.7308\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73420\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2808 - acc: 1.0000 - val_loss: 1.5165 - val_acc: 0.7327\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73420\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 1.5049 - val_acc: 0.7339\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73420\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2722 - acc: 1.0000 - val_loss: 1.4943 - val_acc: 0.7351\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.73420 to 0.73510, saving model to /content/saved_models/cifar10_ResNet20v1_model.010.h5\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3956 - acc: 0.9688 - val_loss: 1.4862 - val_acc: 0.7348\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73510\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3032 - acc: 1.0000 - val_loss: 1.4795 - val_acc: 0.7346\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73510\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3062 - acc: 1.0000 - val_loss: 1.4726 - val_acc: 0.7359\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.73510 to 0.73590, saving model to /content/saved_models/cifar10_ResNet20v1_model.003.h5\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2877 - acc: 1.0000 - val_loss: 1.4676 - val_acc: 0.7361\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.73590 to 0.73610, saving model to /content/saved_models/cifar10_ResNet20v1_model.004.h5\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3063 - acc: 0.9688 - val_loss: 1.4657 - val_acc: 0.7368\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.73610 to 0.73680, saving model to /content/saved_models/cifar10_ResNet20v1_model.005.h5\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3436 - acc: 0.9688 - val_loss: 1.4577 - val_acc: 0.7382\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.73680 to 0.73820, saving model to /content/saved_models/cifar10_ResNet20v1_model.006.h5\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2751 - acc: 1.0000 - val_loss: 1.4565 - val_acc: 0.7365\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3166 - acc: 0.9688 - val_loss: 1.4655 - val_acc: 0.7338\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2734 - acc: 1.0000 - val_loss: 1.4773 - val_acc: 0.7321\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3418 - acc: 0.9688 - val_loss: 1.4931 - val_acc: 0.7280\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2748 - acc: 1.0000 - val_loss: 1.5108 - val_acc: 0.7239\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73820\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2711 - acc: 1.0000 - val_loss: 1.5287 - val_acc: 0.7225\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73820\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.5459 - val_acc: 0.7203\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73820\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2706 - acc: 1.0000 - val_loss: 1.5630 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73820\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2713 - acc: 1.0000 - val_loss: 1.5802 - val_acc: 0.7174\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2733 - acc: 1.0000 - val_loss: 1.5959 - val_acc: 0.7148\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2782 - acc: 1.0000 - val_loss: 1.6087 - val_acc: 0.7132\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2755 - acc: 1.0000 - val_loss: 1.6190 - val_acc: 0.7111\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2965 - acc: 1.0000 - val_loss: 1.6283 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.6368 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2950 - acc: 0.9688 - val_loss: 1.6413 - val_acc: 0.7072\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73820\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2727 - acc: 1.0000 - val_loss: 1.6451 - val_acc: 0.7056\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73820\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2873 - acc: 1.0000 - val_loss: 1.6464 - val_acc: 0.7052\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73820\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2858 - acc: 1.0000 - val_loss: 1.6465 - val_acc: 0.7047\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73820\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2697 - acc: 1.0000 - val_loss: 1.6467 - val_acc: 0.7043\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2707 - acc: 1.0000 - val_loss: 1.6466 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4211 - acc: 0.9688 - val_loss: 1.6265 - val_acc: 0.7052\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2730 - acc: 1.0000 - val_loss: 1.6096 - val_acc: 0.7068\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2732 - acc: 1.0000 - val_loss: 1.5967 - val_acc: 0.7071\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2811 - acc: 1.0000 - val_loss: 1.5849 - val_acc: 0.7078\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2735 - acc: 1.0000 - val_loss: 1.5728 - val_acc: 0.7091\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73820\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2724 - acc: 1.0000 - val_loss: 1.5632 - val_acc: 0.7099\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73820\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.5545 - val_acc: 0.7106\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73820\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2763 - acc: 1.0000 - val_loss: 1.5466 - val_acc: 0.7116\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73820\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2727 - acc: 1.0000 - val_loss: 1.5397 - val_acc: 0.7119\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2865 - acc: 1.0000 - val_loss: 1.5371 - val_acc: 0.7122\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 1.5350 - val_acc: 0.7128\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2737 - acc: 1.0000 - val_loss: 1.5334 - val_acc: 0.7139\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2711 - acc: 1.0000 - val_loss: 1.5314 - val_acc: 0.7140\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2711 - acc: 1.0000 - val_loss: 1.5306 - val_acc: 0.7148\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "[0.27351266, 0.27242652, 0.27099964, 0.27627712, 0.27269456, 0.286533, 0.2704616, 0.27368137, 0.27110267, 0.27113327]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAIJ9nrnaNjk",
        "colab_type": "code",
        "outputId": "a194db45-69be-467c-8e6c-d8c1e1fef82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Score trained model.\n",
        "x_test, y_test = dataset.test_set()\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 315us/step\n",
            "Test loss: 1.5306017761230468\n",
            "Test accuracy: 0.7148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-sKr5pbY390",
        "colab_type": "code",
        "outputId": "1af8d53b-fb53-4776-c5d6-6da612570555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(losses)\n",
        "print(accs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Y7noknSmXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}