{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xDiTP_67MV",
        "colab_type": "code",
        "outputId": "4750f6c0-b733-48cc-d3c5-dda3ad2c0c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import (Dense, Conv2D, BatchNormalization, Activation, \n",
        "                          AveragePooling2D, Input, Flatten, \n",
        "                          Concatenate, Dropout, Lambda, \n",
        "                          Reshape, Embedding, Multiply)\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, Callback\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from __future__ import print_function\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSaEOfjG6ztu",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_hPw_jy8jea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset_CIFAR10:\n",
        "    def __init__(self, num_labeled):\n",
        "\n",
        "        def preprocess_imgs(x):\n",
        "            # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "            return x\n",
        "\n",
        "        def preprocess_labels(y):\n",
        "            y = y.reshape(-1, 1)\n",
        "            y = to_categorical(y, num_classes = 10)\n",
        "            return y\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train = preprocess_imgs(x_train)\n",
        "        y_train = preprocess_labels(y_train)\n",
        "        x_test = preprocess_imgs(x_test)\n",
        "        y_test = preprocess_labels(y_test)\n",
        "\n",
        "        # Number labeled examples to use for training\n",
        "        self.num_labeled = num_labeled\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        del x_train, y_train, x_test, y_test\n",
        "\n",
        "    def batch_labeled(self, batch_size):\n",
        "        # Get a random batch of labeled images and their labels\n",
        "        idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        labels = self.y_train[idx]\n",
        "        return imgs, labels\n",
        "\n",
        "    def batch_unlabeled(self, batch_size):\n",
        "        # Get a random batch of unlabeled images\n",
        "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        return imgs\n",
        "\n",
        "    def training_set(self):\n",
        "        return self.x_train, self.y_train\n",
        "\n",
        "    def test_set(self):\n",
        "        return self.x_test, self.y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5nWXxcZdgQb",
        "colab_type": "text"
      },
      "source": [
        "## Check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5jJexNdigk",
        "colab_type": "code",
        "outputId": "b08e22d1-1dac-4212-eede-bb6718c0d3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Training set\n",
        "d_ytrain = {}\n",
        "for i in range(10):\n",
        "  d_ytrain[i] = 0\n",
        "for i in range(len(y_train)):\n",
        "  d_ytrain[y_train[i][0]] = d_ytrain.get(y_train[i][0]) + 1\n",
        "print(\"CIFAR-10 training set:\")\n",
        "print(d_ytrain)\n",
        "\n",
        "# Test set\n",
        "d_ytest = {}\n",
        "for i in range(10):\n",
        "  d_ytest[i] = 0\n",
        "for i in range(len(y_test)):\n",
        "  d_ytest[y_test[i][0]] = d_ytest.get(y_test[i][0]) + 1\n",
        "print(\"CIFAR-10 test set:\")\n",
        "print(d_ytest)\n",
        "\n",
        "del x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CIFAR-10 training set:\n",
            "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n",
            "CIFAR-10 test set:\n",
            "{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrpoGSKdmrp",
        "colab_type": "text"
      },
      "source": [
        "# Number of labeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvYSG5vKdoSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labeled = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjhxJulqdyFs",
        "colab_type": "text"
      },
      "source": [
        "# SCGAN-2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcTHBiShR9m",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA0DtW_2d14g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "input_shape = img_shape = (32, 32, 3)\n",
        "num_classes = 10\n",
        "z_dim = 100   # Size of the noise vector, used as input to the Generator\n",
        "n = 3\n",
        "depth = n * 6 + 2   # Depth of ResNet model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgzlORUA7eUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eZ2B42_7fiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abhpWtyBfaxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "def build_generator(z_dim):\n",
        "  \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ), dtype='float32')\n",
        "    label_embedding = Dense(z_dim, input_dim=num_classes)(label)\n",
        "    joined_representation = Multiply()([z, label_embedding])\n",
        "    \n",
        "#     model = Sequential()\n",
        "\n",
        "    # Reshape input into 8x8x256 tensor via a fully connected layer\n",
        "    model = Dense(256 * 8 * 8, input_dim=z_dim)(joined_representation)\n",
        "    model = Reshape((8, 8, 256))(model)\n",
        "\n",
        "    # Transposed convolution layer, from 8x8x256 into 16x16x128 tensor\n",
        "    model = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(model)\n",
        "\n",
        "    # Batch normalization\n",
        "    model = BatchNormalization()(model)\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model = LeakyReLU(alpha=0.01)(model)\n",
        "\n",
        "    # Transposed convolution layer, from 16x16x128 to 16x16x64 tensor\n",
        "    model = Conv2DTranspose(64, kernel_size=3, strides=1, padding='same')(model)\n",
        "\n",
        "    # Batch normalization\n",
        "    model = BatchNormalization()(model)\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model = LeakyReLU(alpha=0.01)(model)\n",
        "\n",
        "    # Transposed convolution layer, from 16x16x64 to 32x32x3 tensor\n",
        "    model = Conv2DTranspose(3, kernel_size=3, strides=2, padding='same')(model)\n",
        "\n",
        "    # Output layer with tanh activation\n",
        "    conditioned_img = Activation('tanh')(model)\n",
        "    \n",
        "#     conditioned_img = model(joined_representation)\n",
        "\n",
        "    model = Model([z, label], conditioned_img)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ09_xGfffbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_net(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes, kernel_initializer='he_normal')(y)\n",
        "    # outputs = Dense(num_classes,\n",
        "    #                 activation='softmax',\n",
        "    #                 kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx6rxPK2hiqj",
        "colab_type": "code",
        "outputId": "61923916-4b3a-44e2-a230-b56393aa1280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "build_discriminator_net(img_shape, depth).summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 32, 32, 16)   448         input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 32, 32, 16)   64          conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 32, 32, 16)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 32, 32, 16)   2320        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 32, 32, 16)   64          conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 32, 32, 16)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 32, 32, 16)   2320        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 32, 32, 16)   64          conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 32, 32, 16)   0           activation_123[0][0]             \n",
            "                                                                 batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 32, 32, 16)   0           add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 32, 32, 16)   2320        activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 32, 32, 16)   64          conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 32, 32, 16)   0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 32, 32, 16)   2320        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 32, 32, 16)   64          conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 32, 32, 16)   0           activation_125[0][0]             \n",
            "                                                                 batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 32, 32, 16)   0           add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 32, 32, 16)   2320        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 32, 32, 16)   64          conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 32, 32, 16)   0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 16)   2320        activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 32, 32, 16)   64          conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 32, 32, 16)   0           activation_127[0][0]             \n",
            "                                                                 batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 32, 32, 16)   0           add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 16, 16, 32)   4640        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 16, 16, 32)   128         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 16, 32)   0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 16, 16, 32)   9248        activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 16, 16, 32)   544         activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 16, 16, 32)   128         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 16, 16, 32)   0           conv2d_136[0][0]                 \n",
            "                                                                 batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 16, 16, 32)   0           add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 16, 16, 32)   9248        activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 32)   128         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 16, 16, 32)   0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 32)   9248        activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 16, 16, 32)   128         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 16, 16, 32)   0           activation_131[0][0]             \n",
            "                                                                 batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 16, 16, 32)   0           add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 32)   9248        activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 16, 16, 32)   128         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 16, 16, 32)   0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 32)   9248        activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 16, 16, 32)   128         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 16, 16, 32)   0           activation_133[0][0]             \n",
            "                                                                 batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 16, 16, 32)   0           add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 8, 8, 64)     18496       activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 64)     256         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 64)     0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 8, 8, 64)     36928       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 8, 8, 64)     2112        activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 64)     256         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 8, 8, 64)     0           conv2d_143[0][0]                 \n",
            "                                                                 batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 64)     0           add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 8, 8, 64)     36928       activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 8, 8, 64)     256         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 64)     0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 64)     36928       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 8, 8, 64)     256         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 8, 8, 64)     0           activation_137[0][0]             \n",
            "                                                                 batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 64)     0           add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 64)     36928       activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 8, 8, 64)     256         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 8, 8, 64)     0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 64)     36928       activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 8, 8, 64)     256         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 8, 8, 64)     0           activation_139[0][0]             \n",
            "                                                                 batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 8, 8, 64)     0           add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 1, 1, 64)     0           activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 64)           0           average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 10)           650         flatten_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTS0nBXhMhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_supervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    # Softmax activation, giving predicted probability distribution over the real classes\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuxULq-IhOlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_unsupervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    def predict(x):\n",
        "        # Transform distribution over real classes into a binary real-vs-fake probability\n",
        "        prediction = 1.0 - (1.0 / (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
        "        return prediction\n",
        "\n",
        "    # 'Real-vs-fake' output neuron defined above\n",
        "    model.add(Lambda(predict))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3LcMzjZhQFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ))\n",
        "    img = generator([z, label])\n",
        "    output = discriminator(img)\n",
        "    model = Model([z, label], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X4FZirFhWA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Core Discriminator network:\n",
        "# These layers are shared during supervised and unsupervised training\n",
        "\n",
        "discriminator_net = build_discriminator_net(input_shape=img_shape, depth=depth)\n",
        "\n",
        "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
        "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "# discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "#                                  metrics=['accuracy'],\n",
        "#                                  optimizer=Adam(lr=lr_schedule(0)))\n",
        "\n",
        "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
        "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "                                metrics=['accuracy'],\n",
        "                                optimizer=Adam())\n",
        "# discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "#                                 metrics=['accuracy'],\n",
        "#                                 optimizer=Adam(lr=lr_schedule(0)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1nVA_1egKoX",
        "colab_type": "code",
        "outputId": "086537e3-5a04-4b68-8b8e-22cf20ffb108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "discriminator_unsupervised.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_1 (Model)              (None, 10)                274442    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpsqRccThV02",
        "colab_type": "code",
        "outputId": "f9c6fe23-c8d2-4879-ca98-8b98dbfced09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Build the Generator\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "discriminator_supervised.trainable = False\n",
        "discriminator_unsupervised.trainable = False\n",
        "gan = build_gan(generator, discriminator_unsupervised)\n",
        "gan.compile(loss='binary_crossentropy', \n",
        "            metrics=['accuracy'], \n",
        "            optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yrZyUchhhER",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW25Txjghf7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir models\n",
        "%mkdir losses\n",
        "%mkdir models/models-label-1000\n",
        "%mkdir losses/losses-label-1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNb1Q6IQkL-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'models')\n",
        "model_name = 'cifar10_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.accs = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.accs.append(logs.get('acc'))\n",
        "\n",
        "history = LossHistory()\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3q3OrbkUGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR2Hun3ChbSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain(iterations_pre, batch_size, save_interval, iter_epochs):\n",
        "  for iteration in range(iterations_pre):\n",
        "      imgs, labels = dataset.training_set()\n",
        "      # imgs, labels = dataset.batch_labeled(batch_size)\n",
        "      x_test, y_test = dataset.test_set()\n",
        "\n",
        "      # Compute quantities required for featurewise normalization (std, mean, and principal components if ZCA whitening is applied).\n",
        "      datagen.fit(imgs)\n",
        "      discriminator_supervised.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  epochs=iter_epochs, verbose=1, workers=4,\n",
        "                  callbacks=callbacks)\n",
        "      \n",
        "      if (iteration + 1) % save_interval == 0:\n",
        "          \n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [D loss class: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, history.losses[-1], 100 * history.accs[-1]))\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "          losses.append(history.losses[-1])\n",
        "          accs.append(history.accs[-1])\n",
        "          discriminator_supervised.save_weights(\"./models/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "          \n",
        "          # x, y = dataset.training_set()\n",
        "          # _, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "          # print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGIfXoRgW-0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def pretrain(iterations_pre, batch_size, save_interval):\n",
        "#   for iteration in range(iterations_pre):\n",
        "#       # imgs, labels = dataset.training_set()\n",
        "#       imgs, labels = dataset.batch_labeled(1000)\n",
        "      \n",
        "#       loss, acc = discriminator_supervised.train_on_batch(imgs, labels)\n",
        "      \n",
        "#       if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "#           losses.append(loss)\n",
        "#           accs.append(acc)\n",
        "#           iteration_checkpoints.append(iteration + 1)\n",
        "          \n",
        "#           # Output training progress\n",
        "#           print(\n",
        "#               \"%d [D loss class: %.4f, acc: %.2f%%]\"\n",
        "#               % (iteration + 1, loss, 100 * acc))\n",
        "#           discriminator_supervised.save(\"./models/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "          \n",
        "#           # x, y = dataset.training_set()\n",
        "#           # _, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "#           # print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nIEI7nOhsUa",
        "colab_type": "code",
        "outputId": "6d9640f6-292e-4d11-910e-f93503d450a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations_pre = 1\n",
        "iter_epochs = 40    # 20\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "losses = []\n",
        "accs = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "dataset = Dataset_CIFAR10(num_labeled)\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "pretrain(iterations_pre, batch_size, save_interval\n",
        "         , iter_epochs\n",
        "         )\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Use time:\" + str(endtime-starttime) + \"s\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 106s 68ms/step - loss: 1.5490 - acc: 0.4975 - val_loss: 2.0545 - val_acc: 0.4434\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.44340, saving model to /content/models/cifar10_model.001.h5\n",
            "Epoch 2/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 1.1919 - acc: 0.6332 - val_loss: 1.6460 - val_acc: 0.5119\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.44340 to 0.51190, saving model to /content/models/cifar10_model.002.h5\n",
            "Epoch 3/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 1.0340 - acc: 0.6938 - val_loss: 1.1841 - val_acc: 0.6501\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.51190 to 0.65010, saving model to /content/models/cifar10_model.003.h5\n",
            "Epoch 4/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.9363 - acc: 0.7319 - val_loss: 1.1508 - val_acc: 0.6720\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.65010 to 0.67200, saving model to /content/models/cifar10_model.004.h5\n",
            "Epoch 5/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.8683 - acc: 0.7572 - val_loss: 1.0044 - val_acc: 0.7180\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.67200 to 0.71800, saving model to /content/models/cifar10_model.005.h5\n",
            "Epoch 6/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.8278 - acc: 0.7737 - val_loss: 1.0547 - val_acc: 0.7098\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.71800\n",
            "Epoch 7/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.7887 - acc: 0.7857 - val_loss: 1.2466 - val_acc: 0.6545\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.71800\n",
            "Epoch 8/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.7550 - acc: 0.8015 - val_loss: 0.9416 - val_acc: 0.7541\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.71800 to 0.75410, saving model to /content/models/cifar10_model.008.h5\n",
            "Epoch 9/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.7360 - acc: 0.8075 - val_loss: 0.9049 - val_acc: 0.7618\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.75410 to 0.76180, saving model to /content/models/cifar10_model.009.h5\n",
            "Epoch 10/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.7087 - acc: 0.8194 - val_loss: 0.8899 - val_acc: 0.7599\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.76180\n",
            "Epoch 11/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.6923 - acc: 0.8243 - val_loss: 0.8137 - val_acc: 0.7825\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.76180 to 0.78250, saving model to /content/models/cifar10_model.011.h5\n",
            "Epoch 12/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.6828 - acc: 0.8282 - val_loss: 0.9889 - val_acc: 0.7481\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.78250\n",
            "Epoch 13/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.6654 - acc: 0.8349 - val_loss: 0.8339 - val_acc: 0.7891\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.78250 to 0.78910, saving model to /content/models/cifar10_model.013.h5\n",
            "Epoch 14/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6571 - acc: 0.8377 - val_loss: 0.7599 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.78910 to 0.80750, saving model to /content/models/cifar10_model.014.h5\n",
            "Epoch 15/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.6443 - acc: 0.8432 - val_loss: 0.7632 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.80750 to 0.80990, saving model to /content/models/cifar10_model.015.h5\n",
            "Epoch 16/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 97s 62ms/step - loss: 0.6395 - acc: 0.8455 - val_loss: 0.7110 - val_acc: 0.8272\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.80990 to 0.82720, saving model to /content/models/cifar10_model.016.h5\n",
            "Epoch 17/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 96s 61ms/step - loss: 0.6225 - acc: 0.8514 - val_loss: 0.7128 - val_acc: 0.8251\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.82720\n",
            "Epoch 18/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 97s 62ms/step - loss: 0.6186 - acc: 0.8535 - val_loss: 0.9003 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.82720\n",
            "Epoch 19/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.6120 - acc: 0.8545 - val_loss: 0.6934 - val_acc: 0.8347\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.82720 to 0.83470, saving model to /content/models/cifar10_model.019.h5\n",
            "Epoch 20/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.6060 - acc: 0.8601 - val_loss: 1.0452 - val_acc: 0.7516\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.83470\n",
            "Epoch 21/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6014 - acc: 0.8590 - val_loss: 0.7993 - val_acc: 0.8048\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.83470\n",
            "Epoch 22/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.5971 - acc: 0.8629 - val_loss: 0.7566 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.83470\n",
            "Epoch 23/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.5905 - acc: 0.8643 - val_loss: 0.8287 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.83470\n",
            "Epoch 24/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.5885 - acc: 0.8667 - val_loss: 0.6778 - val_acc: 0.8391\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.83470 to 0.83910, saving model to /content/models/cifar10_model.024.h5\n",
            "Epoch 25/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5837 - acc: 0.8682 - val_loss: 0.8672 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.83910\n",
            "Epoch 26/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5792 - acc: 0.8674 - val_loss: 0.7639 - val_acc: 0.8161\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.83910\n",
            "Epoch 27/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5733 - acc: 0.8709 - val_loss: 0.6888 - val_acc: 0.8388\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.83910\n",
            "Epoch 28/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5715 - acc: 0.8713 - val_loss: 0.6886 - val_acc: 0.8404\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.83910 to 0.84040, saving model to /content/models/cifar10_model.028.h5\n",
            "Epoch 29/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5619 - acc: 0.8738 - val_loss: 0.7359 - val_acc: 0.8246\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.84040\n",
            "Epoch 30/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5658 - acc: 0.8741 - val_loss: 0.7969 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.84040\n",
            "Epoch 31/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5646 - acc: 0.8738 - val_loss: 0.7187 - val_acc: 0.8245\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.84040\n",
            "Epoch 32/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5617 - acc: 0.8762 - val_loss: 0.6895 - val_acc: 0.8361\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.84040\n",
            "Epoch 33/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5551 - acc: 0.8786 - val_loss: 1.0604 - val_acc: 0.7528\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.84040\n",
            "Epoch 34/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5518 - acc: 0.8779 - val_loss: 0.7479 - val_acc: 0.8202\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.84040\n",
            "Epoch 35/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 96s 61ms/step - loss: 0.5523 - acc: 0.8795 - val_loss: 0.7805 - val_acc: 0.8216\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.84040\n",
            "Epoch 36/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 95s 61ms/step - loss: 0.5519 - acc: 0.8794 - val_loss: 0.6805 - val_acc: 0.8427\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.84040 to 0.84270, saving model to /content/models/cifar10_model.036.h5\n",
            "Epoch 37/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 96s 62ms/step - loss: 0.5461 - acc: 0.8823 - val_loss: 0.6663 - val_acc: 0.8465\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.84270 to 0.84650, saving model to /content/models/cifar10_model.037.h5\n",
            "Epoch 38/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5404 - acc: 0.8842 - val_loss: 0.7457 - val_acc: 0.8317\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.84650\n",
            "Epoch 39/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5427 - acc: 0.8822 - val_loss: 0.6716 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.84650\n",
            "Epoch 40/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5411 - acc: 0.8834 - val_loss: 0.8284 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.84650\n",
            "1 [D loss class: 0.6735, acc: 81.25%]\n",
            "Use time:5734.692555s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lzo6IL5z9wE",
        "colab_type": "code",
        "outputId": "aab002d2-c93c-4afc-c643-3dcd32533097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "# tmodel.load_weights(\"./models/discriminator_supervised-2000.h5\", by_name=False)\n",
        "tmodel.load_weights(\"./models/cifar10_model.037.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 19s 383us/step\n",
            "Training Accuracy: 87.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etkdmP6Ez7Jy",
        "colab_type": "code",
        "outputId": "c5d920b5-fe5a-48b4-e69b-f1fd9db3b76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/cifar10_model.037.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 352us/step\n",
            "Test Accuracy: 84.65%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xEVpUqmWb90",
        "colab_type": "code",
        "outputId": "558cd0a0-b578-4942-d482-1353f1899da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "div = 1000\n",
        "ty = [history.accs[i*div] for i in range(0, len(history.accs)//div)]\n",
        "tx = [x for x in range(1*div, (len(ty)+1)*div, div)]\n",
        "print(max(ty))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, ty, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.96875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f120039be48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFiCAYAAACkvHqaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQYkD4f3t+wPK1RpZ0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOHm2K6nn-Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accs = []\n",
        "# tx = [x for x in range(100,2100,100)]\n",
        "# acc_max = [0,0]\n",
        "\n",
        "# x, y = dataset.test_set()\n",
        "\n",
        "# tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "# tmodel.compile(loss='categorical_crossentropy',\n",
        "#                          metrics=['accuracy'],\n",
        "#                          optimizer=Adam())\n",
        "\n",
        "# for e in tx:\n",
        "#   tmodel.load_weights(\"./models/discriminator_supervised-\"+ str(e) +\".h5\", by_name=False)\n",
        "#   _, acc = tmodel.evaluate(x, y)\n",
        "#   accs.append(acc)\n",
        "# print(max(accs))\n",
        "\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "# plt.xticks(tx, rotation=90)\n",
        "# plt.title(\"accs with epoch\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.ylabel(\"accs\")\n",
        "# plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSbSVx1khsOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations, batch_size, save_interval, iter_epochs, k):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    # Labels for real images: all ones\n",
        "    real = np.ones((batch_size, 1))\n",
        "\n",
        "    # Labels for fake images: all zeros\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        for _ in range(k):\n",
        "\n",
        "            # -------------------------\n",
        "            #  Train the Discriminator\n",
        "            # -------------------------\n",
        "\n",
        "            # Get labeled and unlabeled examples\n",
        "            imgs, labels = dataset.batch_labeled(batch_size)\n",
        "            imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "            # Generate a batch of fake images\n",
        "            z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "            fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "            fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "            gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "            discriminator_supervised.trainable = True\n",
        "            discriminator_unsupervised.trainable = True\n",
        "\n",
        "            # Train on real labeled examples\n",
        "            datagen.fit(imgs)\n",
        "            discriminator_supervised.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=iter_epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "            loss_d_supervised, acc_d_supervised = history.losses[-1], history.accs[-1]\n",
        "\n",
        "            # Train on real unlabeled examples\n",
        "            # Error\n",
        "            # datagen.fit(imgs_unlabeled)\n",
        "            # discriminator_unsupervised.fit_generator(datagen.flow(imgs_unlabeled, real, batch_size=batch_size),\n",
        "            #             validation_data=(x_test, np.ones((len(x_test), 1))),\n",
        "            #             epochs=iter_epochs, verbose=1, workers=4,\n",
        "            #             callbacks=callbacks)\n",
        "            # loss_d_unsupervised_real, acc_d_unsupervised_real = history.losses[-1], history.accs[-1]\n",
        "            loss_d_unsupervised_real, acc_d_unsupervised_real = discriminator_unsupervised.train_on_batch(imgs_unlabeled, real)\n",
        "\n",
        "            # Train on fake examples\n",
        "            loss_d_unsupervised_fake, acc_d_unsupervised_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "            # Calculate loss and acc\n",
        "            loss_d_unsupervised = 0.5 * np.add(loss_d_unsupervised_real, loss_d_unsupervised_fake)\n",
        "            loss_d = np.add(loss_d_supervised, loss_d_unsupervised)\n",
        "            acc_d_unsupervised = 0.5 * np.add(acc_d_unsupervised_real, acc_d_unsupervised_fake)\n",
        "            acc_d = np.add(acc_d_supervised, acc_d_unsupervised)\n",
        "        \n",
        "        # ---------------------\n",
        "        #  Train the Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "        fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "        gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "        discriminator_supervised.trainable = False\n",
        "        discriminator_unsupervised.trainable = False\n",
        "\n",
        "        # Train Generator\n",
        "        loss_g_unsupervised, acc_g_unsupervised = gan.train_on_batch([z,labels], real)\n",
        "\n",
        "        # Calculate loss and acc\n",
        "        loss_g = loss_g_unsupervised\n",
        "        acc_g = acc_g_unsupervised\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "            # Save losses to be plotted after training\n",
        "            losses_d_supervised.append(loss_d_supervised)\n",
        "            losses_d_unsupervised.append(loss_d_unsupervised)\n",
        "            losses_d_unsupervised_real.append(loss_d_unsupervised_real)\n",
        "            losses_d_unsupervised_fake.append(loss_d_unsupervised_fake)\n",
        "            losses_d.append(loss_d)\n",
        "            losses_g.append(loss_g)\n",
        "            \n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # Output training progress\n",
        "            print(\n",
        "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%]\"\n",
        "                % (iteration + 1, \n",
        "                   loss_d_supervised, 100 * acc_d_supervised,\n",
        "                   loss_d_unsupervised, 100 * acc_d_unsupervised, \n",
        "                   loss_g, 100 * acc_g))\n",
        "            \n",
        "            discriminator_supervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "            discriminator_unsupervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_unsupervised-\" + str(iteration+1) + \".h5\")\n",
        "            generator.save(\"./models/models-label-\" + str(num_labeled) + \"/generator-\" + str(iteration+1) + \".h5\")\n",
        "            file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_d_supervised.json\"\n",
        "            file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_d_unsupervised.json\"\n",
        "            file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_g.json\"\n",
        "            with open(file1, 'w') as json_file:\n",
        "                  json.dump(str(losses_d_supervised), json_file)\n",
        "            with open(file2, 'w') as json_file:\n",
        "                  json.dump(str(losses_d_unsupervised), json_file)\n",
        "            with open(file3, 'w') as json_file:\n",
        "                  json.dump(str(losses_g), json_file)\n",
        "\n",
        "            # x,y = dataset.training_set()\n",
        "            # _, acc = discriminator_supervised.evaluate(x,y)\n",
        "            # print(str(100*acc)+\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__5V6FJn6xB",
        "colab_type": "code",
        "outputId": "9b2d7bb8-ca4e-4554-eeb6-123cf0dc4bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 300 # 8000\n",
        "iter_epochs = 1 # 10\n",
        "batch_size = 32\n",
        "save_interval = 10\n",
        "k = 1 # iteration of Discriminator\n",
        "\n",
        "losses_d_supervised = []\n",
        "losses_d_unsupervised = []\n",
        "losses_d_unsupervised_real = []\n",
        "losses_d_unsupervised_fake = []\n",
        "losses_d = []\n",
        "losses_g = []\n",
        "\n",
        "iteration_checkpoints = []\n",
        "\n",
        "# discriminator_supervised = load_model(\"./models/discriminator_supervised-1200.h5\")\n",
        "discriminator_supervised = load_model(\"./useless_models/cifar10_model.019.h5\")\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the SCGAN-2D for the specified number of iterations\n",
        "train(iterations, batch_size, save_interval, iter_epochs, k)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 26s 26s/step - loss: 0.5122 - acc: 0.8438 - val_loss: 0.6732 - val_acc: 0.8324\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.83500\n",
            "1 [D loss supervised: 0.5122, acc.: 84.38%] [D loss unsupervised: 0.2705, acc.: 100.00%] [G loss: 2.962576, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4464 - acc: 0.8750 - val_loss: 0.6423 - val_acc: 0.8387\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.83500 to 0.83870, saving model to /content/models/cifar10_model.001.h5\n",
            "2 [D loss supervised: 0.4464, acc.: 87.50%] [D loss unsupervised: 0.2706, acc.: 98.44%] [G loss: 3.046097, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5002 - acc: 0.8750 - val_loss: 0.6390 - val_acc: 0.8393\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.83870 to 0.83930, saving model to /content/models/cifar10_model.001.h5\n",
            "3 [D loss supervised: 0.5002, acc.: 87.50%] [D loss unsupervised: 0.2517, acc.: 98.44%] [G loss: 3.417806, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7070 - acc: 0.7500 - val_loss: 0.6347 - val_acc: 0.8402\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.83930 to 0.84020, saving model to /content/models/cifar10_model.001.h5\n",
            "4 [D loss supervised: 0.7070, acc.: 75.00%] [D loss unsupervised: 0.2323, acc.: 100.00%] [G loss: 3.211754, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4532 - acc: 0.9375 - val_loss: 0.6202 - val_acc: 0.8451\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.84020 to 0.84510, saving model to /content/models/cifar10_model.001.h5\n",
            "5 [D loss supervised: 0.4532, acc.: 93.75%] [D loss unsupervised: 0.2693, acc.: 100.00%] [G loss: 3.331815, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6076 - acc: 0.8438 - val_loss: 0.6205 - val_acc: 0.8451\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84510\n",
            "6 [D loss supervised: 0.6076, acc.: 84.38%] [D loss unsupervised: 0.2580, acc.: 98.44%] [G loss: 3.034502, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5138 - acc: 0.8125 - val_loss: 0.6171 - val_acc: 0.8479\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.84510 to 0.84790, saving model to /content/models/cifar10_model.001.h5\n",
            "7 [D loss supervised: 0.5138, acc.: 81.25%] [D loss unsupervised: 0.3089, acc.: 98.44%] [G loss: 2.775570, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6660 - acc: 0.8125 - val_loss: 0.6215 - val_acc: 0.8467\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "8 [D loss supervised: 0.6660, acc.: 81.25%] [D loss unsupervised: 0.3049, acc.: 96.88%] [G loss: 1.878350, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5548 - acc: 0.8750 - val_loss: 0.6253 - val_acc: 0.8456\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "9 [D loss supervised: 0.5548, acc.: 87.50%] [D loss unsupervised: 0.3039, acc.: 98.44%] [G loss: 2.005913, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4002 - acc: 0.9375 - val_loss: 0.6323 - val_acc: 0.8425\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "10 [D loss supervised: 0.4002, acc.: 93.75%] [D loss unsupervised: 0.2748, acc.: 100.00%] [G loss: 2.399218, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6118 - acc: 0.7812 - val_loss: 0.6317 - val_acc: 0.8429\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "11 [D loss supervised: 0.6118, acc.: 78.12%] [D loss unsupervised: 0.2828, acc.: 96.88%] [G loss: 3.089212, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5628 - acc: 0.8750 - val_loss: 0.6319 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "12 [D loss supervised: 0.5628, acc.: 87.50%] [D loss unsupervised: 0.2197, acc.: 100.00%] [G loss: 3.238440, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4304 - acc: 0.9375 - val_loss: 0.6361 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "13 [D loss supervised: 0.4304, acc.: 93.75%] [D loss unsupervised: 0.2516, acc.: 96.88%] [G loss: 3.356405, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3316 - acc: 0.9688 - val_loss: 0.6417 - val_acc: 0.8413\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "14 [D loss supervised: 0.3316, acc.: 96.88%] [D loss unsupervised: 0.2455, acc.: 98.44%] [G loss: 2.931774, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6292 - acc: 0.7500 - val_loss: 0.6474 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "15 [D loss supervised: 0.6292, acc.: 75.00%] [D loss unsupervised: 0.2580, acc.: 100.00%] [G loss: 1.386154, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4959 - acc: 0.9375 - val_loss: 0.6552 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "16 [D loss supervised: 0.4959, acc.: 93.75%] [D loss unsupervised: 0.4160, acc.: 93.75%] [G loss: 1.233681, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3636 - acc: 0.9062 - val_loss: 0.6626 - val_acc: 0.8336\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "17 [D loss supervised: 0.3636, acc.: 90.62%] [D loss unsupervised: 0.2938, acc.: 96.88%] [G loss: 2.309983, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3913 - acc: 0.9375 - val_loss: 0.6678 - val_acc: 0.8328\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "18 [D loss supervised: 0.3913, acc.: 93.75%] [D loss unsupervised: 0.2665, acc.: 98.44%] [G loss: 2.740803, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4414 - acc: 0.9062 - val_loss: 0.6709 - val_acc: 0.8322\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "19 [D loss supervised: 0.4414, acc.: 90.62%] [D loss unsupervised: 0.3321, acc.: 95.31%] [G loss: 1.947205, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5215 - acc: 0.8750 - val_loss: 0.6667 - val_acc: 0.8327\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "20 [D loss supervised: 0.5215, acc.: 87.50%] [D loss unsupervised: 0.2952, acc.: 95.31%] [G loss: 1.575520, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5330 - acc: 0.9062 - val_loss: 0.6583 - val_acc: 0.8361\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "21 [D loss supervised: 0.5330, acc.: 90.62%] [D loss unsupervised: 0.2524, acc.: 100.00%] [G loss: 1.094013, acc.: 37.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7200 - acc: 0.7812 - val_loss: 0.6548 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "22 [D loss supervised: 0.7200, acc.: 78.12%] [D loss unsupervised: 0.2338, acc.: 98.44%] [G loss: 0.737953, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3979 - acc: 0.9375 - val_loss: 0.6526 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "23 [D loss supervised: 0.3979, acc.: 93.75%] [D loss unsupervised: 0.2173, acc.: 100.00%] [G loss: 0.700754, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5260 - acc: 0.9062 - val_loss: 0.6517 - val_acc: 0.8389\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "24 [D loss supervised: 0.5260, acc.: 90.62%] [D loss unsupervised: 0.2380, acc.: 100.00%] [G loss: 0.993288, acc.: 50.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4677 - acc: 0.8750 - val_loss: 0.6527 - val_acc: 0.8391\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "25 [D loss supervised: 0.4677, acc.: 87.50%] [D loss unsupervised: 0.2276, acc.: 100.00%] [G loss: 1.224947, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5666 - acc: 0.8750 - val_loss: 0.6617 - val_acc: 0.8369\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "26 [D loss supervised: 0.5666, acc.: 87.50%] [D loss unsupervised: 0.2178, acc.: 100.00%] [G loss: 1.818767, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3968 - acc: 0.9688 - val_loss: 0.6706 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "27 [D loss supervised: 0.3968, acc.: 96.88%] [D loss unsupervised: 0.2876, acc.: 95.31%] [G loss: 1.933135, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5457 - acc: 0.8438 - val_loss: 0.6741 - val_acc: 0.8341\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "28 [D loss supervised: 0.5457, acc.: 84.38%] [D loss unsupervised: 0.2676, acc.: 98.44%] [G loss: 2.052097, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4100 - acc: 0.9375 - val_loss: 0.6784 - val_acc: 0.8315\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "29 [D loss supervised: 0.4100, acc.: 93.75%] [D loss unsupervised: 0.2510, acc.: 100.00%] [G loss: 2.358076, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4510 - acc: 0.9062 - val_loss: 0.6855 - val_acc: 0.8294\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "30 [D loss supervised: 0.4510, acc.: 90.62%] [D loss unsupervised: 0.2356, acc.: 100.00%] [G loss: 3.304409, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4931 - acc: 0.8750 - val_loss: 0.6965 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "31 [D loss supervised: 0.4931, acc.: 87.50%] [D loss unsupervised: 0.2049, acc.: 100.00%] [G loss: 3.840486, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5848 - acc: 0.8438 - val_loss: 0.7059 - val_acc: 0.8239\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "32 [D loss supervised: 0.5848, acc.: 84.38%] [D loss unsupervised: 0.2334, acc.: 100.00%] [G loss: 4.014298, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4135 - acc: 0.9062 - val_loss: 0.7168 - val_acc: 0.8211\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "33 [D loss supervised: 0.4135, acc.: 90.62%] [D loss unsupervised: 0.2202, acc.: 100.00%] [G loss: 4.183128, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4350 - acc: 0.9375 - val_loss: 0.7289 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "34 [D loss supervised: 0.4350, acc.: 93.75%] [D loss unsupervised: 0.2255, acc.: 100.00%] [G loss: 4.154865, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4496 - acc: 0.9375 - val_loss: 0.7403 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "35 [D loss supervised: 0.4496, acc.: 93.75%] [D loss unsupervised: 0.2010, acc.: 100.00%] [G loss: 4.013154, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4361 - acc: 0.8438 - val_loss: 0.7514 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "36 [D loss supervised: 0.4361, acc.: 84.38%] [D loss unsupervised: 0.2208, acc.: 98.44%] [G loss: 3.790524, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3807 - acc: 0.9375 - val_loss: 0.7678 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "37 [D loss supervised: 0.3807, acc.: 93.75%] [D loss unsupervised: 0.2111, acc.: 100.00%] [G loss: 3.627892, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4077 - acc: 0.9062 - val_loss: 0.7798 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "38 [D loss supervised: 0.4077, acc.: 90.62%] [D loss unsupervised: 0.2190, acc.: 100.00%] [G loss: 3.119236, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3849 - acc: 0.9688 - val_loss: 0.7945 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "39 [D loss supervised: 0.3849, acc.: 96.88%] [D loss unsupervised: 0.2208, acc.: 100.00%] [G loss: 2.537077, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5484 - acc: 0.8438 - val_loss: 0.8068 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "40 [D loss supervised: 0.5484, acc.: 84.38%] [D loss unsupervised: 0.2361, acc.: 98.44%] [G loss: 2.335386, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3834 - acc: 0.9062 - val_loss: 0.8215 - val_acc: 0.7976\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "41 [D loss supervised: 0.3834, acc.: 90.62%] [D loss unsupervised: 0.2366, acc.: 98.44%] [G loss: 1.869304, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5456 - acc: 0.8750 - val_loss: 0.8370 - val_acc: 0.7949\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "42 [D loss supervised: 0.5456, acc.: 87.50%] [D loss unsupervised: 0.2331, acc.: 100.00%] [G loss: 2.082068, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4766 - acc: 0.9062 - val_loss: 0.8415 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "43 [D loss supervised: 0.4766, acc.: 90.62%] [D loss unsupervised: 0.2291, acc.: 100.00%] [G loss: 2.222253, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6167 - acc: 0.8750 - val_loss: 0.8464 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "44 [D loss supervised: 0.6167, acc.: 87.50%] [D loss unsupervised: 0.3149, acc.: 93.75%] [G loss: 1.960812, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7187 - acc: 0.7812 - val_loss: 0.8487 - val_acc: 0.7934\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "45 [D loss supervised: 0.7187, acc.: 78.12%] [D loss unsupervised: 0.2366, acc.: 100.00%] [G loss: 1.730756, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4296 - acc: 0.9062 - val_loss: 0.8523 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "46 [D loss supervised: 0.4296, acc.: 90.62%] [D loss unsupervised: 0.2170, acc.: 98.44%] [G loss: 1.257480, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4436 - acc: 0.8750 - val_loss: 0.8431 - val_acc: 0.7942\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "47 [D loss supervised: 0.4436, acc.: 87.50%] [D loss unsupervised: 0.2218, acc.: 100.00%] [G loss: 1.126503, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4407 - acc: 0.9375 - val_loss: 0.8257 - val_acc: 0.7977\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "48 [D loss supervised: 0.4407, acc.: 93.75%] [D loss unsupervised: 0.2358, acc.: 100.00%] [G loss: 1.308747, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4082 - acc: 0.9062 - val_loss: 0.8090 - val_acc: 0.8010\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "49 [D loss supervised: 0.4082, acc.: 90.62%] [D loss unsupervised: 0.2191, acc.: 100.00%] [G loss: 3.206339, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2628 - acc: 0.9688 - val_loss: 0.7923 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "50 [D loss supervised: 0.2628, acc.: 96.88%] [D loss unsupervised: 0.2134, acc.: 100.00%] [G loss: 3.530090, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3080 - acc: 0.9688 - val_loss: 0.7829 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "51 [D loss supervised: 0.3080, acc.: 96.88%] [D loss unsupervised: 0.2027, acc.: 100.00%] [G loss: 4.026888, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4940 - acc: 0.8438 - val_loss: 0.7738 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "52 [D loss supervised: 0.4940, acc.: 84.38%] [D loss unsupervised: 0.2079, acc.: 100.00%] [G loss: 4.259899, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3674 - acc: 0.9375 - val_loss: 0.7617 - val_acc: 0.8128\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "53 [D loss supervised: 0.3674, acc.: 93.75%] [D loss unsupervised: 0.2079, acc.: 100.00%] [G loss: 4.148007, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4146 - acc: 0.9375 - val_loss: 0.7569 - val_acc: 0.8138\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "54 [D loss supervised: 0.4146, acc.: 93.75%] [D loss unsupervised: 0.2266, acc.: 98.44%] [G loss: 3.649605, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4981 - acc: 0.8750 - val_loss: 0.7494 - val_acc: 0.8150\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "55 [D loss supervised: 0.4981, acc.: 87.50%] [D loss unsupervised: 0.1978, acc.: 100.00%] [G loss: 3.663976, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2953 - acc: 0.9688 - val_loss: 0.7452 - val_acc: 0.8156\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "56 [D loss supervised: 0.2953, acc.: 96.88%] [D loss unsupervised: 0.1956, acc.: 100.00%] [G loss: 3.113882, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4770 - acc: 0.9062 - val_loss: 0.7348 - val_acc: 0.8182\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "57 [D loss supervised: 0.4770, acc.: 90.62%] [D loss unsupervised: 0.2010, acc.: 100.00%] [G loss: 3.053675, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3356 - acc: 0.9375 - val_loss: 0.7273 - val_acc: 0.8169\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "58 [D loss supervised: 0.3356, acc.: 93.75%] [D loss unsupervised: 0.2060, acc.: 100.00%] [G loss: 2.598295, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5651 - acc: 0.8750 - val_loss: 0.7293 - val_acc: 0.8169\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "59 [D loss supervised: 0.5651, acc.: 87.50%] [D loss unsupervised: 0.2022, acc.: 100.00%] [G loss: 2.292420, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5768 - acc: 0.8750 - val_loss: 0.7420 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "60 [D loss supervised: 0.5768, acc.: 87.50%] [D loss unsupervised: 0.1910, acc.: 100.00%] [G loss: 1.772282, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5079 - acc: 0.8750 - val_loss: 0.7638 - val_acc: 0.8053\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "61 [D loss supervised: 0.5079, acc.: 87.50%] [D loss unsupervised: 0.2056, acc.: 100.00%] [G loss: 1.372975, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4098 - acc: 0.8750 - val_loss: 0.7803 - val_acc: 0.8005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "62 [D loss supervised: 0.4098, acc.: 87.50%] [D loss unsupervised: 0.2042, acc.: 100.00%] [G loss: 1.527498, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4891 - acc: 0.9375 - val_loss: 0.7902 - val_acc: 0.7984\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "63 [D loss supervised: 0.4891, acc.: 93.75%] [D loss unsupervised: 0.2028, acc.: 100.00%] [G loss: 1.277170, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4629 - acc: 0.8750 - val_loss: 0.8015 - val_acc: 0.7955\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "64 [D loss supervised: 0.4629, acc.: 87.50%] [D loss unsupervised: 0.2065, acc.: 100.00%] [G loss: 1.300247, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4596 - acc: 0.9375 - val_loss: 0.8148 - val_acc: 0.7923\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "65 [D loss supervised: 0.4596, acc.: 93.75%] [D loss unsupervised: 0.1953, acc.: 100.00%] [G loss: 1.701754, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3600 - acc: 0.9688 - val_loss: 0.8297 - val_acc: 0.7900\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "66 [D loss supervised: 0.3600, acc.: 96.88%] [D loss unsupervised: 0.1928, acc.: 100.00%] [G loss: 1.430336, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4628 - acc: 0.8750 - val_loss: 0.8371 - val_acc: 0.7871\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "67 [D loss supervised: 0.4628, acc.: 87.50%] [D loss unsupervised: 0.1969, acc.: 100.00%] [G loss: 0.911204, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4118 - acc: 0.9375 - val_loss: 0.8396 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "68 [D loss supervised: 0.4118, acc.: 93.75%] [D loss unsupervised: 0.1995, acc.: 100.00%] [G loss: 1.199234, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4057 - acc: 0.9375 - val_loss: 0.8335 - val_acc: 0.7875\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "69 [D loss supervised: 0.4057, acc.: 93.75%] [D loss unsupervised: 0.1990, acc.: 100.00%] [G loss: 1.321495, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5095 - acc: 0.8125 - val_loss: 0.8278 - val_acc: 0.7901\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "70 [D loss supervised: 0.5095, acc.: 81.25%] [D loss unsupervised: 0.1915, acc.: 100.00%] [G loss: 1.113436, acc.: 37.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4858 - acc: 0.8750 - val_loss: 0.8170 - val_acc: 0.7923\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "71 [D loss supervised: 0.4858, acc.: 87.50%] [D loss unsupervised: 0.2019, acc.: 100.00%] [G loss: 1.004999, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4266 - acc: 0.9375 - val_loss: 0.8091 - val_acc: 0.7935\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "72 [D loss supervised: 0.4266, acc.: 93.75%] [D loss unsupervised: 0.1931, acc.: 100.00%] [G loss: 1.321540, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3559 - acc: 0.9375 - val_loss: 0.7999 - val_acc: 0.7966\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "73 [D loss supervised: 0.3559, acc.: 93.75%] [D loss unsupervised: 0.1989, acc.: 100.00%] [G loss: 1.777720, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3493 - acc: 0.9375 - val_loss: 0.7865 - val_acc: 0.7993\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "74 [D loss supervised: 0.3493, acc.: 93.75%] [D loss unsupervised: 0.2092, acc.: 100.00%] [G loss: 2.034553, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3438 - acc: 0.9688 - val_loss: 0.7743 - val_acc: 0.8034\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "75 [D loss supervised: 0.3438, acc.: 96.88%] [D loss unsupervised: 0.1892, acc.: 100.00%] [G loss: 2.077254, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4798 - acc: 0.9062 - val_loss: 0.7574 - val_acc: 0.8089\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "76 [D loss supervised: 0.4798, acc.: 90.62%] [D loss unsupervised: 0.1963, acc.: 100.00%] [G loss: 1.618526, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3447 - acc: 0.9375 - val_loss: 0.7417 - val_acc: 0.8132\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "77 [D loss supervised: 0.3447, acc.: 93.75%] [D loss unsupervised: 0.2093, acc.: 100.00%] [G loss: 1.296912, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4418 - acc: 0.8438 - val_loss: 0.7293 - val_acc: 0.8160\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "78 [D loss supervised: 0.4418, acc.: 84.38%] [D loss unsupervised: 0.1990, acc.: 100.00%] [G loss: 2.120513, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4379 - acc: 0.8750 - val_loss: 0.7143 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "79 [D loss supervised: 0.4379, acc.: 87.50%] [D loss unsupervised: 0.1997, acc.: 100.00%] [G loss: 2.842155, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5139 - acc: 0.8750 - val_loss: 0.6982 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "80 [D loss supervised: 0.5139, acc.: 87.50%] [D loss unsupervised: 0.2082, acc.: 100.00%] [G loss: 2.563019, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4841 - acc: 0.8438 - val_loss: 0.6898 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "81 [D loss supervised: 0.4841, acc.: 84.38%] [D loss unsupervised: 0.1959, acc.: 100.00%] [G loss: 1.446290, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3832 - acc: 0.9062 - val_loss: 0.6883 - val_acc: 0.8277\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "82 [D loss supervised: 0.3832, acc.: 90.62%] [D loss unsupervised: 0.2074, acc.: 100.00%] [G loss: 1.253576, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4498 - acc: 0.9062 - val_loss: 0.6935 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "83 [D loss supervised: 0.4498, acc.: 90.62%] [D loss unsupervised: 0.2060, acc.: 100.00%] [G loss: 0.617578, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3882 - acc: 0.9375 - val_loss: 0.7042 - val_acc: 0.8238\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "84 [D loss supervised: 0.3882, acc.: 93.75%] [D loss unsupervised: 0.2130, acc.: 100.00%] [G loss: 1.248772, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3460 - acc: 0.9062 - val_loss: 0.7145 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "85 [D loss supervised: 0.3460, acc.: 90.62%] [D loss unsupervised: 0.2067, acc.: 100.00%] [G loss: 1.118393, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3996 - acc: 0.9375 - val_loss: 0.7281 - val_acc: 0.8174\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "86 [D loss supervised: 0.3996, acc.: 93.75%] [D loss unsupervised: 0.1930, acc.: 100.00%] [G loss: 2.045895, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3217 - acc: 0.9375 - val_loss: 0.7463 - val_acc: 0.8133\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "87 [D loss supervised: 0.3217, acc.: 93.75%] [D loss unsupervised: 0.1935, acc.: 100.00%] [G loss: 2.580127, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6033 - acc: 0.8125 - val_loss: 0.7700 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "88 [D loss supervised: 0.6033, acc.: 81.25%] [D loss unsupervised: 0.2015, acc.: 100.00%] [G loss: 2.435107, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3110 - acc: 1.0000 - val_loss: 0.7907 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "89 [D loss supervised: 0.3110, acc.: 100.00%] [D loss unsupervised: 0.1968, acc.: 100.00%] [G loss: 2.125349, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3655 - acc: 0.9688 - val_loss: 0.8085 - val_acc: 0.7971\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "90 [D loss supervised: 0.3655, acc.: 96.88%] [D loss unsupervised: 0.1883, acc.: 100.00%] [G loss: 1.638695, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3518 - acc: 0.9062 - val_loss: 0.8209 - val_acc: 0.7934\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "91 [D loss supervised: 0.3518, acc.: 90.62%] [D loss unsupervised: 0.2008, acc.: 100.00%] [G loss: 1.245601, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2916 - acc: 0.9688 - val_loss: 0.8236 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "92 [D loss supervised: 0.2916, acc.: 96.88%] [D loss unsupervised: 0.1902, acc.: 100.00%] [G loss: 0.949131, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3611 - acc: 0.9375 - val_loss: 0.8338 - val_acc: 0.7913\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "93 [D loss supervised: 0.3611, acc.: 93.75%] [D loss unsupervised: 0.1997, acc.: 100.00%] [G loss: 0.892627, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4110 - acc: 0.9375 - val_loss: 0.8211 - val_acc: 0.7938\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "94 [D loss supervised: 0.4110, acc.: 93.75%] [D loss unsupervised: 0.2012, acc.: 100.00%] [G loss: 0.978767, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4025 - acc: 0.8750 - val_loss: 0.8037 - val_acc: 0.7977\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "95 [D loss supervised: 0.4025, acc.: 87.50%] [D loss unsupervised: 0.1894, acc.: 100.00%] [G loss: 1.452087, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4280 - acc: 0.9375 - val_loss: 0.7863 - val_acc: 0.8013\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "96 [D loss supervised: 0.4280, acc.: 93.75%] [D loss unsupervised: 0.1852, acc.: 100.00%] [G loss: 2.114085, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4329 - acc: 0.8750 - val_loss: 0.7649 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "97 [D loss supervised: 0.4329, acc.: 87.50%] [D loss unsupervised: 0.1876, acc.: 100.00%] [G loss: 1.602327, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4584 - acc: 0.9062 - val_loss: 0.7371 - val_acc: 0.8142\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "98 [D loss supervised: 0.4584, acc.: 90.62%] [D loss unsupervised: 0.1926, acc.: 100.00%] [G loss: 1.832752, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3334 - acc: 0.9688 - val_loss: 0.7136 - val_acc: 0.8199\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "99 [D loss supervised: 0.3334, acc.: 96.88%] [D loss unsupervised: 0.1851, acc.: 100.00%] [G loss: 1.279231, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3751 - acc: 0.9375 - val_loss: 0.6983 - val_acc: 0.8221\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "100 [D loss supervised: 0.3751, acc.: 93.75%] [D loss unsupervised: 0.1855, acc.: 100.00%] [G loss: 1.079559, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3439 - acc: 0.9062 - val_loss: 0.6925 - val_acc: 0.8245\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "101 [D loss supervised: 0.3439, acc.: 90.62%] [D loss unsupervised: 0.1874, acc.: 100.00%] [G loss: 0.926581, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4186 - acc: 0.8750 - val_loss: 0.6928 - val_acc: 0.8254\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "102 [D loss supervised: 0.4186, acc.: 87.50%] [D loss unsupervised: 0.1933, acc.: 100.00%] [G loss: 1.134119, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3908 - acc: 0.9375 - val_loss: 0.6949 - val_acc: 0.8239\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "103 [D loss supervised: 0.3908, acc.: 93.75%] [D loss unsupervised: 0.1843, acc.: 100.00%] [G loss: 0.426428, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5679 - acc: 0.7812 - val_loss: 0.7004 - val_acc: 0.8207\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "104 [D loss supervised: 0.5679, acc.: 78.12%] [D loss unsupervised: 0.1888, acc.: 100.00%] [G loss: 0.717306, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3848 - acc: 0.9062 - val_loss: 0.7091 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "105 [D loss supervised: 0.3848, acc.: 90.62%] [D loss unsupervised: 0.1911, acc.: 100.00%] [G loss: 0.621005, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4374 - acc: 0.9062 - val_loss: 0.7145 - val_acc: 0.8190\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "106 [D loss supervised: 0.4374, acc.: 90.62%] [D loss unsupervised: 0.1874, acc.: 100.00%] [G loss: 0.671032, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4247 - acc: 0.8750 - val_loss: 0.7216 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "107 [D loss supervised: 0.4247, acc.: 87.50%] [D loss unsupervised: 0.1841, acc.: 100.00%] [G loss: 0.678727, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3573 - acc: 0.9688 - val_loss: 0.7317 - val_acc: 0.8155\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "108 [D loss supervised: 0.3573, acc.: 96.88%] [D loss unsupervised: 0.1847, acc.: 100.00%] [G loss: 0.860268, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4592 - acc: 0.9062 - val_loss: 0.7412 - val_acc: 0.8134\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "109 [D loss supervised: 0.4592, acc.: 90.62%] [D loss unsupervised: 0.1845, acc.: 100.00%] [G loss: 0.464023, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2512 - acc: 1.0000 - val_loss: 0.7509 - val_acc: 0.8107\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "110 [D loss supervised: 0.2512, acc.: 100.00%] [D loss unsupervised: 0.1836, acc.: 100.00%] [G loss: 0.560597, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3066 - acc: 0.9375 - val_loss: 0.7575 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "111 [D loss supervised: 0.3066, acc.: 93.75%] [D loss unsupervised: 0.1828, acc.: 100.00%] [G loss: 0.881793, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2409 - acc: 1.0000 - val_loss: 0.7629 - val_acc: 0.8066\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "112 [D loss supervised: 0.2409, acc.: 100.00%] [D loss unsupervised: 0.1930, acc.: 100.00%] [G loss: 0.569604, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3526 - acc: 0.9375 - val_loss: 0.7700 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "113 [D loss supervised: 0.3526, acc.: 93.75%] [D loss unsupervised: 0.1862, acc.: 100.00%] [G loss: 0.734249, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3591 - acc: 0.9688 - val_loss: 0.7726 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "114 [D loss supervised: 0.3591, acc.: 96.88%] [D loss unsupervised: 0.1834, acc.: 100.00%] [G loss: 0.937094, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3321 - acc: 0.9375 - val_loss: 0.7765 - val_acc: 0.8042\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "115 [D loss supervised: 0.3321, acc.: 93.75%] [D loss unsupervised: 0.1898, acc.: 100.00%] [G loss: 1.044278, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4243 - acc: 0.9062 - val_loss: 0.7771 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "116 [D loss supervised: 0.4243, acc.: 90.62%] [D loss unsupervised: 0.1944, acc.: 100.00%] [G loss: 0.885914, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6856 - acc: 0.7812 - val_loss: 0.7761 - val_acc: 0.8023\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "117 [D loss supervised: 0.6856, acc.: 78.12%] [D loss unsupervised: 0.1925, acc.: 100.00%] [G loss: 1.058509, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2936 - acc: 0.9688 - val_loss: 0.7800 - val_acc: 0.8011\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "118 [D loss supervised: 0.2936, acc.: 96.88%] [D loss unsupervised: 0.1894, acc.: 100.00%] [G loss: 0.887597, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2516 - acc: 1.0000 - val_loss: 0.7809 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "119 [D loss supervised: 0.2516, acc.: 100.00%] [D loss unsupervised: 0.1794, acc.: 100.00%] [G loss: 0.963417, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3022 - acc: 0.9375 - val_loss: 0.7796 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "120 [D loss supervised: 0.3022, acc.: 93.75%] [D loss unsupervised: 0.1950, acc.: 100.00%] [G loss: 0.606867, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3370 - acc: 0.9062 - val_loss: 0.7925 - val_acc: 0.7968\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "121 [D loss supervised: 0.3370, acc.: 90.62%] [D loss unsupervised: 0.1840, acc.: 100.00%] [G loss: 0.532771, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2590 - acc: 1.0000 - val_loss: 0.8044 - val_acc: 0.7947\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "122 [D loss supervised: 0.2590, acc.: 100.00%] [D loss unsupervised: 0.1855, acc.: 100.00%] [G loss: 0.434958, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3845 - acc: 0.9375 - val_loss: 0.8137 - val_acc: 0.7914\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "123 [D loss supervised: 0.3845, acc.: 93.75%] [D loss unsupervised: 0.1796, acc.: 100.00%] [G loss: 0.407870, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3407 - acc: 0.8750 - val_loss: 0.8158 - val_acc: 0.7920\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "124 [D loss supervised: 0.3407, acc.: 87.50%] [D loss unsupervised: 0.1830, acc.: 100.00%] [G loss: 0.616656, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2519 - acc: 1.0000 - val_loss: 0.8135 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "125 [D loss supervised: 0.2519, acc.: 100.00%] [D loss unsupervised: 0.1945, acc.: 100.00%] [G loss: 0.542559, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2857 - acc: 0.9688 - val_loss: 0.7979 - val_acc: 0.7970\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "126 [D loss supervised: 0.2857, acc.: 96.88%] [D loss unsupervised: 0.1907, acc.: 100.00%] [G loss: 0.544576, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3714 - acc: 0.9062 - val_loss: 0.7817 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "127 [D loss supervised: 0.3714, acc.: 90.62%] [D loss unsupervised: 0.1796, acc.: 100.00%] [G loss: 0.745494, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4475 - acc: 0.9062 - val_loss: 0.7659 - val_acc: 0.8049\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "128 [D loss supervised: 0.4475, acc.: 90.62%] [D loss unsupervised: 0.1801, acc.: 100.00%] [G loss: 0.618251, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3284 - acc: 0.9375 - val_loss: 0.7549 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "129 [D loss supervised: 0.3284, acc.: 93.75%] [D loss unsupervised: 0.1808, acc.: 100.00%] [G loss: 0.583209, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3768 - acc: 0.9062 - val_loss: 0.7355 - val_acc: 0.8137\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "130 [D loss supervised: 0.3768, acc.: 90.62%] [D loss unsupervised: 0.1839, acc.: 100.00%] [G loss: 0.859364, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2878 - acc: 1.0000 - val_loss: 0.7221 - val_acc: 0.8182\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "131 [D loss supervised: 0.2878, acc.: 100.00%] [D loss unsupervised: 0.1812, acc.: 100.00%] [G loss: 1.138456, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3306 - acc: 0.9375 - val_loss: 0.7107 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "132 [D loss supervised: 0.3306, acc.: 93.75%] [D loss unsupervised: 0.1782, acc.: 100.00%] [G loss: 1.107031, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3388 - acc: 0.9062 - val_loss: 0.7033 - val_acc: 0.8248\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "133 [D loss supervised: 0.3388, acc.: 90.62%] [D loss unsupervised: 0.1779, acc.: 100.00%] [G loss: 1.317492, acc.: 37.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2757 - acc: 0.9688 - val_loss: 0.6975 - val_acc: 0.8261\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "134 [D loss supervised: 0.2757, acc.: 96.88%] [D loss unsupervised: 0.1841, acc.: 100.00%] [G loss: 1.063709, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3443 - acc: 0.9688 - val_loss: 0.6945 - val_acc: 0.8280\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "135 [D loss supervised: 0.3443, acc.: 96.88%] [D loss unsupervised: 0.1831, acc.: 100.00%] [G loss: 0.913772, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2581 - acc: 1.0000 - val_loss: 0.6929 - val_acc: 0.8305\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "136 [D loss supervised: 0.2581, acc.: 100.00%] [D loss unsupervised: 0.1786, acc.: 100.00%] [G loss: 0.758999, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4068 - acc: 0.9688 - val_loss: 0.6924 - val_acc: 0.8304\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "137 [D loss supervised: 0.4068, acc.: 96.88%] [D loss unsupervised: 0.1798, acc.: 100.00%] [G loss: 0.536303, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2534 - acc: 0.9688 - val_loss: 0.6876 - val_acc: 0.8321\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "138 [D loss supervised: 0.2534, acc.: 96.88%] [D loss unsupervised: 0.1814, acc.: 100.00%] [G loss: 0.634963, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2802 - acc: 0.9688 - val_loss: 0.6809 - val_acc: 0.8346\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "139 [D loss supervised: 0.2802, acc.: 96.88%] [D loss unsupervised: 0.2203, acc.: 98.44%] [G loss: 0.642462, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4068 - acc: 0.8750 - val_loss: 0.6774 - val_acc: 0.8334\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "140 [D loss supervised: 0.4068, acc.: 87.50%] [D loss unsupervised: 0.1810, acc.: 100.00%] [G loss: 0.706732, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3570 - acc: 0.9062 - val_loss: 0.6768 - val_acc: 0.8328\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "141 [D loss supervised: 0.3570, acc.: 90.62%] [D loss unsupervised: 0.1814, acc.: 100.00%] [G loss: 0.693946, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2643 - acc: 0.9688 - val_loss: 0.6769 - val_acc: 0.8337\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "142 [D loss supervised: 0.2643, acc.: 96.88%] [D loss unsupervised: 0.1792, acc.: 100.00%] [G loss: 0.938449, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3263 - acc: 0.9375 - val_loss: 0.6737 - val_acc: 0.8363\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "143 [D loss supervised: 0.3263, acc.: 93.75%] [D loss unsupervised: 0.1803, acc.: 100.00%] [G loss: 0.924266, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2918 - acc: 0.9688 - val_loss: 0.6741 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "144 [D loss supervised: 0.2918, acc.: 96.88%] [D loss unsupervised: 0.1955, acc.: 100.00%] [G loss: 1.419905, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2819 - acc: 0.9688 - val_loss: 0.6768 - val_acc: 0.8371\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "145 [D loss supervised: 0.2819, acc.: 96.88%] [D loss unsupervised: 0.1931, acc.: 100.00%] [G loss: 0.860411, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4035 - acc: 0.9062 - val_loss: 0.6795 - val_acc: 0.8345\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "146 [D loss supervised: 0.4035, acc.: 90.62%] [D loss unsupervised: 0.1814, acc.: 100.00%] [G loss: 0.860156, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3377 - acc: 0.9062 - val_loss: 0.6847 - val_acc: 0.8319\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "147 [D loss supervised: 0.3377, acc.: 90.62%] [D loss unsupervised: 0.1893, acc.: 100.00%] [G loss: 0.667705, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4621 - acc: 0.9062 - val_loss: 0.6867 - val_acc: 0.8308\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "148 [D loss supervised: 0.4621, acc.: 90.62%] [D loss unsupervised: 0.1815, acc.: 100.00%] [G loss: 1.137516, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3378 - acc: 0.9375 - val_loss: 0.6947 - val_acc: 0.8255\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "149 [D loss supervised: 0.3378, acc.: 93.75%] [D loss unsupervised: 0.1788, acc.: 100.00%] [G loss: 1.663708, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3715 - acc: 0.9688 - val_loss: 0.7033 - val_acc: 0.8219\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "150 [D loss supervised: 0.3715, acc.: 96.88%] [D loss unsupervised: 0.1808, acc.: 100.00%] [G loss: 1.175135, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3307 - acc: 0.9375 - val_loss: 0.7073 - val_acc: 0.8205\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "151 [D loss supervised: 0.3307, acc.: 93.75%] [D loss unsupervised: 0.1832, acc.: 100.00%] [G loss: 1.522560, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2983 - acc: 0.9688 - val_loss: 0.7147 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "152 [D loss supervised: 0.2983, acc.: 96.88%] [D loss unsupervised: 0.1888, acc.: 100.00%] [G loss: 1.281752, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3427 - acc: 0.9375 - val_loss: 0.7210 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "153 [D loss supervised: 0.3427, acc.: 93.75%] [D loss unsupervised: 0.1832, acc.: 100.00%] [G loss: 1.001329, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2459 - acc: 1.0000 - val_loss: 0.7269 - val_acc: 0.8153\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "154 [D loss supervised: 0.2459, acc.: 100.00%] [D loss unsupervised: 0.1802, acc.: 100.00%] [G loss: 0.734672, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2503 - acc: 0.9688 - val_loss: 0.7370 - val_acc: 0.8131\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "155 [D loss supervised: 0.2503, acc.: 96.88%] [D loss unsupervised: 0.1850, acc.: 100.00%] [G loss: 0.547435, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3993 - acc: 0.8750 - val_loss: 0.7546 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "156 [D loss supervised: 0.3993, acc.: 87.50%] [D loss unsupervised: 0.1844, acc.: 100.00%] [G loss: 0.656891, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3247 - acc: 0.9375 - val_loss: 0.7595 - val_acc: 0.8080\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "157 [D loss supervised: 0.3247, acc.: 93.75%] [D loss unsupervised: 0.1816, acc.: 100.00%] [G loss: 0.527138, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2823 - acc: 1.0000 - val_loss: 0.7678 - val_acc: 0.8072\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "158 [D loss supervised: 0.2823, acc.: 100.00%] [D loss unsupervised: 0.1774, acc.: 100.00%] [G loss: 0.670057, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2967 - acc: 0.9688 - val_loss: 0.7726 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "159 [D loss supervised: 0.2967, acc.: 96.88%] [D loss unsupervised: 0.1828, acc.: 100.00%] [G loss: 0.809147, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3316 - acc: 0.9375 - val_loss: 0.7715 - val_acc: 0.8044\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "160 [D loss supervised: 0.3316, acc.: 93.75%] [D loss unsupervised: 0.1780, acc.: 100.00%] [G loss: 0.536883, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3149 - acc: 0.9375 - val_loss: 0.7657 - val_acc: 0.8045\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "161 [D loss supervised: 0.3149, acc.: 93.75%] [D loss unsupervised: 0.1775, acc.: 100.00%] [G loss: 0.521199, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2293 - acc: 1.0000 - val_loss: 0.7602 - val_acc: 0.8058\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "162 [D loss supervised: 0.2293, acc.: 100.00%] [D loss unsupervised: 0.1785, acc.: 100.00%] [G loss: 0.379391, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2448 - acc: 0.9688 - val_loss: 0.7538 - val_acc: 0.8079\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "163 [D loss supervised: 0.2448, acc.: 96.88%] [D loss unsupervised: 0.1864, acc.: 100.00%] [G loss: 0.580788, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3250 - acc: 0.9375 - val_loss: 0.7464 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "164 [D loss supervised: 0.3250, acc.: 93.75%] [D loss unsupervised: 0.1756, acc.: 100.00%] [G loss: 0.827010, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4454 - acc: 0.9375 - val_loss: 0.7441 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "165 [D loss supervised: 0.4454, acc.: 93.75%] [D loss unsupervised: 0.1844, acc.: 100.00%] [G loss: 0.673782, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2827 - acc: 0.9375 - val_loss: 0.7471 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "166 [D loss supervised: 0.2827, acc.: 93.75%] [D loss unsupervised: 0.1778, acc.: 100.00%] [G loss: 0.624333, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3764 - acc: 0.9062 - val_loss: 0.7516 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "167 [D loss supervised: 0.3764, acc.: 90.62%] [D loss unsupervised: 0.1760, acc.: 100.00%] [G loss: 0.544892, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3176 - acc: 0.9062 - val_loss: 0.7591 - val_acc: 0.8106\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "168 [D loss supervised: 0.3176, acc.: 90.62%] [D loss unsupervised: 0.1788, acc.: 100.00%] [G loss: 0.626657, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3634 - acc: 0.9062 - val_loss: 0.7623 - val_acc: 0.8077\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "169 [D loss supervised: 0.3634, acc.: 90.62%] [D loss unsupervised: 0.1756, acc.: 100.00%] [G loss: 1.718179, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3096 - acc: 0.9688 - val_loss: 0.7615 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "170 [D loss supervised: 0.3096, acc.: 96.88%] [D loss unsupervised: 0.1749, acc.: 100.00%] [G loss: 1.168220, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2700 - acc: 0.9688 - val_loss: 0.7631 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "171 [D loss supervised: 0.2700, acc.: 96.88%] [D loss unsupervised: 0.1766, acc.: 100.00%] [G loss: 1.105413, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4469 - acc: 0.9375 - val_loss: 0.7537 - val_acc: 0.8123\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "172 [D loss supervised: 0.4469, acc.: 93.75%] [D loss unsupervised: 0.1763, acc.: 100.00%] [G loss: 0.839928, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2858 - acc: 0.9375 - val_loss: 0.7440 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "173 [D loss supervised: 0.2858, acc.: 93.75%] [D loss unsupervised: 0.1783, acc.: 100.00%] [G loss: 0.913359, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3602 - acc: 0.9688 - val_loss: 0.7395 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "174 [D loss supervised: 0.3602, acc.: 96.88%] [D loss unsupervised: 0.1847, acc.: 100.00%] [G loss: 0.551977, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4603 - acc: 0.9062 - val_loss: 0.7375 - val_acc: 0.8156\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "175 [D loss supervised: 0.4603, acc.: 90.62%] [D loss unsupervised: 0.1760, acc.: 100.00%] [G loss: 0.919580, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3124 - acc: 0.9688 - val_loss: 0.7348 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "176 [D loss supervised: 0.3124, acc.: 96.88%] [D loss unsupervised: 0.1721, acc.: 100.00%] [G loss: 1.525538, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4314 - acc: 0.9062 - val_loss: 0.7336 - val_acc: 0.8160\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "177 [D loss supervised: 0.4314, acc.: 90.62%] [D loss unsupervised: 0.1719, acc.: 100.00%] [G loss: 2.041720, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2600 - acc: 1.0000 - val_loss: 0.7321 - val_acc: 0.8176\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "178 [D loss supervised: 0.2600, acc.: 100.00%] [D loss unsupervised: 0.1734, acc.: 100.00%] [G loss: 2.016696, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2710 - acc: 0.9688 - val_loss: 0.7311 - val_acc: 0.8182\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "179 [D loss supervised: 0.2710, acc.: 96.88%] [D loss unsupervised: 0.1792, acc.: 100.00%] [G loss: 1.553405, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4365 - acc: 0.8750 - val_loss: 0.7350 - val_acc: 0.8173\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "180 [D loss supervised: 0.4365, acc.: 87.50%] [D loss unsupervised: 0.1716, acc.: 100.00%] [G loss: 0.905003, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3363 - acc: 0.9375 - val_loss: 0.7388 - val_acc: 0.8162\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "181 [D loss supervised: 0.3363, acc.: 93.75%] [D loss unsupervised: 0.1726, acc.: 100.00%] [G loss: 0.817367, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3165 - acc: 0.9375 - val_loss: 0.7494 - val_acc: 0.8153\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "182 [D loss supervised: 0.3165, acc.: 93.75%] [D loss unsupervised: 0.1712, acc.: 100.00%] [G loss: 0.781390, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2657 - acc: 0.9688 - val_loss: 0.7621 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "183 [D loss supervised: 0.2657, acc.: 96.88%] [D loss unsupervised: 0.1716, acc.: 100.00%] [G loss: 0.760011, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3156 - acc: 0.9375 - val_loss: 0.7780 - val_acc: 0.8070\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "184 [D loss supervised: 0.3156, acc.: 93.75%] [D loss unsupervised: 0.1706, acc.: 100.00%] [G loss: 0.874116, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3696 - acc: 0.9688 - val_loss: 0.7956 - val_acc: 0.8033\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "185 [D loss supervised: 0.3696, acc.: 96.88%] [D loss unsupervised: 0.1771, acc.: 100.00%] [G loss: 1.150778, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4916 - acc: 0.8750 - val_loss: 0.8240 - val_acc: 0.7949\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "186 [D loss supervised: 0.4916, acc.: 87.50%] [D loss unsupervised: 0.1696, acc.: 100.00%] [G loss: 1.019350, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3048 - acc: 0.9375 - val_loss: 0.8542 - val_acc: 0.7888\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "187 [D loss supervised: 0.3048, acc.: 93.75%] [D loss unsupervised: 0.1705, acc.: 100.00%] [G loss: 1.055216, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3937 - acc: 0.9688 - val_loss: 0.8774 - val_acc: 0.7810\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "188 [D loss supervised: 0.3937, acc.: 96.88%] [D loss unsupervised: 0.1704, acc.: 100.00%] [G loss: 0.774622, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2900 - acc: 1.0000 - val_loss: 0.8923 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "189 [D loss supervised: 0.2900, acc.: 100.00%] [D loss unsupervised: 0.1749, acc.: 100.00%] [G loss: 0.656659, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2785 - acc: 0.9688 - val_loss: 0.8977 - val_acc: 0.7762\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "190 [D loss supervised: 0.2785, acc.: 96.88%] [D loss unsupervised: 0.1731, acc.: 100.00%] [G loss: 0.320684, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2449 - acc: 1.0000 - val_loss: 0.9018 - val_acc: 0.7754\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "191 [D loss supervised: 0.2449, acc.: 100.00%] [D loss unsupervised: 0.1700, acc.: 100.00%] [G loss: 0.707587, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3536 - acc: 0.9062 - val_loss: 0.8972 - val_acc: 0.7766\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "192 [D loss supervised: 0.3536, acc.: 90.62%] [D loss unsupervised: 0.1697, acc.: 100.00%] [G loss: 0.918111, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2848 - acc: 0.9688 - val_loss: 0.8980 - val_acc: 0.7780\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "193 [D loss supervised: 0.2848, acc.: 96.88%] [D loss unsupervised: 0.1696, acc.: 100.00%] [G loss: 0.829549, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3121 - acc: 0.9375 - val_loss: 0.8881 - val_acc: 0.7804\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "194 [D loss supervised: 0.3121, acc.: 93.75%] [D loss unsupervised: 0.1718, acc.: 100.00%] [G loss: 1.115487, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3170 - acc: 0.9688 - val_loss: 0.8761 - val_acc: 0.7821\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "195 [D loss supervised: 0.3170, acc.: 96.88%] [D loss unsupervised: 0.1711, acc.: 100.00%] [G loss: 0.414875, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2139 - acc: 1.0000 - val_loss: 0.8649 - val_acc: 0.7863\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "196 [D loss supervised: 0.2139, acc.: 100.00%] [D loss unsupervised: 0.1742, acc.: 100.00%] [G loss: 0.622547, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2957 - acc: 0.9375 - val_loss: 0.8454 - val_acc: 0.7921\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "197 [D loss supervised: 0.2957, acc.: 93.75%] [D loss unsupervised: 0.1686, acc.: 100.00%] [G loss: 0.814850, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3328 - acc: 0.9062 - val_loss: 0.8227 - val_acc: 0.7962\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "198 [D loss supervised: 0.3328, acc.: 90.62%] [D loss unsupervised: 0.1681, acc.: 100.00%] [G loss: 1.012279, acc.: 50.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2443 - acc: 0.9688 - val_loss: 0.8029 - val_acc: 0.8006\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "199 [D loss supervised: 0.2443, acc.: 96.88%] [D loss unsupervised: 0.1676, acc.: 100.00%] [G loss: 0.506509, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2422 - acc: 1.0000 - val_loss: 0.7862 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "200 [D loss supervised: 0.2422, acc.: 100.00%] [D loss unsupervised: 0.1734, acc.: 100.00%] [G loss: 0.485015, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2369 - acc: 1.0000 - val_loss: 0.7732 - val_acc: 0.8061\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "201 [D loss supervised: 0.2369, acc.: 100.00%] [D loss unsupervised: 0.1710, acc.: 100.00%] [G loss: 0.471460, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3566 - acc: 0.9375 - val_loss: 0.7600 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "202 [D loss supervised: 0.3566, acc.: 93.75%] [D loss unsupervised: 0.1708, acc.: 100.00%] [G loss: 0.462150, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3300 - acc: 0.9688 - val_loss: 0.7455 - val_acc: 0.8143\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "203 [D loss supervised: 0.3300, acc.: 96.88%] [D loss unsupervised: 0.1698, acc.: 100.00%] [G loss: 0.356749, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3384 - acc: 0.9375 - val_loss: 0.7348 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "204 [D loss supervised: 0.3384, acc.: 93.75%] [D loss unsupervised: 0.1738, acc.: 100.00%] [G loss: 0.631112, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3417 - acc: 0.9375 - val_loss: 0.7232 - val_acc: 0.8202\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "205 [D loss supervised: 0.3417, acc.: 93.75%] [D loss unsupervised: 0.1672, acc.: 100.00%] [G loss: 0.633173, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2364 - acc: 0.9688 - val_loss: 0.7157 - val_acc: 0.8220\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "206 [D loss supervised: 0.2364, acc.: 96.88%] [D loss unsupervised: 0.1667, acc.: 100.00%] [G loss: 0.522074, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2333 - acc: 1.0000 - val_loss: 0.7126 - val_acc: 0.8235\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "207 [D loss supervised: 0.2333, acc.: 100.00%] [D loss unsupervised: 0.1703, acc.: 100.00%] [G loss: 0.691296, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2795 - acc: 1.0000 - val_loss: 0.7085 - val_acc: 0.8250\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "208 [D loss supervised: 0.2795, acc.: 100.00%] [D loss unsupervised: 0.1677, acc.: 100.00%] [G loss: 0.949985, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2172 - acc: 1.0000 - val_loss: 0.7074 - val_acc: 0.8242\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "209 [D loss supervised: 0.2172, acc.: 100.00%] [D loss unsupervised: 0.1691, acc.: 100.00%] [G loss: 0.420123, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3663 - acc: 0.9062 - val_loss: 0.7065 - val_acc: 0.8250\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "210 [D loss supervised: 0.3663, acc.: 90.62%] [D loss unsupervised: 0.1749, acc.: 100.00%] [G loss: 0.729642, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1956 - acc: 1.0000 - val_loss: 0.7068 - val_acc: 0.8250\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "211 [D loss supervised: 0.1956, acc.: 100.00%] [D loss unsupervised: 0.1685, acc.: 100.00%] [G loss: 0.465370, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3006 - acc: 1.0000 - val_loss: 0.7095 - val_acc: 0.8236\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "212 [D loss supervised: 0.3006, acc.: 100.00%] [D loss unsupervised: 0.1667, acc.: 100.00%] [G loss: 0.446711, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2658 - acc: 1.0000 - val_loss: 0.7152 - val_acc: 0.8226\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "213 [D loss supervised: 0.2658, acc.: 100.00%] [D loss unsupervised: 0.1686, acc.: 100.00%] [G loss: 0.442487, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4555 - acc: 0.9062 - val_loss: 0.7186 - val_acc: 0.8223\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "214 [D loss supervised: 0.4555, acc.: 90.62%] [D loss unsupervised: 0.1667, acc.: 100.00%] [G loss: 0.292809, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2667 - acc: 0.9375 - val_loss: 0.7173 - val_acc: 0.8236\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "215 [D loss supervised: 0.2667, acc.: 93.75%] [D loss unsupervised: 0.1688, acc.: 100.00%] [G loss: 0.532791, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3174 - acc: 0.9375 - val_loss: 0.7174 - val_acc: 0.8233\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "216 [D loss supervised: 0.3174, acc.: 93.75%] [D loss unsupervised: 0.1664, acc.: 100.00%] [G loss: 0.930709, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3015 - acc: 0.9375 - val_loss: 0.7235 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "217 [D loss supervised: 0.3015, acc.: 93.75%] [D loss unsupervised: 0.1648, acc.: 100.00%] [G loss: 0.913649, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2682 - acc: 0.9688 - val_loss: 0.7344 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "218 [D loss supervised: 0.2682, acc.: 96.88%] [D loss unsupervised: 0.1650, acc.: 100.00%] [G loss: 1.103546, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2846 - acc: 0.9688 - val_loss: 0.7434 - val_acc: 0.8163\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "219 [D loss supervised: 0.2846, acc.: 96.88%] [D loss unsupervised: 0.1647, acc.: 100.00%] [G loss: 0.881550, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2591 - acc: 0.9688 - val_loss: 0.7467 - val_acc: 0.8150\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "220 [D loss supervised: 0.2591, acc.: 96.88%] [D loss unsupervised: 0.1649, acc.: 100.00%] [G loss: 1.411882, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3133 - acc: 0.9375 - val_loss: 0.7510 - val_acc: 0.8153\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "221 [D loss supervised: 0.3133, acc.: 93.75%] [D loss unsupervised: 0.1658, acc.: 100.00%] [G loss: 0.845170, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2281 - acc: 0.9688 - val_loss: 0.7545 - val_acc: 0.8148\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "222 [D loss supervised: 0.2281, acc.: 96.88%] [D loss unsupervised: 0.1652, acc.: 100.00%] [G loss: 0.926159, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2232 - acc: 0.9688 - val_loss: 0.7565 - val_acc: 0.8133\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "223 [D loss supervised: 0.2232, acc.: 96.88%] [D loss unsupervised: 0.1644, acc.: 100.00%] [G loss: 2.249823, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3220 - acc: 0.9375 - val_loss: 0.7645 - val_acc: 0.8116\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "224 [D loss supervised: 0.3220, acc.: 93.75%] [D loss unsupervised: 0.1645, acc.: 100.00%] [G loss: 1.935474, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4034 - acc: 0.9375 - val_loss: 0.7763 - val_acc: 0.8070\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "225 [D loss supervised: 0.4034, acc.: 93.75%] [D loss unsupervised: 0.1648, acc.: 100.00%] [G loss: 1.555964, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2237 - acc: 1.0000 - val_loss: 0.7890 - val_acc: 0.8033\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "226 [D loss supervised: 0.2237, acc.: 100.00%] [D loss unsupervised: 0.1639, acc.: 100.00%] [G loss: 1.136408, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3266 - acc: 0.9375 - val_loss: 0.7907 - val_acc: 0.8027\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "227 [D loss supervised: 0.3266, acc.: 93.75%] [D loss unsupervised: 0.1693, acc.: 100.00%] [G loss: 0.852863, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3549 - acc: 0.9062 - val_loss: 0.7888 - val_acc: 0.8037\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "228 [D loss supervised: 0.3549, acc.: 90.62%] [D loss unsupervised: 0.1636, acc.: 100.00%] [G loss: 1.065823, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2531 - acc: 0.9688 - val_loss: 0.7917 - val_acc: 0.8036\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "229 [D loss supervised: 0.2531, acc.: 96.88%] [D loss unsupervised: 0.1655, acc.: 100.00%] [G loss: 0.875978, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4112 - acc: 0.9375 - val_loss: 0.7914 - val_acc: 0.8046\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "230 [D loss supervised: 0.4112, acc.: 93.75%] [D loss unsupervised: 0.1645, acc.: 100.00%] [G loss: 1.149959, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2617 - acc: 0.9688 - val_loss: 0.7958 - val_acc: 0.8028\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "231 [D loss supervised: 0.2617, acc.: 96.88%] [D loss unsupervised: 0.1649, acc.: 100.00%] [G loss: 0.604720, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3502 - acc: 0.9062 - val_loss: 0.7858 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "232 [D loss supervised: 0.3502, acc.: 90.62%] [D loss unsupervised: 0.1633, acc.: 100.00%] [G loss: 1.534730, acc.: 50.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2073 - acc: 1.0000 - val_loss: 0.7749 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "233 [D loss supervised: 0.2073, acc.: 100.00%] [D loss unsupervised: 0.1638, acc.: 100.00%] [G loss: 1.393650, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2215 - acc: 1.0000 - val_loss: 0.7637 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "234 [D loss supervised: 0.2215, acc.: 100.00%] [D loss unsupervised: 0.1654, acc.: 100.00%] [G loss: 0.963685, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4123 - acc: 0.9375 - val_loss: 0.7485 - val_acc: 0.8143\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "235 [D loss supervised: 0.4123, acc.: 93.75%] [D loss unsupervised: 0.1631, acc.: 100.00%] [G loss: 0.631813, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3458 - acc: 0.9688 - val_loss: 0.7390 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "236 [D loss supervised: 0.3458, acc.: 96.88%] [D loss unsupervised: 0.1627, acc.: 100.00%] [G loss: 0.841634, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3468 - acc: 0.9375 - val_loss: 0.7311 - val_acc: 0.8201\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "237 [D loss supervised: 0.3468, acc.: 93.75%] [D loss unsupervised: 0.1624, acc.: 100.00%] [G loss: 0.959138, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1904 - acc: 1.0000 - val_loss: 0.7261 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "238 [D loss supervised: 0.1904, acc.: 100.00%] [D loss unsupervised: 0.1643, acc.: 100.00%] [G loss: 0.735548, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2101 - acc: 1.0000 - val_loss: 0.7252 - val_acc: 0.8175\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "239 [D loss supervised: 0.2101, acc.: 100.00%] [D loss unsupervised: 0.1624, acc.: 100.00%] [G loss: 0.939298, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2751 - acc: 0.9688 - val_loss: 0.7308 - val_acc: 0.8163\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "240 [D loss supervised: 0.2751, acc.: 96.88%] [D loss unsupervised: 0.1642, acc.: 100.00%] [G loss: 0.553844, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2541 - acc: 0.9688 - val_loss: 0.7365 - val_acc: 0.8149\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "241 [D loss supervised: 0.2541, acc.: 96.88%] [D loss unsupervised: 0.1638, acc.: 100.00%] [G loss: 0.466552, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2694 - acc: 0.9688 - val_loss: 0.7452 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "242 [D loss supervised: 0.2694, acc.: 96.88%] [D loss unsupervised: 0.1664, acc.: 100.00%] [G loss: 0.704896, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2048 - acc: 1.0000 - val_loss: 0.7529 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "243 [D loss supervised: 0.2048, acc.: 100.00%] [D loss unsupervised: 0.1627, acc.: 100.00%] [G loss: 0.685761, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3746 - acc: 0.9375 - val_loss: 0.7585 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "244 [D loss supervised: 0.3746, acc.: 93.75%] [D loss unsupervised: 0.1626, acc.: 100.00%] [G loss: 0.629869, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3539 - acc: 0.9375 - val_loss: 0.7543 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "245 [D loss supervised: 0.3539, acc.: 93.75%] [D loss unsupervised: 0.1607, acc.: 100.00%] [G loss: 0.706157, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2302 - acc: 1.0000 - val_loss: 0.7507 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "246 [D loss supervised: 0.2302, acc.: 100.00%] [D loss unsupervised: 0.1708, acc.: 100.00%] [G loss: 0.891870, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2775 - acc: 0.9375 - val_loss: 0.7505 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "247 [D loss supervised: 0.2775, acc.: 93.75%] [D loss unsupervised: 0.1612, acc.: 100.00%] [G loss: 1.044902, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2747 - acc: 0.9375 - val_loss: 0.7453 - val_acc: 0.8131\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "248 [D loss supervised: 0.2747, acc.: 93.75%] [D loss unsupervised: 0.1613, acc.: 100.00%] [G loss: 0.935235, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2883 - acc: 0.9688 - val_loss: 0.7374 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "249 [D loss supervised: 0.2883, acc.: 96.88%] [D loss unsupervised: 0.1612, acc.: 100.00%] [G loss: 0.654087, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4148 - acc: 0.9062 - val_loss: 0.7285 - val_acc: 0.8167\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "250 [D loss supervised: 0.4148, acc.: 90.62%] [D loss unsupervised: 0.1631, acc.: 100.00%] [G loss: 0.634800, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3467 - acc: 0.9375 - val_loss: 0.7226 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "251 [D loss supervised: 0.3467, acc.: 93.75%] [D loss unsupervised: 0.1605, acc.: 100.00%] [G loss: 0.872235, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2048 - acc: 1.0000 - val_loss: 0.7194 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "252 [D loss supervised: 0.2048, acc.: 100.00%] [D loss unsupervised: 0.1610, acc.: 100.00%] [G loss: 1.224985, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3731 - acc: 0.9375 - val_loss: 0.7237 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "253 [D loss supervised: 0.3731, acc.: 93.75%] [D loss unsupervised: 0.1632, acc.: 100.00%] [G loss: 0.459081, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2336 - acc: 1.0000 - val_loss: 0.7282 - val_acc: 0.8188\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "254 [D loss supervised: 0.2336, acc.: 100.00%] [D loss unsupervised: 0.1614, acc.: 100.00%] [G loss: 0.907497, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2305 - acc: 1.0000 - val_loss: 0.7347 - val_acc: 0.8178\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "255 [D loss supervised: 0.2305, acc.: 100.00%] [D loss unsupervised: 0.1606, acc.: 100.00%] [G loss: 0.675659, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1937 - acc: 1.0000 - val_loss: 0.7406 - val_acc: 0.8171\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "256 [D loss supervised: 0.1937, acc.: 100.00%] [D loss unsupervised: 0.1610, acc.: 100.00%] [G loss: 0.850139, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4366 - acc: 0.9375 - val_loss: 0.7442 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "257 [D loss supervised: 0.4366, acc.: 93.75%] [D loss unsupervised: 0.1612, acc.: 100.00%] [G loss: 0.529339, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2004 - acc: 1.0000 - val_loss: 0.7504 - val_acc: 0.8156\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "258 [D loss supervised: 0.2004, acc.: 100.00%] [D loss unsupervised: 0.1611, acc.: 100.00%] [G loss: 0.766105, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2540 - acc: 1.0000 - val_loss: 0.7505 - val_acc: 0.8150\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "259 [D loss supervised: 0.2540, acc.: 100.00%] [D loss unsupervised: 0.1780, acc.: 98.44%] [G loss: 0.254707, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2297 - acc: 1.0000 - val_loss: 0.7492 - val_acc: 0.8162\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "260 [D loss supervised: 0.2297, acc.: 100.00%] [D loss unsupervised: 0.1613, acc.: 100.00%] [G loss: 0.180443, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2361 - acc: 0.9688 - val_loss: 0.7343 - val_acc: 0.8200\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "261 [D loss supervised: 0.2361, acc.: 96.88%] [D loss unsupervised: 0.1657, acc.: 100.00%] [G loss: 0.379833, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2571 - acc: 0.9688 - val_loss: 0.7217 - val_acc: 0.8226\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "262 [D loss supervised: 0.2571, acc.: 96.88%] [D loss unsupervised: 0.1598, acc.: 100.00%] [G loss: 1.646220, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4568 - acc: 0.8438 - val_loss: 0.7111 - val_acc: 0.8246\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "263 [D loss supervised: 0.4568, acc.: 84.38%] [D loss unsupervised: 0.1591, acc.: 100.00%] [G loss: 3.665997, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2357 - acc: 1.0000 - val_loss: 0.7038 - val_acc: 0.8252\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "264 [D loss supervised: 0.2357, acc.: 100.00%] [D loss unsupervised: 0.1579, acc.: 100.00%] [G loss: 3.923737, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2394 - acc: 0.9688 - val_loss: 0.7017 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "265 [D loss supervised: 0.2394, acc.: 96.88%] [D loss unsupervised: 0.1601, acc.: 100.00%] [G loss: 4.213125, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5383 - acc: 0.9062 - val_loss: 0.7096 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "266 [D loss supervised: 0.5383, acc.: 90.62%] [D loss unsupervised: 0.1587, acc.: 100.00%] [G loss: 3.924668, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3391 - acc: 0.9375 - val_loss: 0.7181 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "267 [D loss supervised: 0.3391, acc.: 93.75%] [D loss unsupervised: 0.1606, acc.: 100.00%] [G loss: 3.554521, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3092 - acc: 0.9688 - val_loss: 0.7261 - val_acc: 0.8215\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "268 [D loss supervised: 0.3092, acc.: 96.88%] [D loss unsupervised: 0.1579, acc.: 100.00%] [G loss: 4.506756, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2150 - acc: 1.0000 - val_loss: 0.7349 - val_acc: 0.8193\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "269 [D loss supervised: 0.2150, acc.: 100.00%] [D loss unsupervised: 0.1580, acc.: 100.00%] [G loss: 2.555573, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.7426 - val_acc: 0.8157\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "270 [D loss supervised: 0.2105, acc.: 100.00%] [D loss unsupervised: 0.1580, acc.: 100.00%] [G loss: 1.446114, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3050 - acc: 0.9688 - val_loss: 0.7471 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "271 [D loss supervised: 0.3050, acc.: 96.88%] [D loss unsupervised: 0.1587, acc.: 100.00%] [G loss: 1.067310, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2124 - acc: 1.0000 - val_loss: 0.7510 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "272 [D loss supervised: 0.2124, acc.: 100.00%] [D loss unsupervised: 0.1588, acc.: 100.00%] [G loss: 0.656670, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2607 - acc: 0.9688 - val_loss: 0.7508 - val_acc: 0.8142\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "273 [D loss supervised: 0.2607, acc.: 96.88%] [D loss unsupervised: 0.1598, acc.: 100.00%] [G loss: 1.178219, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2540 - acc: 0.9688 - val_loss: 0.7493 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "274 [D loss supervised: 0.2540, acc.: 96.88%] [D loss unsupervised: 0.1590, acc.: 100.00%] [G loss: 0.736979, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2534 - acc: 0.9688 - val_loss: 0.7493 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "275 [D loss supervised: 0.2534, acc.: 96.88%] [D loss unsupervised: 0.1576, acc.: 100.00%] [G loss: 0.954533, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2561 - acc: 1.0000 - val_loss: 0.7449 - val_acc: 0.8158\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "276 [D loss supervised: 0.2561, acc.: 100.00%] [D loss unsupervised: 0.1691, acc.: 100.00%] [G loss: 0.310560, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2363 - acc: 1.0000 - val_loss: 0.7374 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "277 [D loss supervised: 0.2363, acc.: 100.00%] [D loss unsupervised: 0.1580, acc.: 100.00%] [G loss: 0.219060, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3174 - acc: 0.9375 - val_loss: 0.7264 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "278 [D loss supervised: 0.3174, acc.: 93.75%] [D loss unsupervised: 0.1581, acc.: 100.00%] [G loss: 0.263907, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3303 - acc: 0.9375 - val_loss: 0.7232 - val_acc: 0.8235\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "279 [D loss supervised: 0.3303, acc.: 93.75%] [D loss unsupervised: 0.1577, acc.: 100.00%] [G loss: 0.299360, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2248 - acc: 0.9688 - val_loss: 0.7219 - val_acc: 0.8240\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "280 [D loss supervised: 0.2248, acc.: 96.88%] [D loss unsupervised: 0.1576, acc.: 100.00%] [G loss: 0.308231, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2805 - acc: 0.9688 - val_loss: 0.7231 - val_acc: 0.8245\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "281 [D loss supervised: 0.2805, acc.: 96.88%] [D loss unsupervised: 0.1567, acc.: 100.00%] [G loss: 0.295135, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2888 - acc: 0.9375 - val_loss: 0.7268 - val_acc: 0.8253\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "282 [D loss supervised: 0.2888, acc.: 93.75%] [D loss unsupervised: 0.1561, acc.: 100.00%] [G loss: 0.475218, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2394 - acc: 1.0000 - val_loss: 0.7312 - val_acc: 0.8246\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "283 [D loss supervised: 0.2394, acc.: 100.00%] [D loss unsupervised: 0.1561, acc.: 100.00%] [G loss: 1.385925, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 0.7374 - val_acc: 0.8217\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "284 [D loss supervised: 0.2190, acc.: 100.00%] [D loss unsupervised: 0.1560, acc.: 100.00%] [G loss: 0.539374, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2638 - acc: 0.9688 - val_loss: 0.7414 - val_acc: 0.8220\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "285 [D loss supervised: 0.2638, acc.: 96.88%] [D loss unsupervised: 0.1556, acc.: 100.00%] [G loss: 0.428945, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2503 - acc: 1.0000 - val_loss: 0.7455 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "286 [D loss supervised: 0.2503, acc.: 100.00%] [D loss unsupervised: 0.1555, acc.: 100.00%] [G loss: 0.382199, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3385 - acc: 0.9688 - val_loss: 0.7504 - val_acc: 0.8187\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "287 [D loss supervised: 0.3385, acc.: 96.88%] [D loss unsupervised: 0.1565, acc.: 100.00%] [G loss: 0.606466, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2440 - acc: 0.9688 - val_loss: 0.7557 - val_acc: 0.8175\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "288 [D loss supervised: 0.2440, acc.: 96.88%] [D loss unsupervised: 0.1564, acc.: 100.00%] [G loss: 0.414641, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3776 - acc: 0.9375 - val_loss: 0.7534 - val_acc: 0.8186\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "289 [D loss supervised: 0.3776, acc.: 93.75%] [D loss unsupervised: 0.1561, acc.: 100.00%] [G loss: 0.352731, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3309 - acc: 0.9375 - val_loss: 0.7450 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "290 [D loss supervised: 0.3309, acc.: 93.75%] [D loss unsupervised: 0.1549, acc.: 100.00%] [G loss: 0.612685, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2318 - acc: 1.0000 - val_loss: 0.7402 - val_acc: 0.8220\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "291 [D loss supervised: 0.2318, acc.: 100.00%] [D loss unsupervised: 0.1547, acc.: 100.00%] [G loss: 0.266468, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2941 - acc: 0.9688 - val_loss: 0.7327 - val_acc: 0.8240\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "292 [D loss supervised: 0.2941, acc.: 96.88%] [D loss unsupervised: 0.1546, acc.: 100.00%] [G loss: 0.253580, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2313 - acc: 1.0000 - val_loss: 0.7293 - val_acc: 0.8237\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "293 [D loss supervised: 0.2313, acc.: 100.00%] [D loss unsupervised: 0.1543, acc.: 100.00%] [G loss: 0.259703, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3230 - acc: 0.9062 - val_loss: 0.7251 - val_acc: 0.8251\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "294 [D loss supervised: 0.3230, acc.: 90.62%] [D loss unsupervised: 0.1573, acc.: 100.00%] [G loss: 0.306037, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2763 - acc: 0.9688 - val_loss: 0.7221 - val_acc: 0.8258\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "295 [D loss supervised: 0.2763, acc.: 96.88%] [D loss unsupervised: 0.1565, acc.: 100.00%] [G loss: 0.257794, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2371 - acc: 0.9688 - val_loss: 0.7192 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "296 [D loss supervised: 0.2371, acc.: 96.88%] [D loss unsupervised: 0.1539, acc.: 100.00%] [G loss: 0.213428, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3578 - acc: 0.9688 - val_loss: 0.7115 - val_acc: 0.8299\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "297 [D loss supervised: 0.3578, acc.: 96.88%] [D loss unsupervised: 0.1538, acc.: 100.00%] [G loss: 0.198397, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3623 - acc: 0.9062 - val_loss: 0.7010 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "298 [D loss supervised: 0.3623, acc.: 90.62%] [D loss unsupervised: 0.1542, acc.: 100.00%] [G loss: 0.213941, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2195 - acc: 1.0000 - val_loss: 0.6947 - val_acc: 0.8335\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "299 [D loss supervised: 0.2195, acc.: 100.00%] [D loss unsupervised: 0.1544, acc.: 100.00%] [G loss: 0.259715, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2428 - acc: 0.9688 - val_loss: 0.6942 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.84790\n",
            "300 [D loss supervised: 0.2428, acc.: 96.88%] [D loss unsupervised: 0.1537, acc.: 100.00%] [G loss: 0.180611, acc.: 100.00%]\n",
            "Training time: 1315.0732s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfUFnOB4qLYj",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rt3ZhdCn6mk",
        "colab_type": "code",
        "outputId": "396e6132-f050-4b19-f433-a050fdcc3990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-300.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 33s 665us/step\n",
            "Training Accuracy: 86.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO7jfk1Pa-J8",
        "colab_type": "code",
        "outputId": "2b8cd7af-c070-4215-e64d-e25ea5ed14e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-300.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 516us/step\n",
            "Test Accuracy: 83.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk87Xx_Aa-Bc",
        "colab_type": "code",
        "outputId": "c099a813-23cc-4252-ade1-e9b2a659f5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "div = 10\n",
        "accs = []\n",
        "tx = [x for x in range(1*div, (len(iteration_checkpoints)+1) * div, div)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"SCGAN-2D's accs with epoch, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 516us/step\n",
            "10000/10000 [==============================] - 5s 518us/step\n",
            "10000/10000 [==============================] - 5s 515us/step\n",
            "10000/10000 [==============================] - 5s 511us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 514us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 513us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 524us/step\n",
            "10000/10000 [==============================] - 5s 514us/step\n",
            "10000/10000 [==============================] - 5s 535us/step\n",
            "10000/10000 [==============================] - 5s 534us/step\n",
            "10000/10000 [==============================] - 5s 514us/step\n",
            "10000/10000 [==============================] - 5s 515us/step\n",
            "10000/10000 [==============================] - 5s 521us/step\n",
            "10000/10000 [==============================] - 5s 525us/step\n",
            "10000/10000 [==============================] - 5s 513us/step\n",
            "10000/10000 [==============================] - 5s 511us/step\n",
            "10000/10000 [==============================] - 5s 512us/step\n",
            "10000/10000 [==============================] - 5s 519us/step\n",
            "10000/10000 [==============================] - 5s 513us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 516us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 512us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 518us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 515us/step\n",
            "10000/10000 [==============================] - 5s 529us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 516us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 492us/step\n",
            "10000/10000 [==============================] - 5s 537us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 531us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 490us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 494us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 494us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 493us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 510us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 494us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 513us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 511us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 515us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 494us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 492us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 511us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 512us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 512us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 514us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 494us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 503us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 493us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 507us/step\n",
            "10000/10000 [==============================] - 5s 514us/step\n",
            "10000/10000 [==============================] - 5s 509us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 504us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "10000/10000 [==============================] - 5s 502us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 495us/step\n",
            "10000/10000 [==============================] - 5s 493us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 493us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 493us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 489us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 499us/step\n",
            "10000/10000 [==============================] - 5s 497us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 500us/step\n",
            "10000/10000 [==============================] - 5s 506us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 505us/step\n",
            "10000/10000 [==============================] - 5s 508us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 496us/step\n",
            "10000/10000 [==============================] - 5s 493us/step\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "10000/10000 [==============================] - 5s 501us/step\n",
            "0.8479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efe38a92748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFWCAYAAAA2SU9mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXfwxMiWbBLD",
        "colab_type": "code",
        "outputId": "aa97e694-a55e-4865-808d-145a23a90997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "d_supervised_loss = np.array(losses_d_supervised)\n",
        "d_unsupervised_loss = np.array(losses_d_unsupervised)\n",
        "d_unsupervised_real_loss = np.array(losses_d_unsupervised_real)\n",
        "d_unsupervised_fake_loss = np.array(losses_d_unsupervised_fake)\n",
        "d_loss = np.array(losses_d)\n",
        "g_loss = np.array(losses_g)  # Generator unsupervised loss\n",
        "all_loss = np.add(d_loss, g_loss)\n",
        "\n",
        "# Plot Discriminator supervised loss\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, d_supervised_loss, label=\"Discriminator supervised loss\", color='blue', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, d_unsupervised_loss, label=\"Discriminator unsupervised loss\", color='green', linestyle='dashed')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_real_loss, label=\"Discriminator unsupervised real loss\", color='yellow')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_fake_loss, label=\"Discriminator unsupervised fake loss\", color='yellow')\n",
        "plt.plot(iteration_checkpoints, g_loss, label=\"Generator unsupervised loss\", color='tab:red', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, all_loss, label=\"All losses\", color='black')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"SCGAN-2D's Discriminator Loss + Generator Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efe384c2e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAFWCAYAAAAR586OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWAwGDo4zHAzGAwGg8FgMBgMhg6O/w+9Pm50cnoougAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq-YzbJRqOMH",
        "colab_type": "text"
      },
      "source": [
        "## Generate Pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB2PSr3uEFw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMF87KVVEFuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "# fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "# fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "# gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "def sample_images(image_grid_rows=2, image_grid_columns=5):\n",
        "\n",
        "    # Sample random noise\n",
        "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # Get image labels 0-9\n",
        "    fake_labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "    fake_labels_category = to_categorical(fake_labels, num_classes=num_classes)\n",
        "\n",
        "    # Generate images from random noise\n",
        "    gen_imgs = generator.predict([z, fake_labels_category])\n",
        "\n",
        "    # Rescale image pixel values to [0, 1]\n",
        "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    # gen_imgs = (gen_imgs+1) * 255/2\n",
        "\n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows,\n",
        "                            image_grid_columns,\n",
        "                            figsize=(10, 4),\n",
        "                            sharey=True,\n",
        "                            sharex=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output a grid of images\n",
        "            axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')\n",
        "            axs[i, j].set_title(\"Class: \" + str(d_name[fake_labels[cnt]]))\n",
        "            cnt += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx4Ocr7DLE0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_images()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKcTHSEFEFq8",
        "colab_type": "code",
        "outputId": "f4cc8bb5-ec3d-480b-cdd4-b5c9eba5221d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "d_name = {0:\"airplane\", 1:\"automobile\", 2:\"bird\", 3:\"cat\", 4:\"deer\", 5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9:\"truck\"}\t\t\n",
        "i = 1\t\t\t\t\n",
        "plt.imshow(x_test[i],cmap='binary')\n",
        "print(y_test[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAX80lEQVR4nO3dbWxcVXoH8P8znkwGZ+I6ZmIc26Qm\nBCtKIwipldJtiii7oBStCqwQSlaskKCbVbtIRdr9gOi2ULXq7lYFlg8VVSgs7IryUl5EWtF2eVmU\npghCyAZjkjSEYIKTOMa1vcY4zmQ8Tz/MRTj0PGfsebljc/4/Kcr4Pj5zz1zfZ67nPj7niKqCiL78\nEvXuABHFg8lOFAgmO1EgmOxEgWCyEwWCyU4UiGQljUVkM4D7ATQA+CdV/ZHv+5ctW6YdHR3OWKFQ\n8O2ngl5SRcqszGoZPzKBp5GvH+ILlvMCFu75dvz4cYyOjjpfQNnJLiINAP4BwFUABgC8KSI7VHW/\n1aajowNPP/20MzY1NWXuK5H4cv4CMm9el/0+6415mxlnVsHTKmk1KrWzRN4OGbGC55fahCctfBel\ncpVzHlj92Lp1q72fOe/lcxsBHFbVI6qaA/AEgGsreD4iqqFKkr0DwEczvh6IthHRPFTz3yNFZJuI\n7BGRPaOjo7XeHREZKkn2YwDOn/F1Z7TtLKq6XVV7VLVn2bJlFeyOiCpRSbK/CeAiEblARFIAtgDY\nUZ1uEVG1lX03XlXzInIbgP9EsfT2sKq+62sjIkilUuXu8ktn3tyN90jkc2bMe1866X5ted/1peA5\nNwqeu+dJuycJWHfqfb1fuHfjfWXqiursqvoCgBcqeQ4iisf8v7QQUVUw2YkCwWQnCgSTnSgQTHai\nQFR0N74cVsmgFiWN+S7O1+wt7/j6UbAHmXiraGYZzT7lxiftwVDpxkZ7Zzm7j6lEOcfY85rniXLO\nHV7ZiQLBZCcKBJOdKBBMdqJAMNmJAhH73XjrrvBCGBRiWfCVBM+hz3leWyFvN5zKu+9oT07ZA2te\neOklM3bx+nVmLD8xYcbWrl7l3N7cZN/dzy+An2c5+bJwM4yI5oTJThQIJjtRIJjsRIFgshMFgslO\nFIgFMRBmIZflfMp9XdUv9dn9SDVmzFjOMy/cyOC4c3v/0SGzTe/Bw2aspb3VjHW3t5uxZMJ9ivtW\nfbFWkamI52cd19n95cwiIvp/mOxEgWCyEwWCyU4UCCY7USCY7ESBqKj0JiL9AD4BMA0gr6o9/u8H\nksayQL4RVHHyVJNKrHfk5iuvJcssveU879F5Y7RZKmX/qCcmJs3Y/kMDZmxgaMyMjYy7R7cNDbtL\ncgCQbMqasaERe2RbW9b+wUwZIbug6K2S1UQ1S8s1W/4p8geqOlyF5yGiGpofl1MiqrlKk10B/EJE\n3hKRbdXoEBHVRqW/xm9S1WMi0grgRRE5qKo7Z35D9CawDQDaPX/WSES1VdGVXVWPRf8PAXgOwEbH\n92xX1R5V7Tn33JZKdkdEFSg72UVkiYgs/ewxgKsB9FWrY0RUXZX8Gn8egOeiW/1JAP+sqv/ha3Am\nP42h4RF3MG+XT9Ip91JCBU+bVNpafsgfS3iWC7LKcsl8eYcx6Xuv9ZRjBsftkpc1Iq4lnTbbjHmW\nXdrrKb31HbFjeeO1TVq1MADDxwftfXlGxL2+e68Zu+Garzm3X33FZWabVMGeFNM74rDgOQ98l1Uj\n5lu5yjp37MJbBcmuqkcAXFJueyKKF0tvRIFgshMFgslOFAgmO1EgmOxEgYh1wslcfhr9I+5RT21Z\ne0LBZNq9Llcub5eMvNUwz1tcyhNLGrW3RLLMw1jmJJv79u42Y6tXr3Zub2m2x3mNj9njmLJNdrv1\na9eYsYJxkIeG7bJha8be18SYUbIFkEraE0QOjrvPtynfBJAJu0zpn+zT95xltPK0sbqhdhNe2YlC\nwWQnCgSTnSgQTHaiQDDZiQIR6914SS1GurPbGct57mhPJo2BKwl7wIIvlsvbsaTvDrm1dFU5k9PB\nP9+dMVUfAGBqwp7HLWEN4vBULro8SytNTnpeW8pdJQGAbJt77gLf3fhEqskTsw9IU4vdj4RxIKeM\nZaEAoOBb/anMn5lvAkOr9/6nm/s5xys7USCY7ESBYLITBYLJThQIJjtRIJjsRIGItfT2/qH3cO3X\nNruDnvnkYAyEWdbebDbZfOUmM/anN91g78pzRKw573yDIwq+eoxndMSUp1S2yhjsAgCZJvcxsQam\nAEAmY5e8ulfZ8/UVYMfSxqCWjGcuPDTaP8+xKft49A8csWNHjzq3Hz/ab7aZtOZJBLwTw3V3d5mx\naza758IDgMaM+5j4qmtWSRGe5Z94ZScKBJOdKBBMdqJAMNmJAsFkJwoEk50oECVLbyLyMICvAxhS\n1XXRthYATwLoAtAP4EZVHS25t/w0YI16OmOPhgLcpYnRj+wWezwlntw3rjNjY4UJM5Y0Sm9NGXvB\nSl/5JOcr2XnKcitXrzVjSaudZ1ThRN4e5pXyzAsHz8gx6xnzntFfr+58yYzt7rOXETx86JAZGxlx\nl9Fy43Ypb2LEPgd03J6v77e+crEZ2/QVe7mpVqP05hsp5yulWmbT4hEAXyyO3wHgZVW9CMDL0ddE\nNI+VTPZovfUvvj1eC+DR6PGjAOxLJRHNC+V+Zj9PVU9EjwdRXNGViOaxiv9cVlVVRMzpqkVkG4Bt\nxa8aKt0dEZWp3Cv7SRFZAQDR/0PWN6rqdlXtUdUeCG/+E9VLudm3A8DN0eObATxfne4QUa3MpvT2\nOIArAGRFZADAXQB+BOApEbkVwIcAbpzNzjouWIXbfrjdGRv3jDRqbXGXthKe0kSLWc4AEp4JBQcG\nBsxYfmrSub0xbY/WSrfYsULaHjU2MmmXfwp5+7UljRJbozFyEADSnn40NnqWNErOvXQ46Sk3juXd\nxxcAWjvbzNiqri4zlptwP2dzyi6X9h9yj5QDgNd3v2rGNl9pjOgEkEp6SsHGMUl5yq/lLP9UMtlV\ndasR+mqptkQ0f/BDNFEgmOxEgWCyEwWCyU4UCCY7USBinXBSC4r8pLvulfJ0xSoMtWXsNcpamu1J\nFEfG7PLa8KS9DtyrL73q3J7xjHrbdOXlZuyV1/aYsZ/f90Mz5neOe3ODfTzgKQ+e29lpxrpWutdz\nA4BbbrnJuX3tmlVmm6sv22jGkgm7PJjyjL6bGHOvi5f2lMJG1tkTevZs6LJjGzeYsVzOPq+Gh93l\nQavkDPgGHHLCSaLgMdmJAsFkJwoEk50oEEx2okAw2YkCEWvp7fiHH+Evtn3fHTxjj3gCjBFgi7Nm\ni+WektEV19iT/63ttkdXdW9wrx+3es06s01zq13W6n92pxkr3yn35mljOwB8fNIM/e/Hh83Yuq4/\nNmPdre6yXGvKLnkVPJeeiQl7gsipnLu8BgDDxppukzn7fGvJ2j+zri673Nu7r9eMHTxoH8eWVneJ\n7eL19nmVzbpLqbn8tNmGV3aiQDDZiQLBZCcKBJOdKBBMdqJAxHo3HnoKOPOr6j3f6WNm6OMp+67v\nzt2vmbFDe+3dbbnRPdVexjOQZHjcnkuu0TNYB1juidl3n62lspZfcqXZ4sZv/JEZ61nTZcY6s/ZA\njfyY+3W/tm+/2abvyBEztveg3W5o0JzcGP39/c7tpyftY7jIM39hpsn+Weem7Pn1JiftakK2y125\n2OKZ2nGlMQjp1Gl7P7yyEwWCyU4UCCY7USCY7ESBYLITBYLJThSI2Sz/9DCArwMYUtV10ba7AXwb\nwMfRt92pqi/UqpOmC37bDJ3TZL+PnXzj3+2YZ3dtCfc8Yn0H7Xpda+dKM9bdaZdxVv/1D8xY0jPn\n2sqV7v2t6e422xw+fMiMvbLT/rEe7bfn8hs4ety5/fjAsNnm1KhdQvv8VKsWe662M5/ax3d0yj6v\nlnuWoVq1zp6vrynrHtCVabEHeg2OjDm3n6lwIMwjAFyLWN2nquujf/EnOhHNSclkV9WdAOxVF4lo\nQajkM/ttItIrIg+LyLKq9YiIaqLcZH8AwIUA1gM4AeAe6xtFZJuI7BERe5J0Iqq5spJdVU+q6rSq\nFgA8CMCc3V9Vt6tqj6r2lNtJIqpcWckuIitmfHk9gL7qdIeIamU2pbfHAVwBICsiAwDuAnCFiKwH\noAD6AXynhn3E0hWXOLf/5Cd/a7a54wfGXHcwZ2kDAHQsWWrGWtLu98bmhHtJKwC4eKU9F167J9bs\nmQdtCvboKmtU1lTO7uO+HbvN2K4+e161iUm7H+lm91xt7e320kotzXapaXLCN0ehrTHT6NyeSnmW\nG/PE2tvtElpnpx1Lpezr6uCQuxzZ23vQbDM25m4z8al9nEomu6pudWx+qFQ7Ippf+Bd0RIFgshMF\ngslOFAgmO1EgmOxEgRBVjW9nImXt7P6f/qtz+4YN9vI4r++2y0nJpF0yam+0J4HsbHOXk1IZu0yW\nztiTMhY8/chbS14B2H/EHqXWuco9ui3vKbwMDduTLw4O2f04fMQe9dZujACbzNmvOVGw+9iYTJmx\nfN4uK46NuUeHDQ4Nmm0KeffoRgAYHrbbHem3l3gaG7FH+00Ou/uYy9n9yLa6z9PRd3fhzKdjziF9\nvLITBYLJThQIJjtRIJjsRIFgshMFgslOFIh5U3q78HfsgXPfvMW95tWUsZ4YAOQT9vtYc2ebGUv6\n3v+sUohd+UEuZz9fwl5SDHnPem7HB+ySV6rRXQbc02ePQh4ft0dK5cfstcNas+5SJAAMj7lnMntv\n1y6zDdLuEWoAcM4ae8LMU+Oete+OHnVv/7U9ogywS17eH7Y3ZpdgscR9HBt8awEaIy2nB3uhuQmW\n3ohCxmQnCgSTnSgQTHaiQDDZiQIR7934ZJMi456I9qotN5ntDh1yLyU04ZmXbMIzOCIHe1CF5u3B\nGIuM98aEZ064nGfuN/W0a/BOGGa3m54y9ve+PZdc2WuALO2yYxPGHXK1B4vA83MB7Dv1/v67B5nA\nM9AI8JRJvP3w/dB8r81ot8RzLra6qy75Y70onObdeKKgMdmJAsFkJwoEk50oEEx2okAw2YkCUbL0\nJiLnA/gZgPNQXO5pu6reLyItAJ4E0IXiElA3qupoieeKr84HexknwJ4zzl9asconvpJLmfta7BkE\n4ZnXDgXjOac8paaEZ2mllGdQSM4eJANzGSKrFAbAM/jHX17ztbMW+2rwtPGV3jyvGdOemM9y9+bf\nsM+dZZ3upbLG3+9F/lT5pbc8gO+p6loAlwH4roisBXAHgJdV9SIAL0dfE9E8VTLZVfWEqu6NHn8C\n4ACADgDXAng0+rZHAVxXq04SUeXm9JldRLoAXArgDQDnqeqJKDSI4q/5RDRPlVzF9TMikgHwDIDb\nVXVc5POPBaqq1udxEdkGYFulHSWiyszqyi4ii1BM9MdU9dlo80kRWRHFVwAYcrVV1e2q2qOqPdXo\nMBGVp2SyS/ES/hCAA6p674zQDgA3R49vBvB89btHRNUym9LbJgD/BeAdfD7c6k4UP7c/BWAlgA9R\nLL15h0/FW3ojmk+c1bCIu8S2aJldmm3KutuMHX0f+alTzp2V/Myuqrtg9/SrpdoT0fzAv6AjCgST\nnSgQTHaiQDDZiQLBZCcKxKz/go6IAHOEWkkfe2LukYDplH0t7up0j3w8MPih2YZXdqJAMNmJAsFk\nJwoEk50oEEx2okAw2YkCwdIbBeoiM3LBYntdtsHTA2bsFH5dVk/OWeoe3dbWbk8s2t29yrn9g3f2\nm214ZScKBJOdKBBMdqJAMNmJAsFkJwoE78Z/6biXvfrq73/TbLFr104zdlqPePZ1eradqthyLDFj\nXWg1Yzm4l696G++ZbT6owctqWGQvN5Vtd/d/9Zpus03bypXO7cmUXUnglZ0oEEx2okAw2YkCwWQn\nCgSTnSgQTHaiQJQsvYnI+QB+huKSzApgu6reLyJ3A/g2Pp9c605VfaFWHaXZ+sS5de2uV8wWW5L2\naTBypt2M5Y2yFgAkMOXcPmbMtwYARzFtxvrwqRl7Ex+YsTg1nGPHUi0ZM5Zd2enc3pS1S4qJVNq5\nXRL2MlOzqbPnAXxPVfeKyFIAb4nIi1HsPlX9+1k8BxHV2WzWejsB4ET0+BMROQCgo9YdI6LqmtNn\ndhHpAnApiiu4AsBtItIrIg+LyLIq942IqmjWyS4iGQDPALhdVccBPADgQgDrUbzy32O02yYie0Rk\nTxX6S0RlmlWyi8giFBP9MVV9FgBU9aSqTqtqAcCDADa62qrqdlXtUdWeanWaiOauZLKLiAB4CMAB\nVb13xvYVM77tegB91e8eEVXLbO7G/x6AbwF4R0T2RdvuBLBVRNajWI7rB/CdmvSQquI1tUd5rT1j\ntyt4ntMuvAGDxvZ9xnbg8xtB85pd2QJSdrCtrc2MZTvd5c1kY5PZppBwp656Ojibu/G74H6JrKkT\nLSD8CzqiQDDZiQLBZCcKBJOdKBBMdqJAcMLJQAx5Yr6TwJ6+EPD9SeQz/u7Una+Cpr6GnmAqbR8t\nb8yYJDKVdo9sA+yJJYt/FuPGKztRIJjsRIFgshMFgslOFAgmO1EgmOxEgWDpLRAfeWKPeGL/XeV+\nzBfe8toiOySNdiydsSeVTCTt62ou5x4/mEjYbZLGqDeW3oiIyU4UCiY7USCY7ESBYLITBYLJThQI\nlt5o3pTXPEuleU9U9+p2JXiGvS1utmPN2XPNWDpjN2xuyZqxpmZ3u8ZGT52vDLyyEwWCyU4UCCY7\nUSCY7ESBYLITBaLk3XgRSQPYCWBx9P1Pq+pdInIBgCcAnAvgLQDfUlXfikBE3rnf7MWOgLFqd8R3\no9t3CfQMTmn0DITxzUHX2Oiea86amw4ACt6Fudxmc2U/DeBKVb0ExeWZN4vIZQB+DOA+VV0NYBTA\nrXPeOxHFpmSya9FE9OWi6J8CuBLA09H2RwFcV5MeElFVzHZ99oZoBdchAC8CeB/AmKrmo28ZANBR\nmy4SUTXMKtlVdVpV1wPoBLARwJrZ7kBEtonIHhHxTTNORDU2p7vxqjoG4JcAfhdAs4h8doOvE8Ax\no812Ve1R1Z6KekpEFSmZ7CKyXESao8fnALgKwAEUk/6G6NtuBvB8rTpJRJWbzUCYFQAeFZEGFN8c\nnlLVfxOR/QCeEJG/AfArAA/VsJ+0wCwxtn/qaXOyFh1Z7N7c4FnXqjFj9d5edgkAEkk7nQoFu1Q2\nMTHp3J7JuLcDQKbR3Q9Ve3a9ksmuqr0ALnVsP4Li53ciWgD4F3REgWCyEwWCyU4UCCY7USCY7ESB\nEN+t+qrvTORjAB9GX2YBDMe2cxv7cTb242wLrR+/qarLXYFYk/2sHYvsmQ9/Vcd+sB+h9IO/xhMF\ngslOFIh6Jvv2Ou57JvbjbOzH2b40/ajbZ3Yiihd/jScKRF2SXUQ2i8j/iMhhEbmjHn2I+tEvIu+I\nyL44J9cQkYdFZEhE+mZsaxGRF0Xkvej/ZXXqx90iciw6JvtE5JoY+nG+iPxSRPaLyLsi8mfR9liP\niacfsR4TEUmLyG4ReTvqx19F2y8QkTeivHlSRDxj9xxUNdZ/ABpQnNZqFYAUgLcBrI27H1Ff+gFk\n67DfywFsANA3Y9vfAbgjenwHgB/XqR93A/h+zMdjBYAN0eOlAA4BWBv3MfH0I9ZjguIkvJno8SIA\nbwC4DMBTALZE2/8RwJ/M5XnrcWXfCOCwqh7R4tTTTwC4tg79qBtV3Qlg5Aubr0Vx4k4gpgk8jX7E\nTlVPqOre6PEnKE6O0oGYj4mnH7HSoqpP8lqPZO8A8NGMr+s5WaUC+IWIvCUi2+rUh8+cp6onoseD\nAM6rY19uE5He6Nf8mn+cmElEulCcP+EN1PGYfKEfQMzHpBaTvIZ+g26Tqm4A8IcAvisil9e7Q0Dx\nnR3FN6J6eADAhSiuEXACwD1x7VhEMgCeAXC7qo7PjMV5TBz9iP2YaAWTvFrqkezHAJw/42tzsspa\nU9Vj0f9DAJ5DfWfeOSkiKwAg+n+oHp1Q1ZPRiVYA8CBiOiYisgjFBHtMVZ+NNsd+TFz9qNcxifY9\n50leLfVI9jcBXBTdWUwB2AJgR9ydEJElIrL0s8cArgbQ529VUztQnLgTqOMEnp8lV+R6xHBMRERQ\nnMPwgKreOyMU6zGx+hH3ManZJK9x3WH8wt3Ga1C80/k+gD+vUx9WoVgJeBvAu3H2A8DjKP46eAbF\nz163orhm3ssA3gPwEoCWOvXj5wDeAdCLYrKtiKEfm1D8Fb0XwL7o3zVxHxNPP2I9JgAuRnES114U\n31j+csY5uxvAYQD/AmDxXJ6Xf0FHFIjQb9ARBYPJThQIJjtRIJjsRIFgshMFgslOFAgmO1EgmOxE\ngfg/+Mxi6J4mGTIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgIT1giiqQU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images_labels_prediction(images,labels,prediction,idx,num=10):\n",
        "    fig=plt.gcf()\n",
        "    fig.set_size_inches(12,14)\n",
        "    if num>25: num=25\n",
        "    for i in range(0,num):\n",
        "        ax = plt.subplot(5,5,i+1)\n",
        "        ax.imshow(images[idx],cmap='binary') \n",
        "        title= str(i)+' '+label_dict[labels[i][0]]   #显示数字对应的类别\n",
        "        if len(prediction)>0:\n",
        "            title+= '=>'+label_dict[prediction[i]]   #显示数字对应的类别\n",
        "        ax.set_title(title,fontsize=10)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        idx+=1\n",
        "    plt.show()\n",
        "\n",
        "plot_images_labels_prediction(x_train,y_train,[],0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZNbvo9zqSZB",
        "colab_type": "text"
      },
      "source": [
        "# Pseudo Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYa-jKsACxRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pseudo_model = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "pseudo_model.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjVlrccQCxOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pseudo_train(iterations, batch_size, save_interval, alpha_f, t1, t2, iter_epochs):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # Get unlabeled examples and pseudo labels\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "        pseudo_label = pseudo_model.predict(imgs_unlabeled)\n",
        "\n",
        "        # -------------------------\n",
        "        #  Supervised Training\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs_labeled, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Train on labeled examples\n",
        "        alpha = 1\n",
        "        # loss_labeled, acc_labeled = pseudo_model.train_on_batch(imgs_labeled, labels)\n",
        "        datagen.fit(imgs_labeled)\n",
        "        pseudo_model.fit_generator(datagen.flow(imgs_labeled, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        loss_labeled, acc_labeled = history.losses[-1], history.accs[-1]\n",
        "\n",
        "\n",
        "        loss_unlabeled = -1\n",
        "        acc_unlabeled = -1\n",
        "\n",
        "        # -------------------------\n",
        "        #  Supervised Training\n",
        "        # -------------------------\n",
        "\n",
        "        # Set alpha\n",
        "        if iteration < t1: alpha = 0\n",
        "        else:\n",
        "            if t1 <= iteration < t2: alpha = (iteration - t1)/(t2 - t1) * alpha_f\n",
        "            else: alpha = alpha_f\n",
        "\n",
        "            # Train on unlabeled examples\n",
        "            loss_unlabeled, acc_unlabeled = pseudo_model.train_on_batch(imgs_unlabeled, pseudo_label)\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "          # Save losses\n",
        "          losses_pseudo_labeled.append(loss_labeled)\n",
        "          losses_pseudo_unlabeled.append(loss_unlabeled)\n",
        "          losses_pseudo.append(loss_labeled + alpha * loss_unlabeled)\n",
        "          accs_pseudo_labeled.append(acc_labeled)\n",
        "          accs_pseudo_unlabeled.append(acc_unlabeled)\n",
        "          accs_pseudo.append((acc_labeled + alpha*acc_unlabeled)/(1 + alpha))\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [supervised loss: %.4f, acc: %.2f%%] [unsupervised loss: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, loss_labeled, 100 * acc_labeled, \n",
        "                  loss_unlabeled, 100 * acc_unlabeled))\n",
        "          \n",
        "          pseudo_model.save(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\" + str(iteration+1) + \".h5\")\n",
        "          file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_supervised_losses.json\"\n",
        "          file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_unsupervised_losses.json\"\n",
        "          file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_losses.json\"\n",
        "          with open(file1, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo_labeled), json_file)\n",
        "          with open(file2, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo_unlabeled), json_file)\n",
        "          with open(file3, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GnU0n4JCxMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a4ee925-409e-4b18-89f5-d6774a761265"
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 30 # 8000\n",
        "batch_size = 32\n",
        "save_interval = 1 # 100\n",
        "alpha_f = 3\n",
        "t1 = 2 # 500\n",
        "t2 = 4 # 1000\n",
        "iter_epochs = 10\n",
        "\n",
        "losses_pseudo_labeled = []\n",
        "losses_pseudo_unlabeled = []\n",
        "losses_pseudo = []\n",
        "accs_pseudo_labeled = []\n",
        "accs_pseudo_unlabeled = []\n",
        "accs_pseudo = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "pseudo_model = load_model(\"./models/cifar10_model.037.h5\")\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the SGGAN for the specified number of iterations\n",
        "pseudo_train(iterations, batch_size, save_interval, alpha_f, t1, t2, iter_epochs)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 16s 16s/step - loss: 0.5329 - acc: 0.9062 - val_loss: 0.6234 - val_acc: 0.8644\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4477 - acc: 0.9375 - val_loss: 0.6191 - val_acc: 0.8639\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4160 - acc: 0.9375 - val_loss: 0.6162 - val_acc: 0.8641\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3380 - acc: 1.0000 - val_loss: 0.6172 - val_acc: 0.8630\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4056 - acc: 0.9688 - val_loss: 0.6138 - val_acc: 0.8637\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3128 - acc: 0.9688 - val_loss: 0.6136 - val_acc: 0.8640\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3171 - acc: 0.9688 - val_loss: 0.6129 - val_acc: 0.8652\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2698 - acc: 1.0000 - val_loss: 0.6155 - val_acc: 0.8647\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2508 - acc: 1.0000 - val_loss: 0.6194 - val_acc: 0.8641\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2471 - acc: 1.0000 - val_loss: 0.6233 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "1 [supervised loss: 0.2471, acc: 100.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4080 - acc: 0.9375 - val_loss: 0.6236 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4290 - acc: 0.9062 - val_loss: 0.6236 - val_acc: 0.8617\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4021 - acc: 0.9062 - val_loss: 0.6230 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4121 - acc: 0.9375 - val_loss: 0.6226 - val_acc: 0.8621\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3651 - acc: 0.9375 - val_loss: 0.6210 - val_acc: 0.8618\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3313 - acc: 0.9375 - val_loss: 0.6201 - val_acc: 0.8607\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2693 - acc: 1.0000 - val_loss: 0.6213 - val_acc: 0.8601\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3242 - acc: 0.9375 - val_loss: 0.6202 - val_acc: 0.8625\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2392 - acc: 1.0000 - val_loss: 0.6209 - val_acc: 0.8619\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2453 - acc: 1.0000 - val_loss: 0.6224 - val_acc: 0.8598\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "2 [supervised loss: 0.2453, acc: 100.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4114 - acc: 0.8750 - val_loss: 0.6218 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5106 - acc: 0.8750 - val_loss: 0.6208 - val_acc: 0.8598\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3308 - acc: 1.0000 - val_loss: 0.6210 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3697 - acc: 0.9688 - val_loss: 0.6222 - val_acc: 0.8598\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3891 - acc: 0.9375 - val_loss: 0.6238 - val_acc: 0.8586\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3354 - acc: 0.9688 - val_loss: 0.6258 - val_acc: 0.8579\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2985 - acc: 1.0000 - val_loss: 0.6279 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3067 - acc: 0.9688 - val_loss: 0.6311 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2912 - acc: 1.0000 - val_loss: 0.6357 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3250 - acc: 0.9688 - val_loss: 0.6425 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8486 - acc: 0.8438 - val_loss: 0.6467 - val_acc: 0.8535\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7827 - acc: 0.8750 - val_loss: 0.6490 - val_acc: 0.8530\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7162 - acc: 0.8438 - val_loss: 0.6503 - val_acc: 0.8530\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7395 - acc: 0.8750 - val_loss: 0.6510 - val_acc: 0.8521\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6950 - acc: 0.9375 - val_loss: 0.6525 - val_acc: 0.8534\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7053 - acc: 0.8750 - val_loss: 0.6554 - val_acc: 0.8519\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7219 - acc: 0.9062 - val_loss: 0.6625 - val_acc: 0.8499\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6667 - acc: 0.9062 - val_loss: 0.6715 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6532 - acc: 0.9062 - val_loss: 0.6784 - val_acc: 0.8441\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7091 - acc: 0.9375 - val_loss: 0.6830 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "3 [supervised loss: 0.7091, acc: 93.75%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5796 - acc: 0.8750 - val_loss: 0.6870 - val_acc: 0.8414\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6615 - acc: 0.8438 - val_loss: 0.6924 - val_acc: 0.8397\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4619 - acc: 0.8750 - val_loss: 0.6997 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4671 - acc: 0.9062 - val_loss: 0.7103 - val_acc: 0.8349\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3786 - acc: 0.9375 - val_loss: 0.7242 - val_acc: 0.8298\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2916 - acc: 1.0000 - val_loss: 0.7399 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 0.7590 - val_acc: 0.8191\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2722 - acc: 1.0000 - val_loss: 0.7764 - val_acc: 0.8147\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2291 - acc: 1.0000 - val_loss: 0.7942 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2379 - acc: 1.0000 - val_loss: 0.8126 - val_acc: 0.8058\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9441 - acc: 0.7812 - val_loss: 0.8351 - val_acc: 0.7984\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7931 - acc: 0.8438 - val_loss: 0.8644 - val_acc: 0.7910\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9055 - acc: 0.7500 - val_loss: 0.8879 - val_acc: 0.7859\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7520 - acc: 0.8438 - val_loss: 0.9119 - val_acc: 0.7813\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7501 - acc: 0.9062 - val_loss: 0.9407 - val_acc: 0.7747\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7460 - acc: 0.8438 - val_loss: 0.9712 - val_acc: 0.7696\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7601 - acc: 0.8438 - val_loss: 1.0032 - val_acc: 0.7642\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6721 - acc: 0.8750 - val_loss: 1.0352 - val_acc: 0.7588\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6721 - acc: 0.9688 - val_loss: 1.0624 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6355 - acc: 0.9375 - val_loss: 1.0856 - val_acc: 0.7505\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "4 [supervised loss: 0.6355, acc: 93.75%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5661 - acc: 0.8438 - val_loss: 1.1149 - val_acc: 0.7430\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6433 - acc: 0.8438 - val_loss: 1.1337 - val_acc: 0.7407\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4892 - acc: 0.8750 - val_loss: 1.1504 - val_acc: 0.7388\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6395 - acc: 0.8750 - val_loss: 1.1628 - val_acc: 0.7358\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3839 - acc: 0.9375 - val_loss: 1.1702 - val_acc: 0.7338\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3792 - acc: 0.9688 - val_loss: 1.1785 - val_acc: 0.7333\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3151 - acc: 0.9688 - val_loss: 1.1810 - val_acc: 0.7340\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2743 - acc: 1.0000 - val_loss: 1.1832 - val_acc: 0.7331\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3109 - acc: 0.9688 - val_loss: 1.1875 - val_acc: 0.7329\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3209 - acc: 1.0000 - val_loss: 1.1947 - val_acc: 0.7302\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3083 - acc: 0.7812 - val_loss: 1.2304 - val_acc: 0.7244\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3490 - acc: 0.7500 - val_loss: 1.3589 - val_acc: 0.7026\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3657 - acc: 0.7500 - val_loss: 1.5821 - val_acc: 0.6690\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8830 - acc: 0.8125 - val_loss: 2.5697 - val_acc: 0.5567\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7924 - acc: 0.8438 - val_loss: 3.3267 - val_acc: 0.4912\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8383 - acc: 0.9375 - val_loss: 4.2101 - val_acc: 0.4234\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8867 - acc: 0.8438 - val_loss: 5.0678 - val_acc: 0.3691\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7894 - acc: 0.9062 - val_loss: 5.8729 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7355 - acc: 0.8750 - val_loss: 6.5407 - val_acc: 0.2930\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "5 [supervised loss: 0.7355, acc: 87.50%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2086 - acc: 0.6562 - val_loss: 6.3492 - val_acc: 0.2967\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9884 - acc: 0.7500 - val_loss: 5.6695 - val_acc: 0.3215\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6970 - acc: 0.7812 - val_loss: 4.6924 - val_acc: 0.3623\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3970 - acc: 0.9375 - val_loss: 3.7160 - val_acc: 0.4170\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3349 - acc: 0.9688 - val_loss: 2.9681 - val_acc: 0.4752\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2788 - acc: 1.0000 - val_loss: 2.4570 - val_acc: 0.5354\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2572 - acc: 1.0000 - val_loss: 2.1194 - val_acc: 0.5808\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2595 - acc: 1.0000 - val_loss: 1.8950 - val_acc: 0.6147\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2442 - acc: 1.0000 - val_loss: 1.7508 - val_acc: 0.6375\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2223 - acc: 1.0000 - val_loss: 1.6637 - val_acc: 0.6499\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 4.1757 - acc: 0.4062 - val_loss: 1.8190 - val_acc: 0.6260\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.7320 - acc: 0.5625 - val_loss: 2.1697 - val_acc: 0.5744\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.2599 - acc: 0.4688 - val_loss: 2.8153 - val_acc: 0.5027\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7457 - acc: 0.5625 - val_loss: 3.8321 - val_acc: 0.4252\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0643 - acc: 0.6250 - val_loss: 5.0163 - val_acc: 0.3615\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5710 - acc: 0.6562 - val_loss: 6.1737 - val_acc: 0.3109\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3284 - acc: 0.7500 - val_loss: 7.1283 - val_acc: 0.2673\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9538 - acc: 0.8438 - val_loss: 7.9276 - val_acc: 0.2335\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8568 - acc: 0.8750 - val_loss: 8.5462 - val_acc: 0.2145\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7131 - acc: 0.9062 - val_loss: 8.9734 - val_acc: 0.2050\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "6 [supervised loss: 0.7131, acc: 90.62%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.4256 - acc: 0.3125 - val_loss: 8.9663 - val_acc: 0.2056\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.3764 - acc: 0.4688 - val_loss: 8.6321 - val_acc: 0.2107\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.8358 - acc: 0.3750 - val_loss: 7.8809 - val_acc: 0.2317\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4315 - acc: 0.6562 - val_loss: 6.8390 - val_acc: 0.2763\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2350 - acc: 0.6875 - val_loss: 5.5147 - val_acc: 0.3485\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6477 - acc: 0.7812 - val_loss: 4.6237 - val_acc: 0.4088\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5162 - acc: 0.8750 - val_loss: 4.2274 - val_acc: 0.4403\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5389 - acc: 0.9062 - val_loss: 4.1584 - val_acc: 0.4475\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4399 - acc: 0.9062 - val_loss: 4.2671 - val_acc: 0.4449\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3856 - acc: 0.9375 - val_loss: 4.4511 - val_acc: 0.4308\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.8140 - acc: 0.3125 - val_loss: 4.7460 - val_acc: 0.4089\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.4743 - acc: 0.3438 - val_loss: 5.0331 - val_acc: 0.3889\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.0924 - acc: 0.4062 - val_loss: 5.3678 - val_acc: 0.3622\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7166 - acc: 0.4062 - val_loss: 5.6240 - val_acc: 0.3300\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7311 - acc: 0.5312 - val_loss: 5.9241 - val_acc: 0.2867\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3285 - acc: 0.6875 - val_loss: 6.4465 - val_acc: 0.2571\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7295 - acc: 0.8125 - val_loss: 6.9456 - val_acc: 0.2459\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4595 - acc: 0.9688 - val_loss: 7.3005 - val_acc: 0.2405\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4961 - acc: 0.9688 - val_loss: 7.5937 - val_acc: 0.2362\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4297 - acc: 0.9688 - val_loss: 7.8440 - val_acc: 0.2312\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "7 [supervised loss: 0.4297, acc: 96.88%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 4.0132 - acc: 0.1875 - val_loss: 7.7194 - val_acc: 0.2333\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 4.0833 - acc: 0.2500 - val_loss: 7.3105 - val_acc: 0.2424\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.6103 - acc: 0.2188 - val_loss: 6.9021 - val_acc: 0.2502\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2830 - acc: 0.4375 - val_loss: 6.5685 - val_acc: 0.2638\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2338 - acc: 0.4375 - val_loss: 6.3131 - val_acc: 0.2757\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7114 - acc: 0.5000 - val_loss: 6.2017 - val_acc: 0.2816\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8621 - acc: 0.7500 - val_loss: 6.1342 - val_acc: 0.2899\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8393 - acc: 0.7188 - val_loss: 6.1072 - val_acc: 0.2972\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6608 - acc: 0.8125 - val_loss: 6.0976 - val_acc: 0.3027\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5349 - acc: 0.9062 - val_loss: 6.1429 - val_acc: 0.3033\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.2990 - acc: 0.3125 - val_loss: 6.2811 - val_acc: 0.2973\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.1448 - acc: 0.2812 - val_loss: 6.5950 - val_acc: 0.2790\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.3232 - acc: 0.3750 - val_loss: 7.1789 - val_acc: 0.2519\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7590 - acc: 0.4062 - val_loss: 7.9969 - val_acc: 0.2270\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5001 - acc: 0.5000 - val_loss: 8.8343 - val_acc: 0.2082\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7777 - acc: 0.5312 - val_loss: 9.4116 - val_acc: 0.1979\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7444 - acc: 0.6250 - val_loss: 9.8858 - val_acc: 0.1951\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5088 - acc: 0.6562 - val_loss: 10.2585 - val_acc: 0.1889\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2737 - acc: 0.6875 - val_loss: 10.5242 - val_acc: 0.1870\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2130 - acc: 0.6875 - val_loss: 10.6667 - val_acc: 0.1846\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "8 [supervised loss: 1.2130, acc: 68.75%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3568 - acc: 0.4062 - val_loss: 10.6579 - val_acc: 0.1868\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.4592 - acc: 0.4375 - val_loss: 10.3912 - val_acc: 0.1905\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1763 - acc: 0.5625 - val_loss: 10.1880 - val_acc: 0.1939\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7441 - acc: 0.5625 - val_loss: 9.9578 - val_acc: 0.1941\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6904 - acc: 0.5625 - val_loss: 9.7421 - val_acc: 0.1904\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4544 - acc: 0.5625 - val_loss: 9.4727 - val_acc: 0.1923\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9527 - acc: 0.6250 - val_loss: 9.2782 - val_acc: 0.1919\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0897 - acc: 0.6875 - val_loss: 9.1889 - val_acc: 0.1887\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0048 - acc: 0.7188 - val_loss: 9.1171 - val_acc: 0.1864\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6750 - acc: 0.8438 - val_loss: 9.0402 - val_acc: 0.1809\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5948 - acc: 0.3750 - val_loss: 9.0439 - val_acc: 0.1769\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6285 - acc: 0.3125 - val_loss: 9.0462 - val_acc: 0.1734\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1510 - acc: 0.3438 - val_loss: 8.9002 - val_acc: 0.1664\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0182 - acc: 0.5312 - val_loss: 8.8424 - val_acc: 0.1592\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4272 - acc: 0.6250 - val_loss: 8.9163 - val_acc: 0.1514\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5940 - acc: 0.7188 - val_loss: 9.2133 - val_acc: 0.1488\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1013 - acc: 0.7812 - val_loss: 9.4785 - val_acc: 0.1521\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8944 - acc: 0.8125 - val_loss: 9.7631 - val_acc: 0.1607\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7870 - acc: 0.8750 - val_loss: 10.0627 - val_acc: 0.1604\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7711 - acc: 0.9062 - val_loss: 10.3504 - val_acc: 0.1560\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "9 [supervised loss: 0.7711, acc: 90.62%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7365 - acc: 0.5625 - val_loss: 10.4971 - val_acc: 0.1554\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7751 - acc: 0.5625 - val_loss: 10.5927 - val_acc: 0.1532\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6304 - acc: 0.4375 - val_loss: 10.5602 - val_acc: 0.1510\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4455 - acc: 0.5625 - val_loss: 10.4081 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1547 - acc: 0.6250 - val_loss: 10.1670 - val_acc: 0.1555\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1436 - acc: 0.6562 - val_loss: 9.8815 - val_acc: 0.1639\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0625 - acc: 0.6562 - val_loss: 9.5628 - val_acc: 0.1739\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8073 - acc: 0.7500 - val_loss: 9.2444 - val_acc: 0.1836\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7250 - acc: 0.8438 - val_loss: 8.9183 - val_acc: 0.1949\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6095 - acc: 0.8750 - val_loss: 8.6330 - val_acc: 0.2028\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.6106 - acc: 0.2188 - val_loss: 8.3497 - val_acc: 0.2106\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.1978 - acc: 0.2812 - val_loss: 8.1921 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.4704 - acc: 0.3125 - val_loss: 8.0778 - val_acc: 0.2238\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7764 - acc: 0.4375 - val_loss: 8.0868 - val_acc: 0.2250\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5094 - acc: 0.6250 - val_loss: 8.2310 - val_acc: 0.2191\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3307 - acc: 0.4062 - val_loss: 8.4923 - val_acc: 0.2033\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1441 - acc: 0.5938 - val_loss: 8.8864 - val_acc: 0.1854\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9856 - acc: 0.5938 - val_loss: 9.2632 - val_acc: 0.1667\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8208 - acc: 0.7812 - val_loss: 9.6026 - val_acc: 0.1526\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7319 - acc: 0.6875 - val_loss: 9.9408 - val_acc: 0.1435\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "10 [supervised loss: 1.7319, acc: 68.75%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2920 - acc: 0.3125 - val_loss: 10.2194 - val_acc: 0.1404\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0768 - acc: 0.4062 - val_loss: 10.4347 - val_acc: 0.1421\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2345 - acc: 0.3750 - val_loss: 10.4941 - val_acc: 0.1446\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1436 - acc: 0.4062 - val_loss: 10.4737 - val_acc: 0.1479\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7165 - acc: 0.4375 - val_loss: 10.4193 - val_acc: 0.1529\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6563 - acc: 0.4062 - val_loss: 10.1995 - val_acc: 0.1570\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2471 - acc: 0.5312 - val_loss: 10.0098 - val_acc: 0.1582\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0839 - acc: 0.6875 - val_loss: 9.7971 - val_acc: 0.1610\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8926 - acc: 0.7500 - val_loss: 9.5966 - val_acc: 0.1651\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9343 - acc: 0.7500 - val_loss: 9.3899 - val_acc: 0.1696\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.0090 - acc: 0.3125 - val_loss: 9.3163 - val_acc: 0.1732\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.1803 - acc: 0.2500 - val_loss: 9.2557 - val_acc: 0.1772\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5733 - acc: 0.3438 - val_loss: 9.2076 - val_acc: 0.1810\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3732 - acc: 0.4062 - val_loss: 9.2038 - val_acc: 0.1877\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2731 - acc: 0.4375 - val_loss: 9.2125 - val_acc: 0.1885\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0159 - acc: 0.3438 - val_loss: 9.3521 - val_acc: 0.1838\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6927 - acc: 0.5312 - val_loss: 9.5597 - val_acc: 0.1714\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3386 - acc: 0.5625 - val_loss: 9.8992 - val_acc: 0.1531\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2121 - acc: 0.6875 - val_loss: 10.3795 - val_acc: 0.1317\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1200 - acc: 0.6875 - val_loss: 10.8675 - val_acc: 0.1164\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "11 [supervised loss: 1.1200, acc: 68.75%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.6286 - acc: 0.2500 - val_loss: 11.1455 - val_acc: 0.1092\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.3236 - acc: 0.2188 - val_loss: 11.2658 - val_acc: 0.1108\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.0440 - acc: 0.1875 - val_loss: 11.2230 - val_acc: 0.1123\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.9227 - acc: 0.1875 - val_loss: 11.2112 - val_acc: 0.1180\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3497 - acc: 0.3125 - val_loss: 11.3262 - val_acc: 0.1251\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9961 - acc: 0.5000 - val_loss: 11.5582 - val_acc: 0.1288\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9908 - acc: 0.4688 - val_loss: 11.8824 - val_acc: 0.1275\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6503 - acc: 0.6250 - val_loss: 12.1107 - val_acc: 0.1260\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6137 - acc: 0.6250 - val_loss: 12.3166 - val_acc: 0.1246\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3855 - acc: 0.8125 - val_loss: 12.3927 - val_acc: 0.1248\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6774 - acc: 0.1875 - val_loss: 12.3255 - val_acc: 0.1261\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6088 - acc: 0.3125 - val_loss: 12.2080 - val_acc: 0.1362\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6492 - acc: 0.2188 - val_loss: 12.5963 - val_acc: 0.1288\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2288 - acc: 0.2188 - val_loss: 13.2875 - val_acc: 0.1177\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2033 - acc: 0.3438 - val_loss: 13.7539 - val_acc: 0.1107\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9701 - acc: 0.3438 - val_loss: 14.0568 - val_acc: 0.1062\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9288 - acc: 0.5000 - val_loss: 14.2116 - val_acc: 0.1040\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7080 - acc: 0.5625 - val_loss: 14.2794 - val_acc: 0.1034\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6620 - acc: 0.5312 - val_loss: 14.2840 - val_acc: 0.1027\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5309 - acc: 0.6562 - val_loss: 14.2777 - val_acc: 0.1030\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "12 [supervised loss: 1.5309, acc: 65.62%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.0427 - acc: 0.2500 - val_loss: 14.2181 - val_acc: 0.1030\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7921 - acc: 0.3125 - val_loss: 14.0985 - val_acc: 0.1032\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5553 - acc: 0.2812 - val_loss: 13.8532 - val_acc: 0.1047\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.4769 - acc: 0.1875 - val_loss: 13.3847 - val_acc: 0.1071\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8948 - acc: 0.3438 - val_loss: 12.5575 - val_acc: 0.1088\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7257 - acc: 0.3438 - val_loss: 10.7699 - val_acc: 0.1145\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3593 - acc: 0.5625 - val_loss: 8.5989 - val_acc: 0.1535\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2479 - acc: 0.6250 - val_loss: 7.5572 - val_acc: 0.2015\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0554 - acc: 0.7812 - val_loss: 6.9997 - val_acc: 0.2118\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0347 - acc: 0.8125 - val_loss: 6.3750 - val_acc: 0.2216\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 4.3142 - acc: 0.0000e+00 - val_loss: 5.6938 - val_acc: 0.2390\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 4.1791 - acc: 0.0312 - val_loss: 5.0449 - val_acc: 0.2549\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.9402 - acc: 0.0938 - val_loss: 4.5445 - val_acc: 0.2709\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.5627 - acc: 0.1875 - val_loss: 4.2933 - val_acc: 0.2796\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.8620 - acc: 0.2812 - val_loss: 4.3147 - val_acc: 0.2784\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.4668 - acc: 0.2812 - val_loss: 4.6507 - val_acc: 0.2595\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9615 - acc: 0.3750 - val_loss: 5.3275 - val_acc: 0.2255\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6017 - acc: 0.4688 - val_loss: 6.2089 - val_acc: 0.1969\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1829 - acc: 0.7188 - val_loss: 7.1557 - val_acc: 0.1684\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0018 - acc: 0.8125 - val_loss: 8.0182 - val_acc: 0.1459\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "13 [supervised loss: 1.0018, acc: 81.25%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.2392 - acc: 0.2188 - val_loss: 8.3373 - val_acc: 0.1347\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.3812 - acc: 0.1875 - val_loss: 8.1851 - val_acc: 0.1297\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.2422 - acc: 0.2188 - val_loss: 7.7293 - val_acc: 0.1301\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.3056 - acc: 0.2188 - val_loss: 7.0247 - val_acc: 0.1340\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.8462 - acc: 0.2188 - val_loss: 6.2157 - val_acc: 0.1454\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6273 - acc: 0.1875 - val_loss: 5.4488 - val_acc: 0.1701\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3352 - acc: 0.1875 - val_loss: 4.8483 - val_acc: 0.2004\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1967 - acc: 0.2812 - val_loss: 4.4151 - val_acc: 0.2215\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6624 - acc: 0.4688 - val_loss: 4.1168 - val_acc: 0.2365\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3657 - acc: 0.5000 - val_loss: 3.9131 - val_acc: 0.2388\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3144 - acc: 0.7500 - val_loss: 3.8139 - val_acc: 0.2309\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5886 - acc: 0.6875 - val_loss: 3.7756 - val_acc: 0.2224\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6390 - acc: 0.6250 - val_loss: 3.7457 - val_acc: 0.2176\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4381 - acc: 0.6875 - val_loss: 3.7284 - val_acc: 0.2133\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6061 - acc: 0.6562 - val_loss: 3.7175 - val_acc: 0.2119\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4862 - acc: 0.6562 - val_loss: 3.7011 - val_acc: 0.2135\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3898 - acc: 0.6875 - val_loss: 3.7003 - val_acc: 0.2138\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3494 - acc: 0.7812 - val_loss: 3.6966 - val_acc: 0.2152\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2847 - acc: 0.7500 - val_loss: 3.6813 - val_acc: 0.2192\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2274 - acc: 0.8125 - val_loss: 3.6608 - val_acc: 0.2234\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "14 [supervised loss: 1.2274, acc: 81.25%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7005 - acc: 0.3125 - val_loss: 3.5340 - val_acc: 0.2326\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7283 - acc: 0.2812 - val_loss: 3.3376 - val_acc: 0.2441\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6892 - acc: 0.2812 - val_loss: 3.1193 - val_acc: 0.2614\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2794 - acc: 0.2812 - val_loss: 2.9113 - val_acc: 0.2802\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9181 - acc: 0.3750 - val_loss: 2.7300 - val_acc: 0.3003\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6211 - acc: 0.4375 - val_loss: 2.5959 - val_acc: 0.3126\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5065 - acc: 0.5000 - val_loss: 2.5181 - val_acc: 0.3190\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2821 - acc: 0.5000 - val_loss: 2.4797 - val_acc: 0.3194\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1532 - acc: 0.5938 - val_loss: 2.4858 - val_acc: 0.3160\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8961 - acc: 0.7812 - val_loss: 2.5295 - val_acc: 0.3084\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.0549 - acc: 0.2812 - val_loss: 2.6222 - val_acc: 0.2959\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.9531 - acc: 0.2812 - val_loss: 2.7468 - val_acc: 0.2803\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.9200 - acc: 0.2188 - val_loss: 2.8820 - val_acc: 0.2655\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.9669 - acc: 0.2500 - val_loss: 3.0504 - val_acc: 0.2477\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.7846 - acc: 0.2500 - val_loss: 3.2351 - val_acc: 0.2312\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.6439 - acc: 0.3750 - val_loss: 3.4425 - val_acc: 0.2135\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2986 - acc: 0.4062 - val_loss: 3.6795 - val_acc: 0.1963\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1953 - acc: 0.5000 - val_loss: 3.9098 - val_acc: 0.1761\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1249 - acc: 0.4688 - val_loss: 4.1743 - val_acc: 0.1552\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9636 - acc: 0.5625 - val_loss: 4.4302 - val_acc: 0.1418\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "15 [supervised loss: 1.9636, acc: 56.25%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6326 - acc: 0.5000 - val_loss: 4.5944 - val_acc: 0.1338\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5931 - acc: 0.5625 - val_loss: 4.6971 - val_acc: 0.1282\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7425 - acc: 0.5625 - val_loss: 4.7210 - val_acc: 0.1247\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4882 - acc: 0.5938 - val_loss: 4.6749 - val_acc: 0.1237\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4055 - acc: 0.5312 - val_loss: 4.5875 - val_acc: 0.1240\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3958 - acc: 0.5625 - val_loss: 4.4474 - val_acc: 0.1269\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0874 - acc: 0.7500 - val_loss: 4.2916 - val_acc: 0.1317\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0804 - acc: 0.6562 - val_loss: 4.1855 - val_acc: 0.1384\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9743 - acc: 0.7188 - val_loss: 4.0844 - val_acc: 0.1453\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8550 - acc: 0.8438 - val_loss: 4.0091 - val_acc: 0.1536\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7317 - acc: 0.4688 - val_loss: 4.0564 - val_acc: 0.1578\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8400 - acc: 0.3750 - val_loss: 4.1718 - val_acc: 0.1590\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7777 - acc: 0.4688 - val_loss: 4.3685 - val_acc: 0.1571\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6667 - acc: 0.5625 - val_loss: 4.6079 - val_acc: 0.1527\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5670 - acc: 0.5312 - val_loss: 4.8884 - val_acc: 0.1446\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4256 - acc: 0.6875 - val_loss: 5.1515 - val_acc: 0.1379\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3500 - acc: 0.6875 - val_loss: 5.4455 - val_acc: 0.1306\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2988 - acc: 0.8125 - val_loss: 5.7327 - val_acc: 0.1246\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2268 - acc: 0.8438 - val_loss: 6.0256 - val_acc: 0.1182\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0991 - acc: 0.9688 - val_loss: 6.3050 - val_acc: 0.1151\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "16 [supervised loss: 1.0991, acc: 96.88%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.4752 - acc: 0.3125 - val_loss: 6.3260 - val_acc: 0.1151\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5468 - acc: 0.2812 - val_loss: 6.1465 - val_acc: 0.1166\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.5730 - acc: 0.3125 - val_loss: 5.8656 - val_acc: 0.1198\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0820 - acc: 0.3750 - val_loss: 5.5144 - val_acc: 0.1262\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2224 - acc: 0.3750 - val_loss: 5.0009 - val_acc: 0.1384\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6909 - acc: 0.4062 - val_loss: 4.3992 - val_acc: 0.1583\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5250 - acc: 0.4688 - val_loss: 3.8402 - val_acc: 0.1850\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3935 - acc: 0.4688 - val_loss: 3.3214 - val_acc: 0.2205\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2027 - acc: 0.6875 - val_loss: 2.8889 - val_acc: 0.2586\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8660 - acc: 0.8438 - val_loss: 2.5510 - val_acc: 0.2974\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7754 - acc: 0.4688 - val_loss: 2.3462 - val_acc: 0.3255\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7443 - acc: 0.3750 - val_loss: 2.2231 - val_acc: 0.3407\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5930 - acc: 0.4688 - val_loss: 2.1565 - val_acc: 0.3475\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6240 - acc: 0.4688 - val_loss: 2.1357 - val_acc: 0.3455\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5557 - acc: 0.5000 - val_loss: 2.1611 - val_acc: 0.3324\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4529 - acc: 0.5000 - val_loss: 2.2166 - val_acc: 0.3111\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3007 - acc: 0.6562 - val_loss: 2.2944 - val_acc: 0.2881\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1381 - acc: 0.7500 - val_loss: 2.3901 - val_acc: 0.2653\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0820 - acc: 0.8750 - val_loss: 2.4980 - val_acc: 0.2434\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0189 - acc: 0.8750 - val_loss: 2.6095 - val_acc: 0.2224\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "17 [supervised loss: 1.0189, acc: 87.50%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3546 - acc: 0.3125 - val_loss: 2.6777 - val_acc: 0.2120\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1813 - acc: 0.2500 - val_loss: 2.6858 - val_acc: 0.2115\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3514 - acc: 0.2812 - val_loss: 2.6672 - val_acc: 0.2160\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1233 - acc: 0.3125 - val_loss: 2.6178 - val_acc: 0.2262\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0085 - acc: 0.3125 - val_loss: 2.5601 - val_acc: 0.2387\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8022 - acc: 0.3750 - val_loss: 2.4890 - val_acc: 0.2567\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6637 - acc: 0.3750 - val_loss: 2.4183 - val_acc: 0.2801\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3898 - acc: 0.5938 - val_loss: 2.3346 - val_acc: 0.3070\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4523 - acc: 0.5312 - val_loss: 2.2390 - val_acc: 0.3420\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3425 - acc: 0.5938 - val_loss: 2.1332 - val_acc: 0.3767\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6359 - acc: 0.6875 - val_loss: 2.0617 - val_acc: 0.4054\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7328 - acc: 0.6250 - val_loss: 2.0103 - val_acc: 0.4252\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7827 - acc: 0.5312 - val_loss: 1.9782 - val_acc: 0.4359\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7798 - acc: 0.5938 - val_loss: 1.9587 - val_acc: 0.4426\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7195 - acc: 0.5938 - val_loss: 1.9543 - val_acc: 0.4422\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6295 - acc: 0.5938 - val_loss: 1.9544 - val_acc: 0.4394\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6837 - acc: 0.5938 - val_loss: 1.9577 - val_acc: 0.4332\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6073 - acc: 0.6562 - val_loss: 1.9575 - val_acc: 0.4308\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5843 - acc: 0.7188 - val_loss: 1.9626 - val_acc: 0.4268\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5169 - acc: 0.7500 - val_loss: 1.9723 - val_acc: 0.4205\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "18 [supervised loss: 1.5169, acc: 75.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3768 - acc: 0.5312 - val_loss: 1.9585 - val_acc: 0.4203\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3843 - acc: 0.5312 - val_loss: 1.9159 - val_acc: 0.4306\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3700 - acc: 0.5625 - val_loss: 1.8647 - val_acc: 0.4455\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2600 - acc: 0.5938 - val_loss: 1.8187 - val_acc: 0.4605\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1820 - acc: 0.6250 - val_loss: 1.7792 - val_acc: 0.4780\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9997 - acc: 0.6562 - val_loss: 1.7452 - val_acc: 0.4971\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0151 - acc: 0.7188 - val_loss: 1.7235 - val_acc: 0.5117\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9015 - acc: 0.7500 - val_loss: 1.7104 - val_acc: 0.5219\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8543 - acc: 0.7812 - val_loss: 1.7009 - val_acc: 0.5290\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6607 - acc: 0.9062 - val_loss: 1.6914 - val_acc: 0.5325\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1048 - acc: 0.7500 - val_loss: 1.6764 - val_acc: 0.5388\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1712 - acc: 0.5938 - val_loss: 1.6579 - val_acc: 0.5463\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1024 - acc: 0.6562 - val_loss: 1.6399 - val_acc: 0.5518\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1170 - acc: 0.5312 - val_loss: 1.6233 - val_acc: 0.5579\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0444 - acc: 0.6250 - val_loss: 1.6149 - val_acc: 0.5626\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9291 - acc: 0.6562 - val_loss: 1.6162 - val_acc: 0.5611\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8735 - acc: 0.6875 - val_loss: 1.6325 - val_acc: 0.5581\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7892 - acc: 0.6562 - val_loss: 1.6674 - val_acc: 0.5501\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7999 - acc: 0.7188 - val_loss: 1.7187 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7926 - acc: 0.6875 - val_loss: 1.7855 - val_acc: 0.5299\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "19 [supervised loss: 1.7926, acc: 68.75%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5206 - acc: 0.5938 - val_loss: 1.8580 - val_acc: 0.5214\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3306 - acc: 0.6562 - val_loss: 1.9347 - val_acc: 0.5161\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3645 - acc: 0.5625 - val_loss: 2.0019 - val_acc: 0.5131\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2821 - acc: 0.6562 - val_loss: 2.0681 - val_acc: 0.5113\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2018 - acc: 0.7188 - val_loss: 2.1262 - val_acc: 0.5092\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2739 - acc: 0.7188 - val_loss: 2.1689 - val_acc: 0.5077\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0182 - acc: 0.7500 - val_loss: 2.1949 - val_acc: 0.5075\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9668 - acc: 0.7812 - val_loss: 2.2059 - val_acc: 0.5068\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8687 - acc: 0.8438 - val_loss: 2.1921 - val_acc: 0.5089\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8518 - acc: 0.8438 - val_loss: 2.1470 - val_acc: 0.5131\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8651 - acc: 0.6250 - val_loss: 2.1217 - val_acc: 0.5146\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7758 - acc: 0.5938 - val_loss: 2.1075 - val_acc: 0.5139\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9135 - acc: 0.5938 - val_loss: 2.1196 - val_acc: 0.5110\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8278 - acc: 0.5938 - val_loss: 2.1408 - val_acc: 0.5069\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6910 - acc: 0.5938 - val_loss: 2.1647 - val_acc: 0.5048\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7382 - acc: 0.5938 - val_loss: 2.1899 - val_acc: 0.4981\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7131 - acc: 0.6250 - val_loss: 2.2472 - val_acc: 0.4891\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6867 - acc: 0.5312 - val_loss: 2.3179 - val_acc: 0.4781\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7196 - acc: 0.6250 - val_loss: 2.3955 - val_acc: 0.4655\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6413 - acc: 0.6250 - val_loss: 2.4874 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "20 [supervised loss: 1.6413, acc: 62.50%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2959 - acc: 0.6562 - val_loss: 2.5079 - val_acc: 0.4477\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2700 - acc: 0.6250 - val_loss: 2.4904 - val_acc: 0.4449\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3012 - acc: 0.6875 - val_loss: 2.4326 - val_acc: 0.4463\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1274 - acc: 0.7188 - val_loss: 2.3535 - val_acc: 0.4519\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9658 - acc: 0.8438 - val_loss: 2.2654 - val_acc: 0.4601\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0120 - acc: 0.8125 - val_loss: 2.1925 - val_acc: 0.4666\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9194 - acc: 0.8125 - val_loss: 2.1334 - val_acc: 0.4729\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8943 - acc: 0.8125 - val_loss: 2.0974 - val_acc: 0.4793\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8075 - acc: 0.8750 - val_loss: 2.0797 - val_acc: 0.4842\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7791 - acc: 0.8125 - val_loss: 2.0820 - val_acc: 0.4899\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6665 - acc: 0.6875 - val_loss: 2.1124 - val_acc: 0.4883\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7318 - acc: 0.5938 - val_loss: 2.1523 - val_acc: 0.4863\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6358 - acc: 0.6562 - val_loss: 2.1999 - val_acc: 0.4802\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5907 - acc: 0.6562 - val_loss: 2.2426 - val_acc: 0.4743\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5899 - acc: 0.6250 - val_loss: 2.2884 - val_acc: 0.4684\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6210 - acc: 0.7188 - val_loss: 2.3438 - val_acc: 0.4625\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4605 - acc: 0.7812 - val_loss: 2.4001 - val_acc: 0.4555\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5896 - acc: 0.7500 - val_loss: 2.4698 - val_acc: 0.4462\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5049 - acc: 0.7188 - val_loss: 2.5553 - val_acc: 0.4367\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5006 - acc: 0.6562 - val_loss: 2.6568 - val_acc: 0.4266\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "21 [supervised loss: 1.5006, acc: 65.62%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1961 - acc: 0.7500 - val_loss: 2.7118 - val_acc: 0.4200\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1655 - acc: 0.7500 - val_loss: 2.7102 - val_acc: 0.4197\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0841 - acc: 0.7812 - val_loss: 2.6718 - val_acc: 0.4224\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9625 - acc: 0.8750 - val_loss: 2.6014 - val_acc: 0.4282\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0480 - acc: 0.8438 - val_loss: 2.5030 - val_acc: 0.4384\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7985 - acc: 0.9062 - val_loss: 2.4006 - val_acc: 0.4467\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8427 - acc: 0.8438 - val_loss: 2.2958 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6933 - acc: 0.9062 - val_loss: 2.1987 - val_acc: 0.4633\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7345 - acc: 0.8750 - val_loss: 2.1043 - val_acc: 0.4731\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7159 - acc: 0.9062 - val_loss: 2.0201 - val_acc: 0.4809\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5098 - acc: 0.6875 - val_loss: 1.9653 - val_acc: 0.4890\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6876 - acc: 0.6562 - val_loss: 1.9318 - val_acc: 0.4904\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6043 - acc: 0.5938 - val_loss: 1.9187 - val_acc: 0.4889\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5611 - acc: 0.6250 - val_loss: 1.9244 - val_acc: 0.4827\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4991 - acc: 0.6875 - val_loss: 1.9617 - val_acc: 0.4686\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5379 - acc: 0.6875 - val_loss: 2.0139 - val_acc: 0.4529\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3837 - acc: 0.6875 - val_loss: 2.0850 - val_acc: 0.4362\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3161 - acc: 0.7188 - val_loss: 2.1668 - val_acc: 0.4176\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4315 - acc: 0.7500 - val_loss: 2.2427 - val_acc: 0.4030\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3084 - acc: 0.7188 - val_loss: 2.3202 - val_acc: 0.3896\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "22 [supervised loss: 1.3084, acc: 71.88%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1209 - acc: 0.6562 - val_loss: 2.3646 - val_acc: 0.3836\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9151 - acc: 0.7812 - val_loss: 2.3883 - val_acc: 0.3824\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9275 - acc: 0.7500 - val_loss: 2.3914 - val_acc: 0.3836\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8910 - acc: 0.7812 - val_loss: 2.3719 - val_acc: 0.3883\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8303 - acc: 0.8438 - val_loss: 2.3303 - val_acc: 0.3956\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7205 - acc: 0.9062 - val_loss: 2.2820 - val_acc: 0.4046\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6481 - acc: 0.9375 - val_loss: 2.2377 - val_acc: 0.4130\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6170 - acc: 0.8750 - val_loss: 2.1985 - val_acc: 0.4230\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5592 - acc: 0.9375 - val_loss: 2.1626 - val_acc: 0.4321\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4789 - acc: 0.9688 - val_loss: 2.1307 - val_acc: 0.4412\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5456 - acc: 0.6562 - val_loss: 2.1098 - val_acc: 0.4501\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4692 - acc: 0.5625 - val_loss: 2.1096 - val_acc: 0.4531\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6060 - acc: 0.5938 - val_loss: 2.1275 - val_acc: 0.4557\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5559 - acc: 0.6562 - val_loss: 2.1509 - val_acc: 0.4578\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5849 - acc: 0.5625 - val_loss: 2.1865 - val_acc: 0.4587\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5680 - acc: 0.5625 - val_loss: 2.2352 - val_acc: 0.4555\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5028 - acc: 0.6562 - val_loss: 2.2925 - val_acc: 0.4530\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4133 - acc: 0.6562 - val_loss: 2.3579 - val_acc: 0.4484\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4830 - acc: 0.6875 - val_loss: 2.4244 - val_acc: 0.4431\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3357 - acc: 0.8125 - val_loss: 2.4986 - val_acc: 0.4366\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "23 [supervised loss: 1.3357, acc: 81.25%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3368 - acc: 0.5312 - val_loss: 2.4919 - val_acc: 0.4386\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4275 - acc: 0.5625 - val_loss: 2.4103 - val_acc: 0.4516\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3428 - acc: 0.6562 - val_loss: 2.2718 - val_acc: 0.4742\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3367 - acc: 0.5625 - val_loss: 2.1150 - val_acc: 0.5002\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0484 - acc: 0.7500 - val_loss: 1.9636 - val_acc: 0.5249\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1397 - acc: 0.6875 - val_loss: 1.8303 - val_acc: 0.5491\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9213 - acc: 0.7500 - val_loss: 1.7173 - val_acc: 0.5727\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8668 - acc: 0.8750 - val_loss: 1.6250 - val_acc: 0.5909\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6681 - acc: 0.9375 - val_loss: 1.5434 - val_acc: 0.6092\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6212 - acc: 0.9688 - val_loss: 1.4699 - val_acc: 0.6306\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8653 - acc: 0.5625 - val_loss: 1.4285 - val_acc: 0.6396\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8641 - acc: 0.5000 - val_loss: 1.4131 - val_acc: 0.6432\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8901 - acc: 0.5938 - val_loss: 1.4214 - val_acc: 0.6410\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7648 - acc: 0.5312 - val_loss: 1.4595 - val_acc: 0.6320\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5612 - acc: 0.6562 - val_loss: 1.5244 - val_acc: 0.6176\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5482 - acc: 0.5938 - val_loss: 1.6198 - val_acc: 0.6012\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4420 - acc: 0.7188 - val_loss: 1.7318 - val_acc: 0.5837\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3976 - acc: 0.5938 - val_loss: 1.8504 - val_acc: 0.5633\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2884 - acc: 0.7500 - val_loss: 1.9779 - val_acc: 0.5425\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2475 - acc: 0.8125 - val_loss: 2.1108 - val_acc: 0.5207\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "24 [supervised loss: 1.2475, acc: 81.25%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0406 - acc: 0.7500 - val_loss: 2.2053 - val_acc: 0.5044\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9809 - acc: 0.7500 - val_loss: 2.2717 - val_acc: 0.4893\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9751 - acc: 0.7500 - val_loss: 2.3101 - val_acc: 0.4790\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9703 - acc: 0.7812 - val_loss: 2.3234 - val_acc: 0.4709\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7777 - acc: 0.8438 - val_loss: 2.3364 - val_acc: 0.4635\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7370 - acc: 0.8438 - val_loss: 2.3495 - val_acc: 0.4583\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6735 - acc: 0.8750 - val_loss: 2.3455 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6254 - acc: 0.9062 - val_loss: 2.3389 - val_acc: 0.4515\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4916 - acc: 0.9688 - val_loss: 2.3371 - val_acc: 0.4495\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5636 - acc: 0.9375 - val_loss: 2.3439 - val_acc: 0.4455\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7490 - acc: 0.6875 - val_loss: 2.3715 - val_acc: 0.4381\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7469 - acc: 0.6562 - val_loss: 2.4017 - val_acc: 0.4324\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8049 - acc: 0.6562 - val_loss: 2.4211 - val_acc: 0.4287\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6776 - acc: 0.6562 - val_loss: 2.4312 - val_acc: 0.4254\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6005 - acc: 0.6562 - val_loss: 2.4434 - val_acc: 0.4210\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5873 - acc: 0.6875 - val_loss: 2.4657 - val_acc: 0.4165\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5738 - acc: 0.5938 - val_loss: 2.4886 - val_acc: 0.4113\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5188 - acc: 0.6562 - val_loss: 2.5003 - val_acc: 0.4067\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4479 - acc: 0.7188 - val_loss: 2.5063 - val_acc: 0.4003\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3660 - acc: 0.8750 - val_loss: 2.5200 - val_acc: 0.3928\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "25 [supervised loss: 1.3660, acc: 87.50%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4613 - acc: 0.5938 - val_loss: 2.4947 - val_acc: 0.3890\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4056 - acc: 0.6250 - val_loss: 2.3738 - val_acc: 0.4002\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3764 - acc: 0.6562 - val_loss: 2.2100 - val_acc: 0.4215\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2693 - acc: 0.6562 - val_loss: 2.0589 - val_acc: 0.4455\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0184 - acc: 0.7812 - val_loss: 1.9316 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0360 - acc: 0.6875 - val_loss: 1.8176 - val_acc: 0.4950\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9220 - acc: 0.8125 - val_loss: 1.7188 - val_acc: 0.5179\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8261 - acc: 0.8438 - val_loss: 1.6410 - val_acc: 0.5368\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6948 - acc: 0.8750 - val_loss: 1.5876 - val_acc: 0.5508\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7238 - acc: 0.8438 - val_loss: 1.5470 - val_acc: 0.5636\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6254 - acc: 0.6250 - val_loss: 1.5352 - val_acc: 0.5652\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6180 - acc: 0.5625 - val_loss: 1.5375 - val_acc: 0.5664\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7719 - acc: 0.5000 - val_loss: 1.5471 - val_acc: 0.5645\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6589 - acc: 0.5312 - val_loss: 1.5666 - val_acc: 0.5609\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4990 - acc: 0.6562 - val_loss: 1.5963 - val_acc: 0.5536\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5853 - acc: 0.6250 - val_loss: 1.6419 - val_acc: 0.5402\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5563 - acc: 0.6875 - val_loss: 1.7060 - val_acc: 0.5248\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4409 - acc: 0.7188 - val_loss: 1.7902 - val_acc: 0.5050\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3527 - acc: 0.6875 - val_loss: 1.8931 - val_acc: 0.4769\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3572 - acc: 0.7188 - val_loss: 2.0142 - val_acc: 0.4503\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "26 [supervised loss: 1.3572, acc: 71.88%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4852 - acc: 0.5938 - val_loss: 2.1133 - val_acc: 0.4282\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3492 - acc: 0.6562 - val_loss: 2.1773 - val_acc: 0.4157\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4712 - acc: 0.6250 - val_loss: 2.1948 - val_acc: 0.4111\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2992 - acc: 0.6250 - val_loss: 2.1746 - val_acc: 0.4186\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1447 - acc: 0.6875 - val_loss: 2.1385 - val_acc: 0.4287\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1810 - acc: 0.6875 - val_loss: 2.0659 - val_acc: 0.4444\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9145 - acc: 0.7188 - val_loss: 1.9755 - val_acc: 0.4626\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8611 - acc: 0.8125 - val_loss: 1.8942 - val_acc: 0.4792\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7014 - acc: 0.8750 - val_loss: 1.8193 - val_acc: 0.4973\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6359 - acc: 0.8750 - val_loss: 1.7531 - val_acc: 0.5111\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6143 - acc: 0.6250 - val_loss: 1.7092 - val_acc: 0.5232\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6874 - acc: 0.5625 - val_loss: 1.6861 - val_acc: 0.5283\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6661 - acc: 0.5938 - val_loss: 1.6769 - val_acc: 0.5306\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6919 - acc: 0.5625 - val_loss: 1.6804 - val_acc: 0.5296\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6177 - acc: 0.6250 - val_loss: 1.6967 - val_acc: 0.5257\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5876 - acc: 0.6875 - val_loss: 1.7237 - val_acc: 0.5187\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6178 - acc: 0.6562 - val_loss: 1.7639 - val_acc: 0.5096\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4462 - acc: 0.7812 - val_loss: 1.8132 - val_acc: 0.4995\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4923 - acc: 0.6875 - val_loss: 1.8690 - val_acc: 0.4886\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3943 - acc: 0.7812 - val_loss: 1.9336 - val_acc: 0.4760\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "27 [supervised loss: 1.3943, acc: 78.12%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1884 - acc: 0.6875 - val_loss: 1.9372 - val_acc: 0.4722\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1439 - acc: 0.6562 - val_loss: 1.8668 - val_acc: 0.4819\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1949 - acc: 0.5938 - val_loss: 1.7624 - val_acc: 0.5058\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0297 - acc: 0.7188 - val_loss: 1.6826 - val_acc: 0.5249\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0113 - acc: 0.7500 - val_loss: 1.6198 - val_acc: 0.5369\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9100 - acc: 0.7500 - val_loss: 1.5731 - val_acc: 0.5485\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7866 - acc: 0.8438 - val_loss: 1.5380 - val_acc: 0.5601\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6880 - acc: 0.8438 - val_loss: 1.5107 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7665 - acc: 0.8438 - val_loss: 1.4873 - val_acc: 0.5764\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6103 - acc: 0.9375 - val_loss: 1.4696 - val_acc: 0.5831\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6144 - acc: 0.5938 - val_loss: 1.4755 - val_acc: 0.5815\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6695 - acc: 0.5625 - val_loss: 1.5052 - val_acc: 0.5757\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6349 - acc: 0.5312 - val_loss: 1.5543 - val_acc: 0.5639\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6326 - acc: 0.5938 - val_loss: 1.6276 - val_acc: 0.5448\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6867 - acc: 0.5000 - val_loss: 1.7307 - val_acc: 0.5228\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5957 - acc: 0.6250 - val_loss: 1.8718 - val_acc: 0.4900\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4996 - acc: 0.5938 - val_loss: 2.0382 - val_acc: 0.4592\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4412 - acc: 0.6875 - val_loss: 2.2274 - val_acc: 0.4319\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4919 - acc: 0.6875 - val_loss: 2.4351 - val_acc: 0.3976\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4687 - acc: 0.6562 - val_loss: 2.6048 - val_acc: 0.3759\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "28 [supervised loss: 1.4687, acc: 65.62%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2990 - acc: 0.6875 - val_loss: 2.7499 - val_acc: 0.3564\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2199 - acc: 0.7188 - val_loss: 2.8570 - val_acc: 0.3460\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2793 - acc: 0.6875 - val_loss: 2.9394 - val_acc: 0.3368\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0147 - acc: 0.7500 - val_loss: 3.0076 - val_acc: 0.3316\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1186 - acc: 0.7500 - val_loss: 3.0521 - val_acc: 0.3294\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9506 - acc: 0.7812 - val_loss: 3.0186 - val_acc: 0.3346\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7179 - acc: 0.8750 - val_loss: 2.9393 - val_acc: 0.3459\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9152 - acc: 0.7500 - val_loss: 2.8233 - val_acc: 0.3611\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8095 - acc: 0.8125 - val_loss: 2.7035 - val_acc: 0.3783\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7027 - acc: 0.8125 - val_loss: 2.5911 - val_acc: 0.3935\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.2873 - acc: 0.5625 - val_loss: 2.6081 - val_acc: 0.3916\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1820 - acc: 0.5000 - val_loss: 2.7827 - val_acc: 0.3737\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1924 - acc: 0.5000 - val_loss: 3.0614 - val_acc: 0.3446\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.9898 - acc: 0.6562 - val_loss: 3.4577 - val_acc: 0.3077\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7808 - acc: 0.6562 - val_loss: 3.9862 - val_acc: 0.2688\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.6696 - acc: 0.5938 - val_loss: 4.5933 - val_acc: 0.2317\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5926 - acc: 0.7188 - val_loss: 5.2121 - val_acc: 0.2014\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5474 - acc: 0.5938 - val_loss: 5.8232 - val_acc: 0.1785\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3930 - acc: 0.7812 - val_loss: 6.4326 - val_acc: 0.1613\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.3515 - acc: 0.7812 - val_loss: 6.9524 - val_acc: 0.1480\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "29 [supervised loss: 1.3515, acc: 78.12%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1524 - acc: 0.7188 - val_loss: 7.1588 - val_acc: 0.1407\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0969 - acc: 0.7500 - val_loss: 7.1482 - val_acc: 0.1398\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2433 - acc: 0.7500 - val_loss: 6.9576 - val_acc: 0.1433\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2003 - acc: 0.7500 - val_loss: 6.4716 - val_acc: 0.1537\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1271 - acc: 0.7500 - val_loss: 5.8350 - val_acc: 0.1700\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9668 - acc: 0.7500 - val_loss: 5.1695 - val_acc: 0.1899\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8739 - acc: 0.7812 - val_loss: 4.5570 - val_acc: 0.2146\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8156 - acc: 0.8438 - val_loss: 4.0105 - val_acc: 0.2462\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7198 - acc: 0.8750 - val_loss: 3.5302 - val_acc: 0.2761\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6271 - acc: 0.9375 - val_loss: 3.1302 - val_acc: 0.3113\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.4519 - acc: 0.4375 - val_loss: 2.9182 - val_acc: 0.3261\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3685 - acc: 0.3750 - val_loss: 2.8419 - val_acc: 0.3272\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.4139 - acc: 0.3438 - val_loss: 2.8762 - val_acc: 0.3076\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.1623 - acc: 0.4688 - val_loss: 3.0127 - val_acc: 0.2785\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.8618 - acc: 0.5000 - val_loss: 3.2125 - val_acc: 0.2431\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.4130 - acc: 0.6250 - val_loss: 3.4768 - val_acc: 0.2072\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.1489 - acc: 0.6875 - val_loss: 3.7769 - val_acc: 0.1764\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8647 - acc: 0.8125 - val_loss: 4.0937 - val_acc: 0.1546\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7011 - acc: 0.8750 - val_loss: 4.4157 - val_acc: 0.1377\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5685 - acc: 0.9062 - val_loss: 4.7308 - val_acc: 0.1268\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "30 [supervised loss: 0.5685, acc: 90.62%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Training time: 1803.0202s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbIxExOyCxJy",
        "colab_type": "code",
        "outputId": "d0f2fe6f-0d47-46a2-b864-f69100bb8144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "plot_pseudo_supervised_losses = np.array(losses_pseudo_labeled)\n",
        "plot_pseudo_unsupervised_losses = np.array(losses_pseudo_unlabeled)\n",
        "plot_pseudo_all_losses = np.array(losses_pseudo)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_all_losses, label=\"All loss\", color='black')\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_supervised_losses, label=\"Supervised loss\", color='tab:blue', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_unsupervised_losses, label=\"Unsupervised loss\", color='tab:green', linestyle='dashed')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Pseudo Label's Supervised and Unsupervised Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11f1493b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAFPCAYAAADuhTf/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMzMzJrA/wG0SN4C/rzriQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-4ZGValCxGj",
        "colab_type": "code",
        "outputId": "4fe68ba7-8a65-473d-daca-2cca17570f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "# tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-8000.h5\")\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-8000.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 34s 677us/step\n",
            "Training Accuracy: 13.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jD1xtbCw7H",
        "colab_type": "code",
        "outputId": "a14ab335-49c1-483c-cbf6-b7c5199e5fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "# tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-6000.h5\")\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-5900.h5\", by_name=False)\n",
        "\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 510us/step\n",
            "Test Accuracy: 10.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeX3mD5qC5TQ",
        "colab_type": "code",
        "outputId": "5804085b-4ff1-4507-9e9e-c60fc45b3998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "accs = []\n",
        "tx = [x for x in range(1,31,1)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  # tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\"+ str(e) +\".h5\")\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"Pseudo Label's accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 424us/step\n",
            "10000/10000 [==============================] - 4s 423us/step\n",
            "10000/10000 [==============================] - 4s 422us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 421us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 420us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 421us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 417us/step\n",
            "10000/10000 [==============================] - 4s 422us/step\n",
            "0.862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11f24c5ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFPCAYAAAAfjmxyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3iUVfrG8e9DB0EWKRY6UqQMomJD\n14aooALKrlJsiCL2vmvX1bX/Vld3UcQVuwKygKAIKygqikpQRJqIKBJgASkqSM/5/fFMlogJJCEz\n78zk/lxXrmRm3sz7zCSEueec8xwLISAiIiIiIiLpr0zUBYiIiIiIiEjJUMATERERERHJEAp4IiIi\nIiIiGUIBT0REREREJEMo4ImIiIiIiGQIBTwREREREZEMoYAnIiLFZmaNzCyYWbkknvMuM3sp2d+b\nTsxsnZk12cnt35nZicmsqSjM7Dkz+2vUdYiIpCMFPBGRNBZ/ob4h/oJ+efyFcdWo6yqMqF/Ex8Pp\nd1GdP5FCCFVDCAsh+udZRESSSwFPRCT9nR5CqAocDLQHbou4HhEREYmIAp6ISIYIISwB3gLaAJjZ\nBWa20Mx+NrNvzaxP7rFmdqGZzTWzNWY2wcwaxq//zZRLM5tsZhfFvy5rZv9nZj+Y2ULg1Lw1mNl+\nZjbGzFab2QIzu7g4j8XMHjOzxWb2k5lNN7Pf73BIJTMbFn9sn5nZgTvU8G8zWxl/3FcV8px/NrMl\n8fv8ysw6FnDcqWb2eby2xWZ21w63H21mH5nZ2vjtF8Svr2xmfzOzRWb2o5lNiV9XycxeMrNV8e+Z\nZmZ753PevmY2Ns/lr83stTyXF5tZu/jXwcyamll/oA/wp/go79g8d9nOzGbGaxlmZpV28tzk+/uS\n51xXxX/XfjCzh82sTPy2MmZ2W/wxrzCzF8ys+q6eq7gaZvZm/OfxiZntX1B9IiKynQKeiEiGMLP6\nQBfgczPbA3gc6BxCqAZ0AGbEj+sG3AKcCdQGPgBeLeRpLgZOAw7CRwv/sMPtQ4FsYL/4bfeZ2QnF\neDjTgHbAXsArwGs7BJBuwGt5bh9tZuXjwWIs8AVQF+gIXGNmJ+94ghDCdyGERgBm1gK4Ajg0/nyd\nDHxXQG3rgfOA3+EB91Iz6x6/n4Z4yP4H/ty2I/68A/8HHIL/LPYC/gTkAOcD1YH6QE1gALAhn/O+\nB/w+Hpr2AyoAR8bP2wSoCszc4TEOBl4GHopP2zw9z81nAacAjYG2wAX5PdhC/r6cgf8+HIz/bC6M\nX39B/ON4ILfGfxbiuQLoCfwFqAEsAO7Nrz4REfk1BTwRkfQ32szWAlPwEHBf/PocoI2ZVQ4hLAsh\nzI5fPwC4P4QwN4SwNX58u7yjMjtxFvD3EMLiEMJq4P7cG+IB8yjgzyGEjSGEGcC/8DBUJCGEl0II\nq0IIW0MIfwMqAi3yHDI9hDAihLAFeASoBBwBHArUDiHcHULYHF+H9jQeFnZmW/wcrcysfDz8fVNA\nbZNDCF+GEHJCCDPxsHNs/ObewMQQwqshhC3xxzAjHjwvBK4OISwJIWwLIXwUQtgEbMGDXdP49dND\nCD/lc96FwM94EDoGmAAsNbMD4uf/IISQs4vHmdfjIYSl8Z/j2Pj95qcwvy8PhhBWhxC+B/4O9Ipf\n3wd4JISwMISwDrgZ6BkfIc73ucpzn6NCCJ/Gz/nyTuoTEZE8FPBERNJf9xDC70IIDUMIl4UQNoQQ\n1gNn4y/Ol8Wnuh0QP74h8Fh8WtxaYDVg+IjXruwHLM5zedEOt60OIfy8w+2Fud9fMbMb4lMCf4zX\nWB2oleeQ/9UQDzW5o4YNgf1yH1v8e28BfjPlMa8QwgLgGuAuYIWZDY2PkuVX2+Fm9m58CuiP+HOc\nW1t9IL9gWAsPofnd9iIe1oaa2VIze8jMyhdQ6nvAcXjAew+YjIe7Y+OXi+K/eb7+BR9dy09hfl92\n/J3Ife7249e/I4uAcvjPo6Dnqqj1iYhIHgp4IiIZKoQwIYTQCdgXmIePZIG/GL8kHgpzPyqHED7C\npx8CVMlzV/vk+XoZ/sI8V4M8Xy8F9jKzajvcvqQodZuvt/sTPlpYI4TwO+BHPFTkqp/n+DJAvfj5\nFwPf7vDYqoUQuuzqvCGEV0IIR+OBJgAPFnDoK8AYoH4IoTowKE9ti4H81or9AGzM77b46NVfQgit\n8Ombp1HwqGduwPt9/Ov32HXACwVcX1g7+33JtePvxNL410vx5zPvbVuB5RT8XImIyG5QwBMRyUBm\ntreZdYuvxdsErMOnbIIHkpvNrHX82Opm9keAEMJKPJCdY95Q5UJ+/SJ8OHCVmdUzsxrATbk3hBAW\nAx8B98cbh7QF+gE723eubPzY3I8KQDU8BKwEypnZHcCeO3zfIWZ2Znyq3zXxx/gx8Cnws3nDlMrx\nx9DGzA7dxfPVwsxOMLOKeBDbkOf52lE1fKRyo5kdhk81zPUycKKZnWVm5cysppm1i48yDgEeMW8C\nU9bMjjSzimZ2vJnFzKws8BM+ZbOgc7+Hr2erHELIxtfDnYJP8fy8gO9Zjq9/K64Cf1/yuNHMasSn\n6V4NDItf/ypwrZk1Nt++4z5gWJ5pl795rnajThERQQFPRCRTlQGuw0dQVuMjPJcChBBG4aNTQ83s\nJ2AW0DnP914M3AisAlrjoS3X0/h0wi+Az4CRO5y3F9Aoft5RwJ0hhIk7qfMmPEzlfrwTv//xwHx8\nSt9Gfj0FEOB1fArqGuBc4Mz4SNg2fASsHfAtPnL2L3yK585UBB6IH/9foA6+Xiw/lwF3m9nPwB14\n6AUgvgatC3A9/rzPAHI7fN4AfIk3kFmN/wzK4COkI/BwNxcPcS/md+IQwnw8rH8Qv/wTsBD4MP7Y\n8/MMvrZwrZmN3tmTUMA5d/X7Av7zmB5/vG/Gzwkeal8E3sd/HhuBK+P3u7PnSkREislC2N2ZGyIi\nIlJamVkAmsXXMYqISMQ0giciIiIiIpIhFPBEREREREQyhKZoioiIiIiIZAiN4ImIiIiIiGSIclEX\nUFS1atUKjRo1iroMERERERGRSEyfPv2HEELt/G5Lu4DXqFEjsrKyoi5DREREREQkEma2qKDbNEVT\nREREREQkQyjgiYiIiIiIZAgFPBERERERkQyRdmvwREREREREtmzZQnZ2Nhs3boy6lISpVKkS9erV\no3z58oX+HgU8ERERERFJO9nZ2VSrVo1GjRphZlGXU+JCCKxatYrs7GwaN25c6O/TFE0REREREUk7\nGzdupGbNmhkZ7gDMjJo1axZ5hFIBT0RERERE0lKmhrtcxXl8CngiIiIiIiIZQgFPREREREQkQ6jJ\nSglYtgx++QUqVvz1R4UKkOGjxiIiIiIikkIU8ErATTfBCy/kf1uFCtvD3o4BsDgf1avDoYdCLAbl\n9NMTEREREYlM9+7dWbx4MRs3buTqq6+mf//+jB8/nltuuYVt27ZRq1YtJk2axLp167jyyivJysrC\nzLjzzjvp3r07/fr1+991F154Iddee+1u16SIUAIGDIATT4RNm4r3sX49rF796+s2bvz15R3tsQcc\nfjgceSR06ABHHAF77ZX8xy4iIiIiErVrroEZM0r2Ptu1g7//fefHDBkyhL322osNGzZw6KGH0q1b\nNy6++GLef/99GjduzOrVqwG45557qF69Ol9++SUAa9asYcaMGSxZsoRZs2YBsHbt2hKpWwGvBBx5\npH8kSgiwZYsHvZUr4ZNP4KOP/OOBB2DbNj+uZUsPex06eD0tWkAZrbIUEREREUmIxx9/nFGjRgGw\nePFiBg8ezDHHHPO/fev2io/ATJw4kaFDh/7v+2rUqEGTJk1YuHAhV155JaeeeionnXRSidSkgJcG\nzHyKZ4UKUK0aNGkCvXr5bevXw7Rp2wPfqFHwzDN+W40a20f4OnTwqZ1Vq0b3OEREREREEmFXI22J\nMHnyZCZOnMjUqVOpUqUKxx13HO3atWPevHmF+v4aNWrwxRdfMGHCBAYNGsTw4cMZMmTIbtelgJfm\n9tgDjjvOPwBycmD+fJg6dXvoGzfObytTBg48cHvg69ABGjZUIxgRERERkaL68ccfqVGjBlWqVGHe\nvHl8/PHHbNy4kffff59vv/32f1M099prLzp16sTAgQP5ezyJrlmzhm3btlGhQgV69OhBixYtOOec\nc0qkLgshlMgdJUv79u1DVlZW1GWklTVr4OOPt4e+jz/2kT+Affb5deA7+GBv5iIiIiIiksrmzp1L\ny5YtIzv/pk2b6N69O9999x0tWrRg7dq13HXXXWzYsIFbbrmFnJwc6tSpw9tvv826deu4/PLLmT59\nOmXLluXOO+9k//33p2/fvuTk5ABw//3307lz59+cJ7/HaWbTQwjt86tLAa8U2roVZs3ysJcb+hYu\n9NsqVID27eHoo+HGG6FWrWhrFRERERHJT9QBL1mKGvA0RbMUKlfOuwK1aweXXebX/fe/28Pe1Knw\nyCMweTK8845PAxURERERkdSnHosC+FTNM86Ahx+GKVPgtdcgKwt69vQRPxERERERSX0KeJKv7t3h\n8cfhjTfgiit8qwYRERERkVSSbsvNiqo4j09TNKVAl18OixfDgw9CgwZwyy1RVyQiIiIi4ipVqsSq\nVauoWbMmloFt4UMIrFq1ikqVKhXp+xTwZKfuuw+ys+HWW6FePTjvvKgrEhERERGBevXqkZ2dzcqV\nK6MuJWEqVapEvXr1ivQ9CniyU2XKwJAhsGwZ9OsH++4LnTpFXZWIiIiIlHbly5encePGUZeRcrQG\nT3apQgUYORJatoQePWDGjKgrEhERERGR/CjgSaFUrw7jxvnnLl1g0aKoKxIRERERkR0p4Emh1asH\nb70Fv/wCnTvD6tVRVyQiIiIiInkp4EmRtGkDo0fDN9/4VgobN0ZdkYiIiIiI5FLAkyI77jh4/nn4\n4APvqpmTE3VFIiIiIiICCQ54ZnaKmX1lZgvM7KZ8bm9gZu+a2edmNtPMuiSyHik5PXvCww/Da6/B\nDTdEXY2IiIiIiEACt0kws7LAQKATkA1MM7MxIYQ5eQ67DRgeQnjSzFoB44BGiapJStb11/tG6I8+\nCvXrw7XXRl2RiIiIiEjplsh98A4DFoQQFgKY2VCgG5A34AVgz/jX1YGlCaxHSpgZPPKIb4R+3XVQ\nty6cdVbUVYmIiIiIlF6JnKJZF1ic53J2/Lq87gLOMbNsfPTuyvzuyMz6m1mWmWVl8k716ahsWXjp\nJTjqKDj3XHj//agrEhEREREpvaJustILeC6EUA/oArxoZr+pKYQwOITQPoTQvnbt2kkvUnaucmV4\n/XVo3Bi6dYM5c3b9PSIiIiIiUvISGfCWAPXzXK4Xvy6vfsBwgBDCVKASUCuBNUmC1KwJ48dDpUq+\nR95STbYVEREREUm6RAa8aUAzM2tsZhWAnsCYHY75HugIYGYt8YCnOZhpqlEjePNN3wC9Sxf46aeo\nKxIRERERKV0SFvBCCFuBK4AJwFy8W+ZsM7vbzLrGD7seuNjMvgBeBS4IIYRE1SSJd/DBMGIEzJoF\nPXrA5s1RVyQiIiIiUnpYuuWp9u3bh6ysrKjLkF147jno29cbrzz/vHfcFBERERGR3Wdm00MI7fO7\nLZHbJEgpdsEFvkfeHXf4Hnn33ht1RSIiIiIimU8BTxLmtts85N13n4e8AQOirkhEREREJLMp4EnC\nmMETT8CSJXD55bDfftC1666/T0REREREiifqffAkw5UrB8OGefOVnj3hk0+irkhEREREJHMp4EnC\nVa0Kb7wB++4Lp50GX38ddUUiIiIiIplJAU+SYu+94a23IATfCH3FiqgrEhERERHJPAp4kjTNm/tI\n3pIlPpK3fn3UFYmIiIiIZBYFPEmqI46AoUNh+nRfk7d1a9QViYiIiIhkDgU8Sbpu3eCf//TRvMsv\n92mbIiIiIiKy+7RNgkTi0kvh++/hgQe8w+Yll0RdkYiIiIhI+tMInkTmvvvggANg7NioKxERERER\nyQwKeBIZMzjoIPjyy6grERERERHJDAp4EqlYzKdq/vhj1JWIiIiIiKQ/BTyJVCzmn2fNirYOERER\nEZFMoIAnkcoNeJqmKSIiIiKy+xTwJFINGsCeeyrgiYiIiIiUBAU8iZQZtGmjgCciIiIiUhIU8CRy\nsZgHPG14LiIiIiKyexTwJHKxGKxdC0uWRF2JiIiIiEh6U8CTyLVt659nzoy2DhERERGRdKeAJ5Fr\n08Y/ax2eiIiIiMjuUcCTyNWoAfXqKeCJiIiIiOwuBTxJCbmNVkREREREpPgSGvDM7BQz+8rMFpjZ\nTfnc/qiZzYh/zDeztYmsR1JXLAZz58KWLVFXIiIiIiKSvsol6o7NrCwwEOgEZAPTzGxMCGFO7jEh\nhGvzHH8lcFCi6pHUFot5uJs/H1q3jroaEREREZH0lMgRvMOABSGEhSGEzcBQoNtOju8FvJrAeiSF\nxWL+WdM0RURERESKL5EBry6wOM/l7Ph1v2FmDYHGwDsF3N7fzLLMLGvlypUlXqhE74ADoGxZBTwR\nERERkd2RKk1WegIjQgjb8rsxhDA4hNA+hNC+du3aSS5NkqFiRWjRQgFPRERERGR3JDLgLQHq57lc\nL35dfnqi6ZmlXtu2CngiIiIiIrsjkQFvGtDMzBqbWQU8xI3Z8SAzOwCoAUxNYC2SBmIx+O47+Omn\nqCsREREREUlPCQt4IYStwBXABGAuMDyEMNvM7jazrnkO7QkMDSGERNUi6SG30cqsWdHWISIiIiKS\nrhK2TQJACGEcMG6H6+7Y4fJdiaxB0kfeTpodOkRbi4iIiIhIOkqVJisiNGwI1appHZ6IiIiISHEp\n4EnKMIM2bRTwRERERESKSwFPUkos5gFPKzJFRERERIpOAU9SSiwGa9bA0qVRVyIiIiIikn4U8CSl\n5G20IiIiIiIiRaOAJylFAU9EREREpPgU8CSl7LUX1K2rgCciIiIiUhwKeJJyYjGYOTPqKkRERERE\n0o8CnqScWAzmzoUtW6KuREREREQkvSjgScqJxWDzZvj666grERERERFJLwp4knLUaEVEREREpHgU\n8CTltGwJZcsq4ImIiIiIFJUCnqScihWheXMFPBERERGRolLAk5QUiyngiYiIiIgUlQKepKRYDL79\nFn7+OepKRERERETShwKepKTcRiuzZ0dbh4iIiIhIOlHAk5TUtq1/1jRNEREREZHCU8CTlNSwIVSt\nCjNnRl2JiIiIiEj6UMCTlFSmDLRpoxE8EREREZGiUMCTlJXbSTOEqCsREREREUkPCniSsmIxWL0a\nli2LuhIRERERkfSggCcpK7eTpqZpioiIiIgUjgKepCwFPBERERGRolHAk5RVsybsu68CnoiIiIhI\nYSU04JnZKWb2lZktMLObCjjmLDObY2azzeyVRNYj6Se30YqIiIiIiOxawgKemZUFBgKdgVZALzNr\ntcMxzYCbgaNCCK2BaxJVj6SnWAzmzIGtW6OuREREREQk9SVyBO8wYEEIYWEIYTMwFOi2wzEXAwND\nCGsAQggrEliPpKG2bWHTJliwIOpKRERERERSXyIDXl1gcZ7L2fHr8moONDezD83sYzM7Jb87MrP+\nZpZlZlkrV65MULmSitRoRURERESk8KJuslIOaAYcB/QCnjaz3+14UAhhcAihfQihfe3atZNcokSp\nZUsoWxZmzoy6EhERERGR1JfIgLcEqJ/ncr34dXllA2NCCFtCCN8C8/HAJwJApUrQrJlG8ERERERE\nCiORAW8a0MzMGptZBaAnMGaHY0bjo3eYWS18yubCBNYkaUidNEVERERECidhAS+EsBW4ApgAzAWG\nhxBmm9ndZtY1ftgEYJWZzQHeBW4MIaxKVE2SnmIxWLgQ1q2LuhIRERERkdRWLpF3HkIYB4zb4bo7\n8nwdgOviHyL5ym20Mns2HH54tLWIiIiIiKSyqJusiOySOmmKiIiIiBSOAp6kvMaNYY89FPBERERE\nRHZFAU9SXpky0Lq1Ap6IiIiIyK4o4ElaaNvWA14IUVciIiIiIpK6FPAkLcRi8MMPsHx51JWIiIiI\niKQuBTxJC7mNVmbOjLYOEREREZFUpoAnaUGdNEVEREREdk0BT9JCrVqwzz4KeCIiIiIiO6OAJ2kj\nFlPAExERERHZGQU8SRuxGMyZA9u2RV2JiIiIiEhqUsCTtBGLwcaNsGBB1JWIiIiIiKQmBTxJG2q0\nIiIiIiKycwp4kjZatYIyZRTwREREREQKooAnaaNyZWjWTAFPRERERKQgCniSVtRJU0RERESkYAp4\nklZiMfjmG1i/PupKREQkGUKIugIRkfSigCdpJRbz/+xnz466EhERSbRXXoE6dWDMmKgrERFJHwp4\nklbUSVNEpHT46ivo3x9+/BHOPBNefDHqikRE0oMCnqSVJk2gShUFPBGRTLZxI5x9NlSq5H/vjz8e\nzjsPHn886sokSpquK1I4CniSVsqUgdatFfBERDLZjTfCF1/Ac89Bixbwxhs+inf11fCXv+iFfmn0\n9tuw994wYkTUlYikPgU8STvqpCkikrlGjYJ//hOuvRZOO82vq1gRhg2Dvn3hrrvgmmsgJyfSMiWJ\nPvgAunWDlSvhsstg9eqoKxJJbQp4knZiMf8jv3x51JWIiEhJWrQILrwQDjkEHnjg17eVKwfPPAPX\nXedTNfv2ha1bo6lTkufTT+HUU6FhQ/jPfzzc3Xhj1FWJpDYFPEk7bdv6Z43iiYhkji1boHdv2LbN\nR+sqVPjtMWbwf/8Hf/0rvPAC9Ojh6/UkM33xBZx8MtSuDRMnQqdOcP31MGQIvPde1NWJpK6EBjwz\nO8XMvjKzBWZ2Uz63X2BmK81sRvzjokTWI5lBnTRFRDLPnXfCRx/B4MGw//4FH2cGt94KAwfC2LHQ\npQv8/HPy6pTkmDvXA13VqjBpEtSt69ffeSc0bgyXXKJwL1KQQgU8M7vazPY094yZfWZmJ+3ie8oC\nA4HOQCugl5m1yufQYSGEdvGPfxX5EUipU7u2L7RWwBMRyQxvv+1TMvv1g549C/c9l10GL73k67NO\nOAF++CGxNUryfPMNnHiiN1abNAkaNdp+W5Uq8OSTvo3G/fdHVqJISivsCN6FIYSfgJOAGsC5wAM7\n/xYOAxaEEBaGEDYDQ4Fuxa5UJI9YDGbOjLoKERHZXcuXw7nnQsuWRd8GoXdvGD0aZs2CY46B7OzE\n1CjJ8/330LEjbNrk0zKbN//tMSef7D/7++/3kT4R+bXCBjyLf+4CvBhCmJ3nuoLUBRbnuZwdv25H\nPcxsppmNMLP6+Z7crL+ZZZlZ1sqVKwtZsmSyWAxmz/a1GiIikp5ycjzc/fijr7urUqXo93HqqTBh\nAixZAkcdBV9/XfJ1SnL8978+crd2rTdUadOm4GMffdSnb15yiTqqiuyosAFvupn9Bw94E8ysGlAS\n/5zGAo1CCG2Bt4Hn8zsohDA4hNA+hNC+du3aJXBaSXexmM+9/+abqCsREZHieughn5752GM7fzG/\nK8ccA+++Cxs2wNFHw4wZJVejJMcPP3i4W7oU3noLDj5458fXqeMNdz74wLurish2hQ14/YCbgEND\nCL8AFYC+u/ieJUDeEbl68ev+J4SwKoSwKX7xX8AhhaxHSjk1WhERSW8ffQS33QZnnQUXX7z793fw\nwf5iv2JFOO44mDJl9+9TkmPtWjjpJH/TduxYOPLIwn1f375w7LHwpz/56J+IuMIGvG7ANyGEtfHL\n24Amu/ieaUAzM2tsZhWAnsCYvAeY2b55LnYFNJNaCqVVK++kpoAnIpJ+1qyBXr2gQQPvmmm7WvRR\nSC1aeLDbZx8PDG+9VTL3K4mzbp13Qp01C0aOhOOPL/z3msFTT8Evv8C11yauRpF0U9iAd2cI4cfc\nC/Ggd+fOviGEsBW4ApiAB7fhIYTZZna3mXWNH3aVmc02sy+Aq4ALivoApHSqUgWaNlXAExFJNyF4\nt8ylS2HoUKhevWTvv0EDH8lr2RK6dvW1fZKaNmyA00/3zcyHDYPOnYt+Hy1a+LYZQ4cq0IvkshDC\nrg8ymxlfJ5f3ui9DCLGEVVaA9u3bh6ysrGSfVlJQjx4e8ObPj7oSEREprCeegMsv9/VT11+fuPP8\n9JOHhw8+8Lb6l1ySuHNJ0W3aBN27e4Ocl17yrpi7c1/t2nlgnD0b9tij5OoUSVVmNj2E0D6/2wo7\ngpdlZo+Y2f7xj0eA6SVXokjRtW0LCxb41AwREUl9M2bAddf5SE2ip9TtuSeMH+9dNgcM8H32JDVs\n3epTdMePh6ef3r1wB77ucvBgWLQI7rqrREoUSWuFDXhXApuBYfh+dhuByxNVlEhhxGI+1WfOnKgr\nERGRXVm3Ds4+G2rWhOef902sE61yZV/X1acP3HyzN+MoxMQlSaBt2+D882HUKN/3sF+/krnf3//e\nm/U8+ih8/nnJ3Gcm2LLF9xN87jn97pcm5QpzUAhhPd5FUyRl5O2k2T7fAWoREUkVV1zhe9S98w4k\nc8ej8uXhhRfgd7+Dhx+G1au9MUfZssmrQVxOjk+VfeUV36T8yitL9v4ffBDGjIH+/eHjj0vvz3jD\nBt9HcORI70q6Zo1fv3w5/PnP0dYmyVGo98/M7G0z+12eyzXMbELiyhLZtSZN/N1ZNVoREUltL77o\no3a33+5bGCRbmTLwj3/4+Z95Bnr29HVbkjwhwDXX+PN/++1wUwKGDWrUgL//HbKy4J//LPn7T2U/\n/ujB+Q9/gFq1fH3jmDFw2mk+Wnr22T6KPX581JVKMhS2ycrnIYSDdnVdMqjJiuR16KHegW3ixKgr\nERGR/Myf73vUHXIITJoE5Qo1dyhxHn3U1wGedJKPcKghR+KF4OHiwQf9uf+//yu5rTHyO9epp8L7\n7/sSjgYNEnOeVLBiBbz+uv8eT5rk0zH32QfOOMM/jjvOR7AB1q+Ho47ydYqffgrNmkVaupSAkmiy\nkmNm//snYmaNAM3klcjFYhrBExFJVRs3+shBpUrw8svRhzvw5i5Dhvgbg506bZ++Jonz1796uBsw\nILHhDvy+n3jCg94VV2TeurNFi3yU8phjPMz17+9volx9NXz4ISxZ4o+/U6ft4Q78jYzRo33aavfu\n8PPP0T0GSbzCBrxbgSlm9gOiAowAACAASURBVKKZvQS8B9ycuLJECicW83ewVqyIuhIREdnRn/7k\nnTOfew7q1Yu6mu369oURI2D6dDj2WFi2LOqKMtcjj8Add8B558HAgYkNd7kaNYK//MXXn40cmfjz\nJVJuM7l77/VR8EaN/E2KtWv9eZ0xwzuKP/wwdOiw8+ZFjRrB8OHw1Vf+88jJSdajkGQrVMALIYwH\n2gNfAa8C1wMbEliXSKHkbbQiIiKpY/RoX/d27bW+DijVnHEGjBsHCxd6B8Zvv426oszz5JO+1+Ef\n/+hr75LROTXXNdf43nhXXunr09JJCDBtmk9rbdkSWreG226DChXgoYe8WdHMmb4lxIEHFi00n3AC\n/O1v/u/z3nsT9hAkYoVdg3cRcDVQD5gBHAFMDSGckNjyfktr8CSv5ct9isKjj/ofcxERid733/uL\n6yZNfNpYxYpRV1SwTz7xffkqVfJphN27Q7VqUVeV/p5/Hi64wMP9v//t4STZsrLg8MO9c+cTTyT/\n/EWxdStMmeIjjqNHw+LFPp3yuOPgzDOhWzeoW7dkzhWC/2xeeMHX8HXtWjL3K8lVEmvwrgYOBRaF\nEI4HDgLWllB9IsW2997eblsjeCIiqSF3E+utW2HYsNQOd+AB4IMPfI3Seef5/ys9e/oLX3XaLJ7h\nw+HCC+HEE+G116IJd+BbKF15JQwaBFOnRlPDzmzeDG++6XsB7rsvHH+8b/x+0EE+rXn5cl8retll\nJRfuwEf8Bg3y5+ecc2DevJK7b0kNhQ14G0MIGwHMrGIIYR7QInFliRRe27YKeCIiqeLOO+Gjj2Dw\nYNh//6irKZzWrX1d0pQpvj5v0iQfydtnH988+913fYNu2bWxY31j+Q4dfCSqUqVo67nnHl//2b+/\nB6pU8fnnPr3ytNN8PehJJ3kYXrnS31w4/3yoWTNx569c2UcLK1f20cG1GrbJKIUNeNnxffBGA2+b\n2evAosSVJVJ4sRjMnq3FwiIiUZs40Tew7tfPR8HSSZky3kZ+4EBYuhTeegtOPx2GDvV1Sw0a+Hqy\n6dMzrzNjSXn7bd+H7aCDfGQqFbagqFbNf6azZnkHz6jl5Hgdhx8OP/3k01dXrPAus3/4A1Stmrxa\n6tf3cLlwoYdyvYmROQq1Bu9X32B2LFAdGB9CSPp7IVqDJzsaMsRfTHz9NTRtGnU1IiKl0/LlPiJR\ns6Y3iKhSJeqKSsYvv8Abb/gm0uPG+V5jzZpB797+0bx51BWmhg8+gJNP9ufm3Xdhr72irujX/vAH\n/znOmhXda4UlS3xkbtIkb/Lz9NOJHaUrrCef9Gmgt97qW1pIeiiJNXj/E0J4L4QwJopwJ5IfddIU\nEYlWTo6vX/vxR193lynhDvyxnHWWTzdcvtxflNevD3ffDS1a+DqmRx7xF++l1aef+ubiDRv6KF6q\nhTuAxx/39aADBkQzAjt6tC8pmTrVf4f+/e/UCHfgz8lFF3lXzREjoq5GSkISG9aKJEbr1r5geObM\nqCsRESmdHn4Y/vMfeOwxaNMm6moSp0YNfyE8aRJkZ3uwM/Opm/Xr+1TOf/2rdG2ePn68j9zVru1T\ndOvUibqi/O23HzzwgP/sXnwxeeddv967eJ5xBjRuDJ995r9DydgPsLDM4J//hCOP9O6aesM8/RV5\nimbUNEVT8tOsmU8N0jtPIiLJNXWq7yPXo4evV0ulF67JMn8+vPqqT+OcPx/Kl4cuXbyb6OmnZ9aI\nZq4FC+C667ypSvPmMGGCb6SdynJy4Oij/Wc0bx7UqpXY8332mU/jnT8f/vQnH/WNqqNoYSxb5pup\nV67s06xTcSRWtivRKZoiqSgW0ztOIiLJtmaNh5gGDbxrZmkMd+AB5847PTRMnw5XXeUvkHv29G0X\nzj3Xm7Zs2RJ1pbvv55/hppt89sy77/regTNnpn64A2+kM3iwTyW+4YbEnScnxzckP+IIWLfORw0f\neCC1wx34Vg0jR/rodM+evtWJpCcFPMkIsZi/m7hhQ9SViIiUDiH4VLMlS3zkrnr1qCuKnhkcfLB3\nSfz+ew9AvXp5R8kuXXya4OWX+8hOugkBXnrJ1x0++KAHgNyRqVTf6zCvNm285uef9+BV0rKzoVMn\n+POffQPxmTN9f7t0ccQR3nTl7bfhlluirkaKSwFPMkIs5u+YzZkTdSUiIqXDk0/6u/0PPACHHRZ1\nNamnbFk47jgfMVq2zPc2O/FEePZZnwbXsaNPa0yHlTJZWb6FxLnn+p5yU6d6QNp336grK57bbvNO\nmgMGlOwbwyNHeiOVTz6BZ57xfe3ScZrjhRf6GxEPP+zTjiX9KOBJRlAnTRGR5Jk509dfde4M114b\ndTWpr2JFH8159VUPew8/7NM5TzkF2rXzkbFUnL65fLlvQ3TYYb5X2pAh8PHHPsqTzipXhkGDfObP\nvffu/v2tXw8XX+zrUPff3zcxv/DC9J6y/OijcMwx/vNPxxHn0k4BTzJC06ZQqZICnohIMtx8M+y5\np4/ilNEriSKpXt3Xf337LTz3nG8ufe65HgwefdTXuEVt82bvENq8ObzwgncJnT8f+vbNnJ93x46+\ntceDD/reeMWVleXTcp95xtcmfvihN35Ld+XL+whkrVreAXTlyqgrkqLIkH+mUtqVLesLvhXwREQS\n69tvvWHIpZd6a3wpngoVfNPrL7/0NXpNmvioaIMGvvbpv/+Npq7x432a4fXX+7TMWbN8xHHPPaOp\nJ5H+9jcP3Jdc4ss8imLbNg+HRx4Jv/wC77wD99+f+o1UiqJOHd+/b8UK3wsyFUeZJX8KeJIx1ElT\nRCTxnn7ap55dfHHUlWQGM2/AMnmyr9068UQPDg0b+nP81VfJqWPBAp9G2rmzh5033oBx47ypSqaq\nVctHKj/6yNdKFlZ2tv+cbroJuneHL77w9ZaZ6JBD/LmZPDmxnUelZCngScaIxfwdT00jEBFJjM2b\nfSra6ad7sw0pWYcd5tPivvrK1z699BK0bOkh4qOPEnPO/LY9+PJLOPXUxJwv1Zx7rk/X/POfYenS\nXR//73/7COe0ab4mcfjw9GykUhTnnutrbR9/3KcVS+pLaMAzs1PM7CszW2BmN+3kuB5mFsws3836\nRApDjVZERBIrd7rWgAFRV5LZmjaFJ57wrRZuvx0++MCnSx51lHfjLOp0wvzk5MCLL27f9qBXr/Tc\n9mB3mXlH2E2b4OqrCz5u3TrfFuQPf/Cfz4wZviYxnRupFMVDD3kQHjAAPv006mpkVxIW8MysLDAQ\n6Ay0AnqZWat8jqsGXA18kqhapHRQwBMRSaxBg3xD65NOirqS0qF2bfjLXzzo/eMf3oGze3do1Qr+\n9S/YuLF49zttmofF887zkdiPP/aRmXTd9mB3NWsGd9wBI0b41NQdTZvmjVSGDIFbb/VGKk2bJr/O\nKJUrB8OG+e/ImWdGt0ZUCieRI3iHAQtCCAtDCJuBoUC3fI67B3gQKOafKRG3994+n14BT0Sk5M2b\n51P4Lrkkczoppos99oArrvARtqFDoWpVX5/XqJE39lizpnD3k7vtweGHe7OcZ5/1cHf44QktPy3c\ncINPU73sMh+tA2+kcv/90KGDh+nJk+Gvf/UOk6VRzZo+ir9mjW8JsXlz1BVJQRL5J7ousDjP5ez4\ndf9jZgcD9UMIb+7sjsysv5llmVnWSi2wkgKYqdGKiEiiDB7sL2z79o26ktKrXDk4+2wfUZo0yffQ\nu+UW77x53XU+0pefzZu9Y2Tz5j4tM3fbgwsuUFjPVaGC/44vXuzTYhcv9imJt9ziI1ZffOH7wpV2\nBx7obwx89BFcdVXU1UhBIvtnbWZlgEeA63d1bAhhcAihfQihfW31ZJadiMVg9uySWZ8gIiJuwwaf\nwnfmmT5bQqJlBiec4FsazJjh0zb/8Q/fS+/cc30j+ly52x7ccAMcfXRmb3uwuzp08O0/Hn/cn7Pp\n0/33fuhQqFEj6upSx1lneWOep57yD0k9iQx4S4D6eS7Xi1+XqxrQBphsZt8BRwBj1GhFdkcsBuvX\n+9QTEREpGcOH+7SsSy+NuhLZ0YEH+qjcN9/AlVf6FLoDD4RTTvFup3m3PXjzTR/Fk4Ldf7+PiLZo\n4eH5/PNLTyOVovjrX/137MorfU2ipBYLISTmjs3KAfOBjniwmwb0DiHMLuD4ycANIYSsnd1v+/bt\nQ1bWTg+RUuzTT30twahR/o6miIjsviOPhLVrYc4cvdhNdWvWeDOcxx7zNzzvuMO7Q2bSBtyJtnmz\nT0fW7/rOrVnjW3v8/LOPdtatu+vvkZJjZtNDCPkOjCVsBC+EsBW4ApgAzAWGhxBmm9ndZtY1UeeV\n0q11a/+DrHV4IiIlY8YMb8QxYIBe8KaDGjXg5pt9DdnKlXDjjQp3RVWhgn7XC6NGDR8xXr/ep28X\nt6urlLxyibzzEMI4YNwO191RwLHHJbIWKR322AOaNFHAExEpKU89BZUqeUt9SR/ly5febo+SPK1b\nwwsveMC79FLfSkLhOHrqnSQZR500RURKxs8/w0svQc+eajIhIvk74wyfCvzcc/DPf0ZdjYACnmSg\nWMzbP2uqgIjI7nn5Zd8TbMCAqCsRkVR2553QtStcey18/nnU1YgCnmScWMw7hs2ZE3UlIiLpKwRv\n1tGunTdSEBEpSJkyPoJXvjw880zU1YgCnmScWMw/a5qmiEjxffKJb+6s5ioiUhg1avjWHMOGwZYt\nUVdTuingScZp2hQqVlTAExHZHYMGQbVq0Lt31JWISLro3Rt++AEmToy6ktJNAU8yTrly0KqVAp6I\nSHGtXu3vwp9zjoc8EZHC6NwZfvc7X78r0VHAk4ykTpoiIsX3wgveqOqSS6KuRETSScWK8Mc/bt8f\nT6KhgCcZqW1bWLYMVq2KuhIRkfSS21zlyCPhwAOjrkZE0k3v3h7uxoyJupLSSwFPMpIarYiIFM/k\nyfDVV9oaQUSK55hjoG5deOWVqCspvRTwJCMp4ImIFM+gQd4N749/jLoSEUlHZcpAr14wfrw3XJHk\nU8CTjLTPPlCzpgKeiEhRLF8OI0fCBRdA5cpRVyMi6apPH9i6FUaMiLqS0kkBTzKSmRqtiIgU1ZAh\n/qJMzVVEZHcceCC0bKlumlFRwJOMlRvwcnKirkREJPVt2waDB8MJJ0CLFlFXIyLpzMxH8aZMgUWL\noq6m9FHAk4wVi3kXp+++i7oSEZHU95//+N9LNVcRkZLQq5d/fvXVaOsojRTwJGOp0YqISOE9+STs\nvTd06xZ1JSKSCZo08e1W1E0z+RTwJGO1bu2fFfBERHbu++/hzTehXz+oUCHqakQkU/Tu7a/D9Fos\nuRTwJGNVq+bvHumPiojIzv3rX77B+cUXR12JiGSSs86CsmXVbCXZFPAko6mTpojIzm3Z4gGvc2do\n1CjqakQkk9SpAyed5Ovw1PQueRTwJKPFYjB/PmzaFHUlIiKpaexYWLZMzVVEJDF69/Zp4B9+GHUl\npYcCnmS0WMxbf8+dG3UlIiKpadAgqF8funSJuhIRyUTdu0Plymq2kkwKeJLR1ElTRKRgX38Nb78N\n/fv7OhkRkZJWtap35x0+HDZvjrqa0kEBTzJas2ZQsaICnohIfgYP9mDXr1/UlYhIJuvTB1av9v02\nJfEU8CSjlSsHLVvCzJlRVyIiklo2boRnn/XpU/vuG3U1IpLJTjoJ9tpL3TSTRQFPMp46aYqI/Na/\n/w2rVqm5iogkXoUKvmXC66/DunVRV5P5EhrwzOwUM/vKzBaY2U353D7AzL40sxlmNsXMWiWyHimd\nYjFYutSnBoiIiBs0CJo2hRNOiLoSESkNeveGDRtg9OioK8l8CQt4ZlYWGAh0BloBvfIJcK+EEGIh\nhHbAQ8AjiapHSi81WhER+bVZs2DKFLjkEiijuTwikgRHHQUNGqibZjIk8s/6YcCCEMLCEMJmYCjQ\nLe8BIYSf8lzcAwgJrEdKqbZt/bMCnoiIGzTIG1BdcEHUlYhIaVGmDPTq5Y1WVqyIuprMlsiAVxdY\nnOdydvy6XzGzy83sG3wE76r87sjM+ptZlpllrVy5MiHFSubad19f2KuAJyLi619eeAH++EeoVSvq\nakSkNOnTx/cnfu21qCvJbJFPzAghDAwh7A/8GbitgGMGhxDahxDa165dO7kFStoz82ma77/vXeNE\nREqzoUPh55/VXEVEki8WgzZt1E0z0RIZ8JYA9fNcrhe/riBDge4JrEdKsSuvhHnzfGrA1q1RVyMi\nEp1Bg/wFVocOUVciIqVRnz4wdSosXBh1JZkrkQFvGtDMzBqbWQWgJzAm7wFm1izPxVOBrxNYj5Ri\nPXrAY49556YBAyBotaeIlEJZWTB9uv8dNIu6GhEpjXr18s+vvhptHZksYQEvhLAVuAKYAMwFhocQ\nZpvZ3WbWNX7YFWY228xmANcB5yeqHpGrroLbboNnnoFbb426GhGR5Bs0CKpUgXPOiboSESmtGjaE\no4/2aZp6wz0xyiXyzkMI44BxO1x3R56vr07k+UV2dPfdsHIl3H8/1K4N114bdUUiIsmxdq2/Y967\nN1SvHnU1IlKa9ekDl14KX3wB7dpFXU3mibzJikgymcHAgT5l87rr4MUXo65IRCQ5XnwRfvlFzVVE\nJHp/+AOUK6dmK4migCelTtmy/gflhBOgb194882oKxIRSawQfHrmoYfCIYdEXY2IlHa1asEpp/is\ngpycqKvJPAp4UipVrOgNV9q1872gPvww6opERBJnyhSYM0ejdyKSOnr3hiVLfBsrKVkKeFJqVasG\n48ZB/fpw2mkwa1bUFYmIJMagQb7u7uyzo65ERMR17Qp77AGvvBJ1JZlHAU9KtTp1YMIE7yp38snw\n3XdRVyQiUrJWroQRI+C88/zFlIhIKthjD+jeHV57DTZtirqazKKAJ6Veo0Ye8jZsgJNOghUroq5I\nRKTkPPccbN4Ml1wSdSUiIr/Wp493+B0/PupKMosCngjQpg288QZkZ0PnzvDTT1FXJCKy+3Jy4Kmn\n4Pe/h9ato65GROTXTjzRG66om2bJUsATievQwacxzZzpUwY2boy6IhGR3TNxInzzjZqriEhqKl/e\n1waPHas310uSAp5IHl26wLPPwrvv+rSBbduirkhEpPgGDfJ3x3v0iLoSEZH89enjb6qPGhV1JZlD\nAU9kB+ecA48+CiNHwmWX+f5RIiLpZskSGDMGLrzQt4YREUlFRxwBjRurm2ZJUsATycc118DNN8Pg\nwXD77VFXIyJSdM8847MQ+vePuhIRkYKZ+Z54EyfCf/8bdTWZQQFPpAD33gsXXeSfH3ss6mpERApv\n61Z4+mnvDLz//lFXIyKyc717e1Oo4cOjriQzKOCJFMAMnnwSzjjDR/TU4UlE0sWbb3pXYDVXEZF0\n0KoVtGun11olRQFPZCfKlfM54ccdBxdcAG+9FXVFIiK7NmgQ7LcfnHZa1JWIiBRO797w6aewYEHU\nlaQ/BTyRXahUCV5/HWIx70Q3dWrUFYmIFGzhQpgwwaeYly8fdTUiIoXTq5fPnlKzld2ngCdSCHvu\n6aN3devCqafC7NnR1BGCn/vuu31D9pdfVpdPEfm1p5/2F0kXXxx1JSIihVevHhxzjF7blAQFPJFC\n2ntv+M9/fETv5JNh0aLknDcE+PxzuO02aNkS2rSBu+6CL7/0LR2OOgqmTUtOLSKS2jZv9u6Zp5/u\nL5ZERNJJnz4wfz589lnUlaQ3BTyRImjc2Kc+rV/v3elWrkzMeUKATz6BP/0JmjaFgw+G++/3NTUD\nB/r+Vt9/75uyL1wIhx0GffvCsmWJqUckmTZsSN4bKJlm1Cj/u6TmKiKSjnr08Knlarayeyyk2Rho\n+/btQ1ZWVtRlSCk3ZQp06uSjae+8A9Wq7f59btsGH34I//63b7Kene1/5Dp29D943bpB7dq//b6f\nfoL77vPN2StUgFtv9a6flSrtfk0iybZqlY+Qz5zp/w7UJKTwtm6FDh084H3zDZTRW7gikoa6d/dm\nK4sXQ9myUVeTusxsegihfX636c+/SDEcfTS89ppPnTzjDNi0qXj3s3Wrb+x56aW+vu/YY+Gpp3zE\n7oUXYMUKX/t30UX5hzvw9YEPPOBr8zp29A3aW7f2xjBp9v6NlHLLl8Pxx8OsWdCsmb+xMW5c1FWl\nj7vu8una99yjcCci6atPH5+RNHly1JWkL/0XIFJMp50GQ4bApEm+Fm7btsJ936ZN/qK1Xz9f19ep\nk4e5Y46BoUP93ffXX4dzz4Xf/a7w9TRtCqNHb18n2L27TyOdNat4j08kmbKz/Q2Ob77xPdymTPER\n8jPP9GnRsnMTJvhI/oUX+t8jEZF0ddppPjNK3TSLT1M0RXbTI4/A9df7mpcnnvDudTvasMFfgI0Y\nAWPH+rTKPff0Rgg9eviUtCpVSq6mrVt9H6w77vBzXXop/OUvsNdeJXcOkZLy3Xdwwgnwww/+5sfR\nR/v1q1f7qPTcuf7vplOnSMtMWUuW+AbB++zja3dL8m+JiEgULrjA1xQvX64lJwXRFE2RBLruOvjz\nnz1Q3XXX9uvXrYPhw+Gss3x65Rln+HTLHj18hGLFCnjpJb++pF+QlSsHV1wBX3+9PXg2a+YNWrZu\nLdlzieyOr7+G3/8e1q710fDccAf+hsTEidCiBXTt6rfLr23d6ntHbdjg08YV7kQkE/Tu7W9Qa5p+\n8SR0BM/MTgEeA8oC/wohPLDD7dcBFwFbgZXAhSGEnfZO0wiepKIQfJ3ckCFw1VXeAXDCBNi40adh\nnnGGB7tjj41m4+Evv/TGK++84+vz/v53OPHE5Nchktfs2f57mLsW9cAD8z9u5Uof4cudvnn88cmt\nM5XdeqtPzXzxRU3NFJHMsXWrb/Vy1FHefE5+K5IRPDMrCwwEOgOtgF5m1mqHwz4H2ocQ2gIjgIcS\nVY9IIpl5c5Tu3eHxx2H6dOjfH957z6dPPfmkv5CNItwBxGL+AnrUKPjlF5/q1r27v2AWicLnn8Nx\nx/m/nffeKzjcgY+AT5rk25Scdhq8/37SykxpuevuLrpI4U5EMku5cnD22f6m3tq1UVeTfhI5RfMw\nYEEIYWEIYTMwFOiW94AQwrshhF/iFz8GtC2rpK1y5XxK5qxZPoL32GPeOCVVWvyaeaibM8f31Js4\nEVq1gptugp9/jro6KU0++cRH5KpU8bDWase3/vJRp46PQDdoAF26eBOW0iw720NdLOZvKomIZJo+\nfbwx3ciRUVeSfhIZ8OoCi/Nczo5fV5B+wFv53WBm/c0sy8yyViZqZ2mRElC+vE+BTOUW5ZUqeaib\nP9/X7jz4IDRvDs89Bzk5UVcnme799300u2ZN/7pp08J/7957e8irWxc6d4aPPkpcnaks77q74cOh\ncuWoKxIRKXmHHgr7769umsWREi9DzewcoD3wcH63hxAGhxDahxDa1y5oMzARKZL99vNQ98kn0LAh\n9O0Lhx8OU6dGXZlkqrffhlNO8XUV77/vv3dFte++8O67/vmUU+Djj0u+zlR3xx0+gvnUU3DAAVFX\nIyKSGGY+ivfOO7B0adTVpJdEBrwlQP08l+vFr/sVMzsRuBXoGkIo5nbRIlJchx3mIyEvvODrBTt0\n8Klf2dlRVyaZZOxYXz/XrJmvudtvv+Lf1377ecirU8e3GPn005KrM9W99ZZPsb74Yn/hIyKSyXr3\n9kZ2w4ZFXUl6SWTAmwY0M7PGZlYB6AmMyXuAmR0EPIWHuxUJrEVEdqJMGd9Yff5878o3YoS3pv/r\nX30amMjueO0137C8bdvtwWx31a3r91WrFpx0EpSG5srZ2f7vNBbzNb4iIpmuRQs45BB4+eWoK0kv\nCQt4IYStwBXABGAuMDyEMNvM7jazrvHDHgaqAq+Z2QwzG1PA3YlIElSt6qFu7lxf43T77dCype+f\np+WvUhwvvgg9e/r034kTfW+7klK/voe8GjW8M+xnn5XcfaearVv9edy0yQOz1t2JSGnRp493J//q\nq6grSR8JXYMXQhgXQmgeQtg/hHBv/Lo7Qghj4l+fGELYO4TQLv7Rdef3KCLJ0Lixj+K98443w7ji\nCl/zdOqpvth5/fqoK5R0MHgwnH++b4cwYQJUr17y52jQwEPennt685YZM0r+HKng9tvhww/9OW3R\nIupqRESS5+yzfT2emq0UXko0WRGR1HT88f6u2cyZcMMNvmF6nz4+xa5PHxg3DrZsibpKSUWPPQaX\nXOIjwW+8AXvskbhzNWoEkyf7CPSJJ/rvayYZNw4eeMD31uzVK+pqRESSa7/9fGudl1/29Xiyawp4\nIrJLsZi/wPzuO2+Qcc453uzh1FN9LdQVV3j3Tf3hFfDflWuu8XV3o0YlZzph48Y+kle5MnTs6PtR\nZoLFi+G883wj+L//PepqRESi0bs3fPMNTJsWdSXpQQFPRAqtTBnfvP2pp2DZMhg92kf5nnnGu282\nbepTyebOjbpSiUII3sL/5pv9P+Nhw6BCheSdf//9fVpxhQr+bu+cOck7dyJs2bJ93Z32uxOR0qxH\nD6hYUc1WCksBT0SKpWJF6NbNX8QvX+576u2/P9x3H7RqBQcfDH/7m2+9IJkvBLjxRrjnHujXz7fd\nKFcu+XU0a+Yhr2xZD3nz5iW/hpJy222+hcngwdC8edTViIhEp3p1nzU0bJg3nZKdU8ATkd22557e\nTOM///FW7o8+6i+wb7jBOx127AhDhsCPP0ZdqSRCTo5P0/3b3/zz4MH+849KixY+XRN8hDkdO6+9\n+SY89JCvY9S6OxERX/u/fLm/iSc7ZyHNFs20b98+ZJWGDY9EMsD8+d716uWXYcECH/U77TSfvnfq\nqX5Z0tu2bb7p9rPP+gjegw96t7NUMGeOB7yyZX3taLNmUVdUOIsXQ7t2/ubIxx9DpUpRVyQiEr2N\nG2GffaB7d581VNqZ2fQQQvv8btMInogkTPPmcNddHvQ++cRHIz74wOfS7703XHSRj7Tk5ERdqRTH\nli3ecOfZZ/3nnErhRT0CvgAAFl1JREFUDnyq8KRJXufxx/sC/VSXu+5u82bf707hTkTEVarkrx9G\njoQNG6KuJrUp4IlIwpnBYYd56/wlS2D8eOja1efSn3CC72V2443e6n75cnXjTAebNsFZZ8HQoR7s\n7rwztcJdrjZtfDrPxo0e8hYujLqinbv1Vl939/TT6TPiKCKSLL17w88/+/Y7UjBN0RSRyPzyC4wd\n61M433pr+8Lp6tXhgAP8o0WL7V/vv39yuzJK/jZs8C0Qxo+Hf/zD192lui++8DcTqlb16ZqNGkVd\n0W+98QacfjoMGABPPhl1NSIiqWfbNp++fthh3sm7NNvZFE0FPBFJCatW+abq8+b5x1df+eelS7cf\nU7YsNGmSf/irWTO62kuTdet89HXyZG+mctFFUVdUeJ9/7g1/qlf3+hs2jLqi7b7/Hg46yEezp07V\n1EwRkYJcf72/ubh8OdSoEXU10VHAE5G09dNPvoZvx+A3f76vU8pVs+b2sJc3/DVuHE27/kz044/Q\npYuvp3z+ee9olm6mT4cTT/QXBe+95+8ER23LFjj2WN+cffp0Tc0UEdmZ6dOhfXtfr3z++fD738Me\ne0RdVfIp4IlIxtm2DRYt+m3wmzcPVqzYflz58r4Be97gd+CB/pGKa8ZSUU4OjBjh68MWLYJXX/WF\n7ulq2jQPebVr+0hevXrR1nPjjfB//+drUs86K9paRERSXQi+3+rLL/sbveXLwxFH+N/1jh19+mb5\n8lFXmXgKeCJSqqxZ8+vAl/v1ggXb1/k1buwvps86y6fGKez9VggwcSLcfLO/Y9qmDTz+uDcrSXef\nfAKdOkGdOnDLLd52e6+9kl/H2LE+5fWyy2DgwOSfX0QkXf3yC3z4oXdLnjTJ/58KwddaH3ush72O\nHSEWy8z/4xXwRETwqXDffgtTpngL+okTPfA1bbo97LVtm5n/ERTVp596sHvnHV+rds893r0syg3M\nS9rUqb7Nw8KFPo23Y0f44x897CVjTeeiRf7mQqNG3jlT6+5ERIpv9WqflTFxoge++fP9+jp1vMlW\n7ghfKjbZKg4FPBGRfKxa5V24hg3zILNtm+/dlxv22rQpfWFv3jyfijlypE9hvO02378wUzelD8Hf\n9X3tNf/49lsPsSec4GHvjDOgVq2SP+/mzXDMMb4Z+2ef+ZsMIiJSchYv3j66N2kSLFvm1zdpsj3s\nnXBCYv7GJ4MCnojILqxcCaNGwfDh2zdfP+CA7WGvdeuoK0ysxYvh/9u793iryjqP458vkr5Eboog\noHjJS2qlGGg13jAnB6tRx2A0Z/IyXSYzLdOXNZOGWiZjo2neQrPStBQ1lS5ekhBTI3AUQTioeUEx\nBUSz0WxS+M0fz7M7mw0H4/jsy9l836/Xep11nnXO+q61zz57P7+9nrXWGWekm5b36ZPOCzvxROjX\nr9lb1jgR6UqblWLv8cdTsbfffjBuXLo1xODBZbJOPhnOPTc938aPL7NOMzNbvQjo6Ogs9qZNSxdx\nAxg5snM45957pyGePYELPDOztbBkSTqCNXlyutLiihWw886dxd5OOzV7C8tZtgzOPhsuuii9AX72\ns+mctFKFTE8VAbNnp4vLXH89PPYY9OoFY8akguzQQ9Own+6YMgUOPhiOOy497mZm1lhvvJFGb0yd\nmoZ03nvvyhds2X//dJSvlS/Y4gLPzKybnn++s9i7++7U8X/XuzqLvXe8o9lb2D2vvgrnnw/nnJPu\nbXfkkXD66a11b7hWEQFz5nQe2Xv00VTs7bNPZ7E3dOjftq7KeXfbbJM6FD7vzsys+V57Lb0mV87f\nq75gy6GHplsDtRoXeGZmBTz3HNx4Yyr27rknvfjvumsq9MaP7xn3L/vLX+Dyy9NFUxYvTkeSzjqr\n/YeglhKR7ldXKfYWLEjnaVYXe8OGrf53K+fddXSk8+623bax225mZn+bygVbpk5Npy1885vN3qJV\nucAzMyvs2Wc7i717701tu+3WWey1Wud9xQq49lo47bR01ch99oGJE+H972/2lvVcETBvXucwzvnz\nU7G3117pOfDRj8Lw4Z0/f9JJcN556WfHjWvedpuZWc/nAs/MrI6eeSZ18idPhhkzUtuoUamDv/vu\n6ejY0KHNuSJnBNx6azqv7qGH0hHHs8+GsWPXvSuE1tv8+Z1H9ubNS4/vnnumYq5fv3Rj3s99Di68\nsNlbamZmPZ0LPDOzBlm4sLPYmzmzs33gwFTo7bxz+lqZHzasfoXWffele9ndfXe6LPTXvgaHH57O\nH7P66ujoPLI3d25qGzUqHe1t11tOmJlZ47jAMzNrgsWL05GcyjR/fvr64oudPzNwYGfRV138vZXC\n7+GH073spkyBzTaDr34VPvlJWH/9Mvtla+eRR+C229IR3S22aPbWmJlZO3CBZ2bWIiLSbRiqC77K\n12XLOn+uUvjVFn/Dh3dd+C1cCBMmwFVXpSGBp5wCX/gCbLRRY/bNzMzMGqNpBZ6kscAFwHrAdyNi\nYs3yfYDzgV2AwyPihjdbpws8M2tHEelm66sr/F54ofPnBgxYtegbMQImTYJLL03F3/HHw5e/DIMG\nNW9/zMzMrH6aUuBJWg94FPggsAiYBXwsIuZX/czWQH/gZGCKCzwzs1UtWbJq0Td/fioIK3r1gmOO\nSUfwRoxo3raamZlZ/a2pwOtdx9w9gN9FxBN5I64FDgb+WuBFxFN52Yo6boeZWY82ZEiaxoxZuX3p\n0lToPfZYujT/jjs2ZfPMzMyshdSzwNsceKbq+0XAe7uzIkmfBj4NsOWWW771LTMzawODB8O++6bJ\nzMzMDKBHXCw7Ii6LiNERMXrw4MHN3hwzMzMzM7OWVM8C71mg+kyQLXKbmZmZmZmZ1UE9C7xZwPaS\ntpG0PnA4MKWOeWZmZmZmZuu0uhV4EfEG8DngdqADmBwR8ySdKekgAEm7S1oEjAcmSZpXr+0xMzMz\nMzNrd/W8yAoR8QvgFzVtX62an0UaumlmZmZmZmZvUY+4yIqZmZmZmZm9ORd4ZmZmZmZmbcIFnpmZ\nmZmZWZtwgWdmZmZmZtYmXOCZmZmZmZm1CUVEs7dhrUhaCixs9nasxqbAC22U08gs57R+lnNaP8s5\nrZ3TyCzntH6Wc1o/yzmtn9VuOWtrq4gYvLoFPa7Aa1WS7o+I0e2S08gs57R+lnNaP8s5rZ3TyCzn\ntH6Wc1o/yzmtn9VuOSV5iKaZmZmZmVmbcIFnZmZmZmbWJlzglXNZm+U0Mss5rZ/lnNbPck5r5zQy\nyzmtn+Wc1s9yTutntVtOMT4Hz8zMzMzMrE34CJ6ZmZmZmVmbcIFnZmZmZmbWJlzgmZmZmZmZtQkX\neC1O0o6S9pfUt6Z9bOGcPSTtnud3lvRFSR8qmdFF7lX1zsg5e+V9OqDwet8rqX+e31DSGZJ+Kum/\nJA0omHOCpBGl1reGnPUlHSnp7/P3R0i6SNJxkt5Wh7y3SzpZ0gWSzpP0mcrjaWZmZmZrzxdZKUzS\nMRHx/ULrOgE4DugARgKfj4hb8rIHIuI9hXImAAcCvYFfAu8FpgEfBG6PiLMK5UypbQL2A34FEBEH\nlcjJWTMjYo88/ynS43gTcADw04iYWChnHrBrRLwh6TLgT8ANwP65/dBCOS8DrwKPAz8Gro+IpSXW\nXZNzDel50Af4A9AX+AlpfxQRRxXMOgH4CHA38CHgwZz5T8BnI+KuUllmZtbzSRoSEUuavR0lSRoU\nEcuavR2WSOoNfILUFxmem58FbgGuiIjXm7VtayUiPBWcgKcLrmsu0DfPbw3cTyryAB4snLMeqVP/\nR6B/bt8QmFMw5wHgamAMsG/++lye37fw3+HBqvlZwOA8vxEwt2BOR/X+1SybXXJ/SEfcDwCuAJYC\ntwFHAf0K5szJX3sDi4H18vcq+Vyoft7l+T7AXXl+y5LPb089cwKGNHsb6rBPg5q9DZ7++rcYAEwE\nFgAvAstIH6ZOBAY2cDtuLbiu/sDZwA+BI2qWXVIwZyhwKXAxMAg4Pb+eTwaGFczZpGYaBDwFbAxs\nUvjvMLbmuXEFMAf4EbBZwZyJwKZ5fjTwBPA7YGHJflDub50KbFvycVpNzmjSAYGrgRGkgwQv537X\nboWz+gJnAvNyxlJgBnB04Zwf5+f3+4At8vS+3HZdPR/PkpOHaHaDpDldTHOBzQpG9YqIVwAi4ilS\nQXSgpPNIHe5S3oiI5RHxJ+DxiPhjznwNWFEwZzTwP8BXgJcjHaF5LSKmR8T0gjkAvSRtLGkQ6cjT\nUoCIeBV4o2DOw5KOyfMPSRoNIGkHoOSnPBERKyLijoj4BOlTpUuAsaQ3iFJ6SVof6EcquirDTDcA\nig/RJBWSlfX3BYiIp0tmSRogaaKkBZJelLRMUkduG1gq52/YjlsLrqu/pLMl/VDSETXLLimYM1TS\npZIuljRI0umS5kqaLGlYwZxNaqZBwMz8P7xJqZycNbZqfoCkK/Lr948kFXv9zs+vTfP8aElPAL+V\ntFDSvgVzHpB0qqRtS62zi5zRkqZJulrSCEm/lPSypFmSdiuc1VfSmZLm5YylkmZIOrpgzGTgJWBM\nRGwSEYNII0peysuKkfSeLqZRpNE5pXyf1De4EThc0o2SNsjL3lcw5wfAfOAZUuf+NdIojF8D3ymY\n8wKpz1CZ7gc2JxUv9xfMAfhG1fy5pA+f/5FUqEwqmPPhiHghz38TOCwitiONmDq3YM7GwEBgmqSZ\nkk6UNPzNfqkbLgHOAX4O3AdMiogBwJfzspKuIfV3/gE4A/g28HFgP0nfWNMvrqVREXFsRMyIiEV5\nmhERxwJFX+vqqtkVZk+cSEc2RgJb1UxbA78vmPMrYGRNW2/gKmB5wZzfAn3yfK+q9gHUHJUqlLcF\ncD1wEQWPeNZkPEV6IXgyfx2W2/tS9sjaANKb3eP5cXw9500nDdEsldPlEa3K365Qzol5+xcCJwBT\ngctJn85OKPw3+jzpE9LLSZ+iH5PbBwN3F8y5HfgSMLSqbWhuu6PwPr2ni2kU8FzBnBtJnwQfAkzJ\n32+QlxX7nyUdJT6e9GY9Jz9mI3LbLQVzVuT/1erp9cr/b+G/0QNV898Fvp5fv08Ebi6YM7dqfhqw\ne57fAbi/YM6TwH8DTwMz834ML/mY5ZyZpKH8HyN17Mfl9v2B3xTOugU4Or9XfBE4DdgeuBL4RqGM\nR7qzrJtZy0nv59NWM71WMGd2zfdfAe4lHfkq+bpQPULm6TVtw1vMOSm/Br27qu3Jkn+bqvVWvy7U\nPo4l96kD6J3nZ9QsKzm6qHp/9iYVW8/n59ynG/RcKDoSB3io5vtZ+WsvYEHBnBnAeFbuD/cCDgN+\nW3Kf6jk1fQN64kQ6dL9XF8t+VDBnC6o6pTXL9iyYs0EX7ZtWv7DW4XH8cKk367XI7ANsU4f19gd2\nJXXkiw3nqFr/Dg18jIZXOoikTwDHAXvUKeudef071nF/3JHrfo47ct3PcUeu+1l178gBdwCnVL9e\nk0bgfAm4s/D+PAxs38WyZwrmdFR3SnPb0aQhbQvr8fcBvl6zrNhzO6+v8oHweaSRJUU/8KnKWUT6\nMOEk0oecqlpW8lSV4/Nz7wOkoa0XkE5TOQP4YcGcVd4HSKfijAW+XzDnN6RTR8aTPhg+JLfvS8EP\nsvI67yP3vYGDSNeIqCwr9l5OOlhzHbAEeDRPS3Jb8f5jvabK8ChbC5GGyHW17IiulnUjZ9Ealt1b\nMOf/umh/gTREoi4i4uekw/oNE2kY6pN1WO8fgYdKr7dq/Y/Wa92ryfp91fwfSBeNqVfWPFLno54W\nSjoFuDIiFgPkoXhHk45ElNQB/HtEPFa7QFLJrA0k9YqIFQARcZakZ0kXrOm75l9dK9XD+GuveLte\nqZCIOFfSdcC38uM0AYhS668xRNIXSUPZ+ktS5Hd1yl5Z+hLgF5ImArdJuoB0waIPALML5vxVRPwa\n+LWk40lDvg4DLiu0+j8rXYV4ABCSDomIm/Nw0+WFMipelbRXRNwj6SDSOXJExApJpU5POIx0ZHp6\nfj0I0uicKcA/F8qoOJ2un1vHF8z5Ken5dWelISJ+IOl54MKCObdI6hsRr0TEqZVGSdsBjxTMqfSD\nxufnwS9JH9LWw+WkAhLSkeJNgaWShlLw/zUiLsyn8xxLOprfm3R0+mbSaIJSVukzRMRy0gdptxXM\n+QxpiOYK0tDJYyX9gHRRkk8VzIH0mF0uaXtSv+ETAJIGk84HLSIinsqnQp1LGp21I/B+YH5EFO8/\n1ouvomlmbU3SxqSO3MHAkNxc6chNjIiXCmaNI32CvUonp9IhLpRzDml46Z017WOBCyNi+0I5ZwLn\nRD4XuKp9O9JjN65ETs26DwL+E9g6IobWYf0TapouiYhKR+6ciDiyYNYYVu7IPUPqyH0vIoqcCyzp\n2og4vMS63iRnVzo7cieS9usockcuIu4rmLULafhspSP3bxHxaO7IfSwivl0oZ0fSEaIZ1c9xSWMj\nomQnuJK1OWmIV92y1pBzYESUPA+44ftD+iBh24h4uE3/Rj01ZyfSyJ+65lRlbU4d/2e16pXl9wDu\novCV5euu2YcQPXny5KlZE/m8v3bKaocc0hV83+W/kXPqlUU6x/gRUsH9FHBw1bKi5543Kot0NLCd\nchr5N2qrfWrw/ixo4P9R3bNo0JXl6z01fQM8efLkqVkTdbrITzOznNP6Wc5pfhYNug1RI7Oc0/pZ\nzmn9LFY+3/jBmmXFztOu9+Rz8MysrUma09Uiyt7WpGFZzmn9LOe0fNZKtyHKw2lvkLQVZW9D1Mgs\n57R+lnNaP+svkvpEumbDqEqjpAGUvXVYXbnAM7N2txnp5O/ac+1EuipXT8xyTutnOae1sxZLGhkR\nswEi4hVJHwG+B7y7UEajs5zT+lnOaf2sfSJffDDyhcyyt5HOO+4RXOCZWbv7GWlYxypXQpN0Vw/N\nck7rZzmntbOOBFa60E2kC98cKankja0bmeWc1s9yTotnRZOuLF+ar6JpZmZmZmbWJkre88fMzMzM\nzMyayAWemZmZmZlZm3CBZ2ZmVpikMZJ+1uztMDOzdY8LPDMzMzMzszbhAs/MzNZZkv5V0kxJsyVN\nkrSepFckfUvSPElTJQ3OPztS0gxJcyTdJGnj3L6dpDslPSTpAUnb5tX3lXSDpAWSrpFU+r5QZmZm\nq3CBZ2Zm6yRJOwGHAXtGxEhgOfAvwEbA/RHxTmA6MCH/ylXAlyJiF2BuVfs1wMURsSvwd8BzuX03\n4AvAzsDbgT3rvlNmZrbO833wzMxsXbU/MAqYlQ+ubQgsAVYA1+WfuRr4iaQBwMCImJ7brwSul9QP\n2DwibgKIiD8D5PXNjIhF+fvZwNbAPfXfLTMzW5e5wDMzs3WVgCsj4j9WapROq/m57t4wtvqGucvx\ne66ZmTWAh2iamdm6aiowTtIQAEmbSNqK9N44Lv/MEcA9EfEy8JKkvXP7x4HpEfG/wCJJh+R1bCCp\nT0P3wszMrIo/TTQzs3VSRMyXdCpwh6RewOvAccCrwB552RLSeXoARwHfyQXcE8Axuf3jwCRJZ+Z1\njG/gbpiZma1EEd0deWJmZtZ+JL0SEX2bvR1mZmbd4SGaZmZmZmZmbcJH8MzMzMzMzNqEj+CZmZmZ\nmZm1CRd4ZmZmZmZmbcIFnpmZmZmZWZtwgWdmZmZmZtYmXOCZmZmZmZm1if8HGYqYSlcspHEAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4PM4STC5Q3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3cf6b05-680c-4f04-d170-14ed25eae794"
      },
      "source": [
        "print(max(accs[5:]))\n",
        "# for acc in accs:\n",
        "#   print(acc)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9jhBOIcqT-h",
        "colab_type": "text"
      },
      "source": [
        "# Mean Teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxPsJI6lMwsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "teacher = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "student.compile(loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'], \n",
        "                optimizer=Adam())\n",
        "teacher.compile(loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'], \n",
        "                optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKFkigJMwpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_teacher_train(iterations, batch_size, save_interval, alpha, iter_epochs):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # -------------------------\n",
        "        #  Train the model\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs_labeled, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Get unlabeled examples\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "        # Train on labeled examples\n",
        "        # loss_labeled_classification, acc_labeled_classification = student.train_on_batch(imgs_labeled, labels)\n",
        "        datagen.fit(imgs_labeled)\n",
        "        student.fit_generator(datagen.flow(imgs_labeled, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        loss_labeled_classification, acc_labeled_classification = history.losses[-1], history.accs[-1]\n",
        "        pred_teacher_labeled = teacher.predict(imgs_labeled)\n",
        "        loss_labeled_consistency, acc_labeled_consistency = student.train_on_batch(imgs_labeled, pred_teacher_labeled)\n",
        "\n",
        "        # Train on unlabeled examples\n",
        "        pred_teacher_unlabeled = teacher.predict(imgs_unlabeled)\n",
        "        loss_unlabeled_consistency, acc_unlabeled_consistency = student.train_on_batch(imgs_unlabeled, pred_teacher_unlabeled)\n",
        "\n",
        "        # Update teacher model\n",
        "        teacher_weights_this = teacher.get_weights()\n",
        "        student_weights_this = student.get_weights()\n",
        "        for i in range(len(teacher_weights_this)):\n",
        "          teacher_weights_this[i] = alpha * teacher_weights_this[i] + (1-alpha) * student_weights_this[i]\n",
        "        # teacher_weights_this = alpha * teacher_weights_this + (1-alpha) * student_weights_this\n",
        "        teacher_weights_last = teacher_weights_this\n",
        "        teacher.set_weights(teacher_weights_this)\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "          # Save losses\n",
        "          supervised_losses.append(loss_labeled_classification)\n",
        "          unsupervised_losses.append(loss_labeled_consistency + loss_unlabeled_consistency)\n",
        "          labeled_consistency_costs.append(loss_labeled_consistency)\n",
        "          unlabeled_consistency_costs.append(loss_unlabeled_consistency)\n",
        "          accs_supervised.append(acc_labeled_classification)\n",
        "          accs_unsupervised.append((acc_labeled_consistency + acc_unlabeled_consistency)/2.0)\n",
        "          accs_labeled_consistency.append(acc_labeled_consistency)\n",
        "          accs_unlabeled_consistency.append(acc_unlabeled_consistency)\n",
        "\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [supervised loss: %.4f, acc: %.2f%%] [unsupervised loss: %.4f, acc: %.2f%%] [labeled consistency loss: %.4f, acc:acc: %.2f%%] [unlabeled consistency loss: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, loss_labeled_classification, 100 * acc_labeled_classification, \n",
        "                 loss_labeled_consistency + loss_unlabeled_consistency, 100 * ((acc_labeled_consistency + acc_unlabeled_consistency)/2.0), \n",
        "                 loss_labeled_consistency, 100 * acc_labeled_consistency, \n",
        "                  loss_unlabeled_consistency, 100 * acc_unlabeled_consistency))\n",
        "          \n",
        "          student.save(\"./models/models-label-\" + str(num_labeled) + \"/student-\" + str(iteration+1) + \".h5\")\n",
        "          teacher.save(\"./models/models-label-\" + str(num_labeled) + \"/teacher-\" + str(iteration+1) + \".h5\")\n",
        "          file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_supervised_losses.json\"\n",
        "          file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_unsupervised_losses.json\"\n",
        "          file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_labeled_consistency_costs.json\"\n",
        "          file4 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_unlabeled_consistency_costs.json\"\n",
        "          with open(file1, 'w') as json_file:\n",
        "                json.dump(str(supervised_losses), json_file)\n",
        "          with open(file2, 'w') as json_file:\n",
        "                json.dump(str(unsupervised_losses), json_file)\n",
        "          with open(file3, 'w') as json_file:\n",
        "                json.dump(str(labeled_consistency_costs), json_file)\n",
        "          with open(file4, 'w') as json_file:\n",
        "                json.dump(str(unlabeled_consistency_costs), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS6N6-dRMwnX",
        "colab_type": "code",
        "outputId": "72f9f145-9af0-4df4-ae83-b64d4c3b7f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 50 # 8000\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "alpha = 0.5\n",
        "iter_epochs = 10\n",
        "\n",
        "supervised_losses = [] # classification cost\n",
        "unsupervised_losses = [] # consistency cost\n",
        "labeled_consistency_costs = []\n",
        "unlabeled_consistency_costs = []\n",
        "accs_supervised = []\n",
        "accs_unsupervised = []\n",
        "accs_labeled_consistency = []\n",
        "accs_unlabeled_consistency = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "student = load_model(\"./models/cifar10_model.019.h5\")\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the mean teacher for the specified number of iterations\n",
        "mean_teacher_train(iterations, batch_size, save_interval, alpha, iter_epochs)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 36s 36s/step - loss: 0.5969 - acc: 0.8438 - val_loss: 0.6970 - val_acc: 0.8356\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.4257 - acc: 0.9375 - val_loss: 0.6920 - val_acc: 0.8357\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.5425 - acc: 0.9062 - val_loss: 0.7067 - val_acc: 0.8298\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.4025 - acc: 0.9375 - val_loss: 0.7127 - val_acc: 0.8291\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.4850 - acc: 0.9062 - val_loss: 0.7259 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3556 - acc: 0.9688 - val_loss: 0.7440 - val_acc: 0.8214\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3668 - acc: 0.9062 - val_loss: 0.7632 - val_acc: 0.8160\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3197 - acc: 1.0000 - val_loss: 0.7764 - val_acc: 0.8146\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2886 - acc: 0.9688 - val_loss: 0.7905 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3027 - acc: 1.0000 - val_loss: 0.8063 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "1 [supervised loss: 0.3027, acc: 100.00%] [unsupervised loss: 5.2786, acc: 67.19%] [labeled consistency loss: 2.6615, acc:acc: 65.62%] [unlabeled consistency loss: 2.6171, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4368 - acc: 0.9062 - val_loss: 0.7719 - val_acc: 0.8176\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3010 - acc: 0.9688 - val_loss: 0.7892 - val_acc: 0.8123\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4120 - acc: 0.9375 - val_loss: 0.8140 - val_acc: 0.8035\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3708 - acc: 0.9375 - val_loss: 0.8469 - val_acc: 0.7975\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.3382 - acc: 0.9688 - val_loss: 0.8757 - val_acc: 0.7885\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3460 - acc: 0.9688 - val_loss: 0.9047 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3120 - acc: 0.9688 - val_loss: 0.9259 - val_acc: 0.7731\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2652 - acc: 1.0000 - val_loss: 0.9430 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2809 - acc: 1.0000 - val_loss: 0.9490 - val_acc: 0.7643\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2746 - acc: 1.0000 - val_loss: 0.9465 - val_acc: 0.7645\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "2 [supervised loss: 0.2746, acc: 100.00%] [unsupervised loss: 2.0619, acc: 82.81%] [labeled consistency loss: 0.7934, acc:acc: 90.62%] [unlabeled consistency loss: 1.2686, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.8175 - acc: 0.8125 - val_loss: 0.9431 - val_acc: 0.7655\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.8352 - acc: 0.8438 - val_loss: 0.9298 - val_acc: 0.7685\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6709 - acc: 0.8125 - val_loss: 0.9088 - val_acc: 0.7730\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6483 - acc: 0.8125 - val_loss: 0.8926 - val_acc: 0.7754\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.4589 - acc: 0.9062 - val_loss: 0.8766 - val_acc: 0.7771\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.4303 - acc: 0.9062 - val_loss: 0.8723 - val_acc: 0.7783\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3596 - acc: 0.9688 - val_loss: 0.8789 - val_acc: 0.7762\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3002 - acc: 0.9688 - val_loss: 0.8969 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3212 - acc: 0.9688 - val_loss: 0.9199 - val_acc: 0.7688\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2955 - acc: 1.0000 - val_loss: 0.9474 - val_acc: 0.7650\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "3 [supervised loss: 0.2955, acc: 100.00%] [unsupervised loss: 1.9075, acc: 87.50%] [labeled consistency loss: 1.0450, acc:acc: 87.50%] [unlabeled consistency loss: 0.8625, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.7552 - acc: 0.9062 - val_loss: 0.9973 - val_acc: 0.7539\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6942 - acc: 0.8750 - val_loss: 1.0016 - val_acc: 0.7535\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5379 - acc: 0.8438 - val_loss: 0.9924 - val_acc: 0.7565\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5020 - acc: 0.9062 - val_loss: 0.9844 - val_acc: 0.7574\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4335 - acc: 0.9062 - val_loss: 0.9752 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3019 - acc: 1.0000 - val_loss: 0.9688 - val_acc: 0.7609\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3366 - acc: 0.9688 - val_loss: 0.9667 - val_acc: 0.7608\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2458 - acc: 1.0000 - val_loss: 0.9717 - val_acc: 0.7584\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2390 - acc: 1.0000 - val_loss: 0.9809 - val_acc: 0.7554\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2629 - acc: 1.0000 - val_loss: 0.9918 - val_acc: 0.7530\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "4 [supervised loss: 0.2629, acc: 100.00%] [unsupervised loss: 1.6383, acc: 93.75%] [labeled consistency loss: 0.8507, acc:acc: 100.00%] [unlabeled consistency loss: 0.7876, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7353 - acc: 0.8125 - val_loss: 1.0272 - val_acc: 0.7467\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6607 - acc: 0.8438 - val_loss: 1.0232 - val_acc: 0.7472\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5065 - acc: 0.8750 - val_loss: 1.0124 - val_acc: 0.7491\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5985 - acc: 0.8750 - val_loss: 0.9847 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5326 - acc: 0.8750 - val_loss: 0.9546 - val_acc: 0.7640\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3813 - acc: 0.9688 - val_loss: 0.9304 - val_acc: 0.7683\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3787 - acc: 0.9375 - val_loss: 0.9129 - val_acc: 0.7715\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.3256 - acc: 0.9688 - val_loss: 0.9030 - val_acc: 0.7726\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.2967 - acc: 0.9688 - val_loss: 0.8986 - val_acc: 0.7731\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2560 - acc: 1.0000 - val_loss: 0.8956 - val_acc: 0.7731\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "5 [supervised loss: 0.2560, acc: 100.00%] [unsupervised loss: 2.1391, acc: 85.94%] [labeled consistency loss: 1.1135, acc:acc: 87.50%] [unlabeled consistency loss: 1.0256, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6400 - acc: 0.8438 - val_loss: 0.8787 - val_acc: 0.7785\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5082 - acc: 0.8750 - val_loss: 0.8667 - val_acc: 0.7827\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4073 - acc: 0.9375 - val_loss: 0.8526 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4155 - acc: 0.9375 - val_loss: 0.8406 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4250 - acc: 0.9375 - val_loss: 0.8304 - val_acc: 0.7926\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3456 - acc: 0.9688 - val_loss: 0.8262 - val_acc: 0.7942\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3288 - acc: 0.9688 - val_loss: 0.8268 - val_acc: 0.7949\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2992 - acc: 1.0000 - val_loss: 0.8292 - val_acc: 0.7929\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2476 - acc: 1.0000 - val_loss: 0.8329 - val_acc: 0.7926\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2383 - acc: 1.0000 - val_loss: 0.8389 - val_acc: 0.7923\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "6 [supervised loss: 0.2383, acc: 100.00%] [unsupervised loss: 2.3948, acc: 76.56%] [labeled consistency loss: 1.4320, acc:acc: 78.12%] [unlabeled consistency loss: 0.9628, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4805 - acc: 0.9375 - val_loss: 0.8450 - val_acc: 0.7906\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4955 - acc: 0.8750 - val_loss: 0.8502 - val_acc: 0.7922\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5490 - acc: 0.9375 - val_loss: 0.8592 - val_acc: 0.7919\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6933 - acc: 0.8438 - val_loss: 0.8687 - val_acc: 0.7898\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4038 - acc: 0.9375 - val_loss: 0.8893 - val_acc: 0.7846\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3819 - acc: 0.9375 - val_loss: 0.9127 - val_acc: 0.7771\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2855 - acc: 0.9688 - val_loss: 0.9376 - val_acc: 0.7724\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3003 - acc: 0.9688 - val_loss: 0.9624 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2466 - acc: 1.0000 - val_loss: 0.9908 - val_acc: 0.7603\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2979 - acc: 0.9688 - val_loss: 1.0220 - val_acc: 0.7541\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "7 [supervised loss: 0.2979, acc: 96.88%] [unsupervised loss: 2.2165, acc: 78.12%] [labeled consistency loss: 1.3254, acc:acc: 81.25%] [unlabeled consistency loss: 0.8911, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.9259 - acc: 0.7500 - val_loss: 1.1013 - val_acc: 0.7371\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0633 - acc: 0.7812 - val_loss: 1.1039 - val_acc: 0.7345\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.7397 - acc: 0.8125 - val_loss: 1.1055 - val_acc: 0.7334\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6889 - acc: 0.7500 - val_loss: 1.1034 - val_acc: 0.7321\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5466 - acc: 0.8438 - val_loss: 1.1206 - val_acc: 0.7270\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4147 - acc: 0.8750 - val_loss: 1.1475 - val_acc: 0.7223\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3782 - acc: 0.9688 - val_loss: 1.1825 - val_acc: 0.7118\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3280 - acc: 1.0000 - val_loss: 1.2168 - val_acc: 0.7021\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3191 - acc: 0.9688 - val_loss: 1.2581 - val_acc: 0.6910\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3014 - acc: 1.0000 - val_loss: 1.3012 - val_acc: 0.6817\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "8 [supervised loss: 0.3014, acc: 100.00%] [unsupervised loss: 2.8035, acc: 75.00%] [labeled consistency loss: 1.8399, acc:acc: 71.88%] [unlabeled consistency loss: 0.9636, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5233 - acc: 0.8750 - val_loss: 1.3832 - val_acc: 0.6672\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5606 - acc: 0.8750 - val_loss: 1.3954 - val_acc: 0.6645\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5029 - acc: 0.8750 - val_loss: 1.4047 - val_acc: 0.6616\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4044 - acc: 0.9375 - val_loss: 1.4070 - val_acc: 0.6609\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3232 - acc: 1.0000 - val_loss: 1.4126 - val_acc: 0.6607\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2963 - acc: 0.9688 - val_loss: 1.4204 - val_acc: 0.6587\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2694 - acc: 1.0000 - val_loss: 1.4326 - val_acc: 0.6585\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2353 - acc: 1.0000 - val_loss: 1.4445 - val_acc: 0.6569\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2381 - acc: 1.0000 - val_loss: 1.4563 - val_acc: 0.6555\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2504 - acc: 1.0000 - val_loss: 1.4716 - val_acc: 0.6526\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "9 [supervised loss: 0.2504, acc: 100.00%] [unsupervised loss: 2.0921, acc: 76.56%] [labeled consistency loss: 0.8778, acc:acc: 87.50%] [unlabeled consistency loss: 1.2143, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7997 - acc: 0.7188 - val_loss: 1.5107 - val_acc: 0.6484\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6774 - acc: 0.8438 - val_loss: 1.5071 - val_acc: 0.6485\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7117 - acc: 0.7500 - val_loss: 1.4872 - val_acc: 0.6526\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5618 - acc: 0.9062 - val_loss: 1.4675 - val_acc: 0.6574\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5295 - acc: 0.8438 - val_loss: 1.4636 - val_acc: 0.6591\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4313 - acc: 0.9375 - val_loss: 1.4757 - val_acc: 0.6585\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3351 - acc: 0.9688 - val_loss: 1.4905 - val_acc: 0.6559\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2857 - acc: 1.0000 - val_loss: 1.5072 - val_acc: 0.6533\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2629 - acc: 1.0000 - val_loss: 1.5231 - val_acc: 0.6507\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2543 - acc: 1.0000 - val_loss: 1.5417 - val_acc: 0.6478\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "10 [supervised loss: 0.2543, acc: 100.00%] [unsupervised loss: 3.1589, acc: 65.62%] [labeled consistency loss: 1.8077, acc:acc: 68.75%] [unlabeled consistency loss: 1.3511, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0221 - acc: 0.8125 - val_loss: 1.5181 - val_acc: 0.6528\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0368 - acc: 0.7500 - val_loss: 1.4376 - val_acc: 0.6670\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6929 - acc: 0.8750 - val_loss: 1.3552 - val_acc: 0.6833\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6416 - acc: 0.8438 - val_loss: 1.2866 - val_acc: 0.6976\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4054 - acc: 0.9375 - val_loss: 1.2417 - val_acc: 0.7091\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3924 - acc: 0.9062 - val_loss: 1.2124 - val_acc: 0.7144\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3082 - acc: 1.0000 - val_loss: 1.2087 - val_acc: 0.7153\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3230 - acc: 0.9688 - val_loss: 1.2174 - val_acc: 0.7094\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2547 - acc: 1.0000 - val_loss: 1.2355 - val_acc: 0.7026\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2425 - acc: 1.0000 - val_loss: 1.2615 - val_acc: 0.6968\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "11 [supervised loss: 0.2425, acc: 100.00%] [unsupervised loss: 2.6163, acc: 82.81%] [labeled consistency loss: 1.6815, acc:acc: 78.12%] [unlabeled consistency loss: 0.9348, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4125 - acc: 0.9375 - val_loss: 1.3263 - val_acc: 0.6813\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4816 - acc: 0.9375 - val_loss: 1.3274 - val_acc: 0.6817\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4470 - acc: 0.9062 - val_loss: 1.3004 - val_acc: 0.6841\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4214 - acc: 0.9375 - val_loss: 1.2587 - val_acc: 0.6883\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3725 - acc: 0.9375 - val_loss: 1.2120 - val_acc: 0.6976\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3388 - acc: 0.9688 - val_loss: 1.1676 - val_acc: 0.7041\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2678 - acc: 1.0000 - val_loss: 1.1333 - val_acc: 0.7090\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2491 - acc: 1.0000 - val_loss: 1.1063 - val_acc: 0.7160\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2658 - acc: 1.0000 - val_loss: 1.0770 - val_acc: 0.7243\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2232 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.7316\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "12 [supervised loss: 0.2232, acc: 100.00%] [unsupervised loss: 2.7832, acc: 75.00%] [labeled consistency loss: 1.6984, acc:acc: 78.12%] [unlabeled consistency loss: 1.0848, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5845 - acc: 0.8125 - val_loss: 1.0784 - val_acc: 0.7271\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5068 - acc: 0.9375 - val_loss: 1.0724 - val_acc: 0.7284\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5004 - acc: 0.9062 - val_loss: 1.0656 - val_acc: 0.7302\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4898 - acc: 0.9062 - val_loss: 1.0524 - val_acc: 0.7343\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4523 - acc: 0.9375 - val_loss: 1.0397 - val_acc: 0.7372\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4074 - acc: 0.9375 - val_loss: 1.0258 - val_acc: 0.7430\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3505 - acc: 0.9688 - val_loss: 1.0130 - val_acc: 0.7471\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3406 - acc: 0.9688 - val_loss: 1.0031 - val_acc: 0.7492\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3020 - acc: 1.0000 - val_loss: 0.9952 - val_acc: 0.7509\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3108 - acc: 0.9688 - val_loss: 0.9925 - val_acc: 0.7526\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "13 [supervised loss: 0.3108, acc: 96.88%] [unsupervised loss: 1.7711, acc: 89.06%] [labeled consistency loss: 0.8572, acc:acc: 87.50%] [unlabeled consistency loss: 0.9139, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6306 - acc: 0.8125 - val_loss: 1.0053 - val_acc: 0.7505\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5699 - acc: 0.8750 - val_loss: 0.9997 - val_acc: 0.7536\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6250 - acc: 0.8750 - val_loss: 0.9854 - val_acc: 0.7554\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5368 - acc: 0.8438 - val_loss: 0.9638 - val_acc: 0.7609\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4659 - acc: 0.9375 - val_loss: 0.9436 - val_acc: 0.7664\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3793 - acc: 0.9688 - val_loss: 0.9280 - val_acc: 0.7710\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3320 - acc: 0.9688 - val_loss: 0.9160 - val_acc: 0.7744\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3060 - acc: 0.9688 - val_loss: 0.9067 - val_acc: 0.7752\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2990 - acc: 1.0000 - val_loss: 0.8999 - val_acc: 0.7770\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2776 - acc: 1.0000 - val_loss: 0.8963 - val_acc: 0.7775\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "14 [supervised loss: 0.2776, acc: 100.00%] [unsupervised loss: 2.4271, acc: 81.25%] [labeled consistency loss: 1.6842, acc:acc: 68.75%] [unlabeled consistency loss: 0.7429, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5280 - acc: 0.8750 - val_loss: 0.8945 - val_acc: 0.7786\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4807 - acc: 0.9062 - val_loss: 0.8909 - val_acc: 0.7783\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4179 - acc: 0.9062 - val_loss: 0.8890 - val_acc: 0.7792\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4355 - acc: 0.8750 - val_loss: 0.8904 - val_acc: 0.7780\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3260 - acc: 1.0000 - val_loss: 0.8938 - val_acc: 0.7769\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3577 - acc: 0.9375 - val_loss: 0.8974 - val_acc: 0.7751\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3072 - acc: 0.9375 - val_loss: 0.9042 - val_acc: 0.7739\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2517 - acc: 1.0000 - val_loss: 0.9141 - val_acc: 0.7706\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 0.9262 - val_acc: 0.7678\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2419 - acc: 1.0000 - val_loss: 0.9395 - val_acc: 0.7651\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "15 [supervised loss: 0.2419, acc: 100.00%] [unsupervised loss: 1.5973, acc: 89.06%] [labeled consistency loss: 0.7456, acc:acc: 87.50%] [unlabeled consistency loss: 0.8517, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4968 - acc: 0.9375 - val_loss: 0.9688 - val_acc: 0.7551\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5839 - acc: 0.8438 - val_loss: 0.9688 - val_acc: 0.7554\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5439 - acc: 0.8750 - val_loss: 0.9640 - val_acc: 0.7561\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5849 - acc: 0.9062 - val_loss: 0.9583 - val_acc: 0.7567\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4085 - acc: 0.9375 - val_loss: 0.9561 - val_acc: 0.7582\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2788 - acc: 1.0000 - val_loss: 0.9625 - val_acc: 0.7561\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2935 - acc: 1.0000 - val_loss: 0.9771 - val_acc: 0.7519\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3042 - acc: 0.9688 - val_loss: 1.0007 - val_acc: 0.7470\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2879 - acc: 0.9688 - val_loss: 1.0305 - val_acc: 0.7385\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2696 - acc: 1.0000 - val_loss: 1.0649 - val_acc: 0.7290\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "16 [supervised loss: 0.2696, acc: 100.00%] [unsupervised loss: 1.9240, acc: 81.25%] [labeled consistency loss: 1.0356, acc:acc: 87.50%] [unlabeled consistency loss: 0.8884, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4882 - acc: 0.8750 - val_loss: 1.0674 - val_acc: 0.7260\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5083 - acc: 0.8438 - val_loss: 1.0646 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4478 - acc: 0.9062 - val_loss: 1.0616 - val_acc: 0.7280\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4274 - acc: 0.9062 - val_loss: 1.0570 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4748 - acc: 0.9062 - val_loss: 1.0526 - val_acc: 0.7324\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3680 - acc: 0.9688 - val_loss: 1.0448 - val_acc: 0.7341\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3397 - acc: 0.9375 - val_loss: 1.0382 - val_acc: 0.7380\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3313 - acc: 0.9688 - val_loss: 1.0323 - val_acc: 0.7391\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2897 - acc: 0.9688 - val_loss: 1.0249 - val_acc: 0.7406\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2485 - acc: 1.0000 - val_loss: 1.0197 - val_acc: 0.7414\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "17 [supervised loss: 0.2485, acc: 100.00%] [unsupervised loss: 1.7083, acc: 84.38%] [labeled consistency loss: 0.8775, acc:acc: 90.62%] [unlabeled consistency loss: 0.8309, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4480 - acc: 0.9062 - val_loss: 1.0069 - val_acc: 0.7441\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4008 - acc: 0.9688 - val_loss: 0.9999 - val_acc: 0.7468\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4634 - acc: 0.9375 - val_loss: 0.9868 - val_acc: 0.7517\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3338 - acc: 1.0000 - val_loss: 0.9710 - val_acc: 0.7571\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3168 - acc: 1.0000 - val_loss: 0.9542 - val_acc: 0.7621\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2897 - acc: 1.0000 - val_loss: 0.9420 - val_acc: 0.7666\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2541 - acc: 1.0000 - val_loss: 0.9324 - val_acc: 0.7696\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2657 - acc: 1.0000 - val_loss: 0.9268 - val_acc: 0.7719\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2546 - acc: 1.0000 - val_loss: 0.9244 - val_acc: 0.7724\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2399 - acc: 1.0000 - val_loss: 0.9245 - val_acc: 0.7715\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "18 [supervised loss: 0.2399, acc: 100.00%] [unsupervised loss: 1.7282, acc: 85.94%] [labeled consistency loss: 0.8957, acc:acc: 87.50%] [unlabeled consistency loss: 0.8325, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4985 - acc: 0.9375 - val_loss: 0.9511 - val_acc: 0.7637\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5178 - acc: 0.9062 - val_loss: 0.9501 - val_acc: 0.7634\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3763 - acc: 0.9375 - val_loss: 0.9439 - val_acc: 0.7637\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3494 - acc: 0.9688 - val_loss: 0.9368 - val_acc: 0.7651\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3835 - acc: 0.9375 - val_loss: 0.9309 - val_acc: 0.7651\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3522 - acc: 0.9688 - val_loss: 0.9289 - val_acc: 0.7665\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3166 - acc: 0.9375 - val_loss: 0.9314 - val_acc: 0.7643\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2319 - acc: 1.0000 - val_loss: 0.9384 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2456 - acc: 1.0000 - val_loss: 0.9489 - val_acc: 0.7581\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2333 - acc: 1.0000 - val_loss: 0.9618 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "19 [supervised loss: 0.2333, acc: 100.00%] [unsupervised loss: 1.7877, acc: 85.94%] [labeled consistency loss: 0.9354, acc:acc: 87.50%] [unlabeled consistency loss: 0.8523, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5686 - acc: 0.8438 - val_loss: 0.9767 - val_acc: 0.7528\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4320 - acc: 0.9062 - val_loss: 0.9686 - val_acc: 0.7536\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4481 - acc: 0.9062 - val_loss: 0.9582 - val_acc: 0.7564\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3942 - acc: 0.9062 - val_loss: 0.9583 - val_acc: 0.7563\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2753 - acc: 1.0000 - val_loss: 0.9706 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3327 - acc: 1.0000 - val_loss: 0.9921 - val_acc: 0.7495\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2534 - acc: 1.0000 - val_loss: 1.0181 - val_acc: 0.7446\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2631 - acc: 1.0000 - val_loss: 1.0476 - val_acc: 0.7379\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2425 - acc: 1.0000 - val_loss: 1.0808 - val_acc: 0.7309\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2591 - acc: 1.0000 - val_loss: 1.1180 - val_acc: 0.7224\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "20 [supervised loss: 0.2591, acc: 100.00%] [unsupervised loss: 1.9847, acc: 89.06%] [labeled consistency loss: 1.0792, acc:acc: 90.62%] [unlabeled consistency loss: 0.9055, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6535 - acc: 0.9062 - val_loss: 1.1766 - val_acc: 0.7083\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5739 - acc: 0.9062 - val_loss: 1.1664 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5931 - acc: 0.8750 - val_loss: 1.1190 - val_acc: 0.7205\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5059 - acc: 0.9062 - val_loss: 1.0701 - val_acc: 0.7309\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4523 - acc: 0.9375 - val_loss: 1.0262 - val_acc: 0.7428\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4043 - acc: 0.9375 - val_loss: 0.9840 - val_acc: 0.7502\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3241 - acc: 0.9688 - val_loss: 0.9460 - val_acc: 0.7595\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3197 - acc: 0.9688 - val_loss: 0.9153 - val_acc: 0.7693\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2780 - acc: 1.0000 - val_loss: 0.8912 - val_acc: 0.7745\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2598 - acc: 1.0000 - val_loss: 0.8725 - val_acc: 0.7820\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "21 [supervised loss: 0.2598, acc: 100.00%] [unsupervised loss: 1.9072, acc: 84.38%] [labeled consistency loss: 1.2057, acc:acc: 75.00%] [unlabeled consistency loss: 0.7015, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3941 - acc: 0.9688 - val_loss: 0.8567 - val_acc: 0.7833\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3476 - acc: 0.9375 - val_loss: 0.8531 - val_acc: 0.7842\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5456 - acc: 0.9062 - val_loss: 0.8493 - val_acc: 0.7852\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3728 - acc: 0.9375 - val_loss: 0.8444 - val_acc: 0.7863\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3646 - acc: 0.9688 - val_loss: 0.8423 - val_acc: 0.7871\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3180 - acc: 0.9062 - val_loss: 0.8450 - val_acc: 0.7873\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2673 - acc: 0.9688 - val_loss: 0.8512 - val_acc: 0.7857\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2618 - acc: 1.0000 - val_loss: 0.8601 - val_acc: 0.7840\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2478 - acc: 1.0000 - val_loss: 0.8714 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2554 - acc: 1.0000 - val_loss: 0.8841 - val_acc: 0.7784\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "22 [supervised loss: 0.2554, acc: 100.00%] [unsupervised loss: 2.1230, acc: 78.12%] [labeled consistency loss: 1.0417, acc:acc: 90.62%] [unlabeled consistency loss: 1.0813, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4343 - acc: 0.9375 - val_loss: 0.8889 - val_acc: 0.7755\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4799 - acc: 0.9062 - val_loss: 0.8899 - val_acc: 0.7744\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4209 - acc: 0.9062 - val_loss: 0.8900 - val_acc: 0.7751\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3583 - acc: 0.9688 - val_loss: 0.8896 - val_acc: 0.7766\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4442 - acc: 0.9062 - val_loss: 0.8927 - val_acc: 0.7758\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3158 - acc: 0.9688 - val_loss: 0.8995 - val_acc: 0.7738\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2688 - acc: 1.0000 - val_loss: 0.9095 - val_acc: 0.7721\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2612 - acc: 1.0000 - val_loss: 0.9223 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2357 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 0.7665\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2308 - acc: 1.0000 - val_loss: 0.9539 - val_acc: 0.7628\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "23 [supervised loss: 0.2308, acc: 100.00%] [unsupervised loss: 1.8408, acc: 92.19%] [labeled consistency loss: 1.1059, acc:acc: 90.62%] [unlabeled consistency loss: 0.7349, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4428 - acc: 0.9062 - val_loss: 0.9644 - val_acc: 0.7613\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4162 - acc: 0.9375 - val_loss: 0.9710 - val_acc: 0.7602\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3509 - acc: 0.9688 - val_loss: 0.9784 - val_acc: 0.7603\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3671 - acc: 0.9375 - val_loss: 0.9820 - val_acc: 0.7596\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2769 - acc: 0.9688 - val_loss: 0.9863 - val_acc: 0.7571\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2930 - acc: 0.9688 - val_loss: 0.9880 - val_acc: 0.7565\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2450 - acc: 1.0000 - val_loss: 0.9873 - val_acc: 0.7559\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2939 - acc: 0.9688 - val_loss: 0.9869 - val_acc: 0.7541\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2342 - acc: 1.0000 - val_loss: 0.9867 - val_acc: 0.7536\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2262 - acc: 1.0000 - val_loss: 0.9871 - val_acc: 0.7533\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "24 [supervised loss: 0.2262, acc: 100.00%] [unsupervised loss: 1.2340, acc: 95.31%] [labeled consistency loss: 0.6016, acc:acc: 93.75%] [unlabeled consistency loss: 0.6324, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8754 - acc: 0.8438 - val_loss: 0.9935 - val_acc: 0.7498\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5345 - acc: 0.8438 - val_loss: 1.0101 - val_acc: 0.7465\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5029 - acc: 0.9062 - val_loss: 1.0380 - val_acc: 0.7402\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4885 - acc: 0.9062 - val_loss: 1.0765 - val_acc: 0.7317\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4751 - acc: 0.9375 - val_loss: 1.1308 - val_acc: 0.7205\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3208 - acc: 0.9688 - val_loss: 1.1961 - val_acc: 0.7042\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3061 - acc: 1.0000 - val_loss: 1.2658 - val_acc: 0.6917\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2621 - acc: 1.0000 - val_loss: 1.3397 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2556 - acc: 1.0000 - val_loss: 1.4115 - val_acc: 0.6634\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2916 - acc: 0.9375 - val_loss: 1.4813 - val_acc: 0.6538\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "25 [supervised loss: 0.2916, acc: 93.75%] [unsupervised loss: 1.5398, acc: 89.06%] [labeled consistency loss: 0.9098, acc:acc: 84.38%] [unlabeled consistency loss: 0.6300, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5381 - acc: 0.8750 - val_loss: 1.5833 - val_acc: 0.6352\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5496 - acc: 0.9062 - val_loss: 1.5786 - val_acc: 0.6347\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5493 - acc: 0.8750 - val_loss: 1.5360 - val_acc: 0.6418\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5513 - acc: 0.9062 - val_loss: 1.4744 - val_acc: 0.6526\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4785 - acc: 0.9062 - val_loss: 1.4115 - val_acc: 0.6630\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3539 - acc: 0.9688 - val_loss: 1.3564 - val_acc: 0.6727\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4048 - acc: 0.9375 - val_loss: 1.2992 - val_acc: 0.6842\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3397 - acc: 0.9375 - val_loss: 1.2519 - val_acc: 0.6939\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2534 - acc: 1.0000 - val_loss: 1.2187 - val_acc: 0.7000\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2842 - acc: 1.0000 - val_loss: 1.1959 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "26 [supervised loss: 0.2842, acc: 100.00%] [unsupervised loss: 1.9484, acc: 85.94%] [labeled consistency loss: 1.2902, acc:acc: 75.00%] [unlabeled consistency loss: 0.6583, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.9695 - acc: 0.7500 - val_loss: 1.2115 - val_acc: 0.7011\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7191 - acc: 0.8438 - val_loss: 1.2337 - val_acc: 0.6951\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7172 - acc: 0.7812 - val_loss: 1.2614 - val_acc: 0.6920\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5662 - acc: 0.8438 - val_loss: 1.2993 - val_acc: 0.6829\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3492 - acc: 0.9688 - val_loss: 1.3384 - val_acc: 0.6710\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3349 - acc: 0.9375 - val_loss: 1.3845 - val_acc: 0.6593\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3406 - acc: 0.9375 - val_loss: 1.4324 - val_acc: 0.6479\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2608 - acc: 1.0000 - val_loss: 1.4758 - val_acc: 0.6398\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2755 - acc: 1.0000 - val_loss: 1.5203 - val_acc: 0.6303\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2793 - acc: 0.9688 - val_loss: 1.5738 - val_acc: 0.6210\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "27 [supervised loss: 0.2793, acc: 96.88%] [unsupervised loss: 2.9616, acc: 70.31%] [labeled consistency loss: 2.0147, acc:acc: 71.88%] [unlabeled consistency loss: 0.9469, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0793 - acc: 0.7188 - val_loss: 1.7439 - val_acc: 0.5951\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0464 - acc: 0.7188 - val_loss: 1.7226 - val_acc: 0.5982\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8689 - acc: 0.8125 - val_loss: 1.6598 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5916 - acc: 0.8750 - val_loss: 1.5965 - val_acc: 0.6212\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5939 - acc: 0.8750 - val_loss: 1.5486 - val_acc: 0.6324\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3727 - acc: 0.9688 - val_loss: 1.5231 - val_acc: 0.6398\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3167 - acc: 1.0000 - val_loss: 1.5132 - val_acc: 0.6448\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2653 - acc: 1.0000 - val_loss: 1.5150 - val_acc: 0.6472\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2842 - acc: 0.9688 - val_loss: 1.5295 - val_acc: 0.6428\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2728 - acc: 1.0000 - val_loss: 1.5535 - val_acc: 0.6399\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "28 [supervised loss: 0.2728, acc: 100.00%] [unsupervised loss: 3.4389, acc: 68.75%] [labeled consistency loss: 2.2858, acc:acc: 62.50%] [unlabeled consistency loss: 1.1531, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5047 - acc: 0.8750 - val_loss: 1.6966 - val_acc: 0.6198\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4804 - acc: 0.8438 - val_loss: 1.7250 - val_acc: 0.6154\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5373 - acc: 0.8750 - val_loss: 1.7556 - val_acc: 0.6134\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4221 - acc: 0.9688 - val_loss: 1.7936 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3014 - acc: 1.0000 - val_loss: 1.8470 - val_acc: 0.6047\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3330 - acc: 0.9688 - val_loss: 1.9172 - val_acc: 0.5984\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3476 - acc: 0.9688 - val_loss: 2.0098 - val_acc: 0.5871\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2634 - acc: 1.0000 - val_loss: 2.1235 - val_acc: 0.5745\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2615 - acc: 1.0000 - val_loss: 2.2489 - val_acc: 0.5623\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2296 - acc: 1.0000 - val_loss: 2.3644 - val_acc: 0.5491\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "29 [supervised loss: 0.2296, acc: 100.00%] [unsupervised loss: 3.0098, acc: 79.69%] [labeled consistency loss: 1.8775, acc:acc: 75.00%] [unlabeled consistency loss: 1.1323, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5431 - acc: 0.9062 - val_loss: 2.5236 - val_acc: 0.5331\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5306 - acc: 0.8750 - val_loss: 2.4959 - val_acc: 0.5367\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4675 - acc: 0.9062 - val_loss: 2.4387 - val_acc: 0.5458\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5386 - acc: 0.8438 - val_loss: 2.3369 - val_acc: 0.5564\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4879 - acc: 0.9062 - val_loss: 2.2077 - val_acc: 0.5715\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2831 - acc: 1.0000 - val_loss: 2.1011 - val_acc: 0.5887\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3655 - acc: 0.9375 - val_loss: 2.0213 - val_acc: 0.5998\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3493 - acc: 0.9688 - val_loss: 1.9869 - val_acc: 0.6089\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2308 - acc: 1.0000 - val_loss: 1.9682 - val_acc: 0.6112\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2270 - acc: 1.0000 - val_loss: 1.9598 - val_acc: 0.6134\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "30 [supervised loss: 0.2270, acc: 100.00%] [unsupervised loss: 2.4646, acc: 84.38%] [labeled consistency loss: 1.4644, acc:acc: 84.38%] [unlabeled consistency loss: 1.0002, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5687 - acc: 0.9062 - val_loss: 1.9862 - val_acc: 0.6114\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5723 - acc: 0.9062 - val_loss: 1.9667 - val_acc: 0.6160\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5090 - acc: 0.9062 - val_loss: 1.9569 - val_acc: 0.6193\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3998 - acc: 0.9688 - val_loss: 1.9551 - val_acc: 0.6202\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3790 - acc: 0.9688 - val_loss: 1.9492 - val_acc: 0.6228\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3782 - acc: 0.9375 - val_loss: 1.9281 - val_acc: 0.6288\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3131 - acc: 0.9375 - val_loss: 1.9192 - val_acc: 0.6311\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3384 - acc: 0.9375 - val_loss: 1.9156 - val_acc: 0.6333\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3491 - acc: 0.9688 - val_loss: 1.9388 - val_acc: 0.6323\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2482 - acc: 1.0000 - val_loss: 1.9792 - val_acc: 0.6289\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "31 [supervised loss: 0.2482, acc: 100.00%] [unsupervised loss: 3.7423, acc: 70.31%] [labeled consistency loss: 2.3711, acc:acc: 68.75%] [unlabeled consistency loss: 1.3712, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5735 - acc: 0.8750 - val_loss: 2.2240 - val_acc: 0.5826\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4861 - acc: 0.8750 - val_loss: 2.3315 - val_acc: 0.5632\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4606 - acc: 0.9375 - val_loss: 2.4866 - val_acc: 0.5390\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4511 - acc: 0.9062 - val_loss: 2.6395 - val_acc: 0.5173\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4158 - acc: 0.9062 - val_loss: 2.8553 - val_acc: 0.4905\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3446 - acc: 1.0000 - val_loss: 3.0626 - val_acc: 0.4673\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3165 - acc: 1.0000 - val_loss: 3.3224 - val_acc: 0.4376\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2645 - acc: 1.0000 - val_loss: 3.5752 - val_acc: 0.4132\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2935 - acc: 0.9688 - val_loss: 3.8630 - val_acc: 0.3884\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2591 - acc: 1.0000 - val_loss: 4.0846 - val_acc: 0.3735\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "32 [supervised loss: 0.2591, acc: 100.00%] [unsupervised loss: 2.5895, acc: 71.88%] [labeled consistency loss: 1.4241, acc:acc: 68.75%] [unlabeled consistency loss: 1.1654, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0257 - acc: 0.7500 - val_loss: 4.4628 - val_acc: 0.3539\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7814 - acc: 0.8750 - val_loss: 4.3542 - val_acc: 0.3618\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7224 - acc: 0.8438 - val_loss: 4.0241 - val_acc: 0.3841\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6745 - acc: 0.9062 - val_loss: 3.5853 - val_acc: 0.4120\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5344 - acc: 0.9062 - val_loss: 3.2284 - val_acc: 0.4407\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3973 - acc: 0.9688 - val_loss: 2.9463 - val_acc: 0.4669\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3318 - acc: 0.9688 - val_loss: 2.7230 - val_acc: 0.4898\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2880 - acc: 0.9688 - val_loss: 2.5810 - val_acc: 0.5047\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2578 - acc: 1.0000 - val_loss: 2.4760 - val_acc: 0.5163\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2505 - acc: 1.0000 - val_loss: 2.3944 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "33 [supervised loss: 0.2505, acc: 100.00%] [unsupervised loss: 5.0332, acc: 56.25%] [labeled consistency loss: 3.2371, acc:acc: 53.12%] [unlabeled consistency loss: 1.7961, acc: 59.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7462 - acc: 0.8125 - val_loss: 3.0292 - val_acc: 0.4587\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7057 - acc: 0.8125 - val_loss: 3.1889 - val_acc: 0.4481\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6406 - acc: 0.8438 - val_loss: 3.3504 - val_acc: 0.4399\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5072 - acc: 0.9062 - val_loss: 3.5379 - val_acc: 0.4313\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4288 - acc: 0.9375 - val_loss: 3.7168 - val_acc: 0.4205\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4460 - acc: 0.9375 - val_loss: 3.8897 - val_acc: 0.4114\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3956 - acc: 0.9375 - val_loss: 4.0405 - val_acc: 0.4028\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3498 - acc: 0.9688 - val_loss: 4.1354 - val_acc: 0.3971\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3051 - acc: 1.0000 - val_loss: 4.1683 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3228 - acc: 0.9688 - val_loss: 4.1906 - val_acc: 0.3926\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "34 [supervised loss: 0.3228, acc: 96.88%] [unsupervised loss: 3.6529, acc: 62.50%] [labeled consistency loss: 2.3234, acc:acc: 59.38%] [unlabeled consistency loss: 1.3295, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.1068 - acc: 0.6562 - val_loss: 4.7096 - val_acc: 0.3429\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0054 - acc: 0.7500 - val_loss: 4.4335 - val_acc: 0.3530\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8538 - acc: 0.8438 - val_loss: 3.8943 - val_acc: 0.3813\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6246 - acc: 0.8750 - val_loss: 3.2722 - val_acc: 0.4222\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3499 - acc: 1.0000 - val_loss: 2.7436 - val_acc: 0.4654\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4094 - acc: 0.9375 - val_loss: 2.2985 - val_acc: 0.5119\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3479 - acc: 1.0000 - val_loss: 1.9499 - val_acc: 0.5572\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3132 - acc: 1.0000 - val_loss: 1.7086 - val_acc: 0.5886\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2630 - acc: 1.0000 - val_loss: 1.5489 - val_acc: 0.6150\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2532 - acc: 1.0000 - val_loss: 1.4376 - val_acc: 0.6290\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "35 [supervised loss: 0.2532, acc: 100.00%] [unsupervised loss: 5.8986, acc: 51.56%] [labeled consistency loss: 3.9890, acc:acc: 34.38%] [unlabeled consistency loss: 1.9096, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.2852 - acc: 0.7188 - val_loss: 1.7264 - val_acc: 0.5805\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.2478 - acc: 0.7188 - val_loss: 1.7700 - val_acc: 0.5728\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.9969 - acc: 0.6875 - val_loss: 1.7997 - val_acc: 0.5701\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8199 - acc: 0.7812 - val_loss: 1.7896 - val_acc: 0.5719\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7591 - acc: 0.8125 - val_loss: 1.7553 - val_acc: 0.5792\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4570 - acc: 0.9062 - val_loss: 1.7378 - val_acc: 0.5834\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4143 - acc: 0.9062 - val_loss: 1.7659 - val_acc: 0.5797\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4427 - acc: 0.9062 - val_loss: 1.8502 - val_acc: 0.5681\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3423 - acc: 0.9688 - val_loss: 1.9854 - val_acc: 0.5508\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2753 - acc: 1.0000 - val_loss: 2.1573 - val_acc: 0.5261\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "36 [supervised loss: 0.2753, acc: 100.00%] [unsupervised loss: 4.1614, acc: 57.81%] [labeled consistency loss: 2.8288, acc:acc: 53.12%] [unlabeled consistency loss: 1.3325, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5137 - acc: 0.8750 - val_loss: 2.5503 - val_acc: 0.4712\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7113 - acc: 0.8438 - val_loss: 2.4842 - val_acc: 0.4758\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5489 - acc: 0.9062 - val_loss: 2.3514 - val_acc: 0.4886\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4561 - acc: 0.9375 - val_loss: 2.2177 - val_acc: 0.5050\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4692 - acc: 0.9375 - val_loss: 2.0519 - val_acc: 0.5230\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4132 - acc: 0.9375 - val_loss: 1.8803 - val_acc: 0.5503\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4409 - acc: 0.9062 - val_loss: 1.7175 - val_acc: 0.5772\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3205 - acc: 0.9688 - val_loss: 1.6001 - val_acc: 0.5949\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2597 - acc: 1.0000 - val_loss: 1.5214 - val_acc: 0.6078\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2789 - acc: 1.0000 - val_loss: 1.4695 - val_acc: 0.6162\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "37 [supervised loss: 0.2789, acc: 100.00%] [unsupervised loss: 3.2788, acc: 60.94%] [labeled consistency loss: 1.9398, acc:acc: 59.38%] [unlabeled consistency loss: 1.3391, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7080 - acc: 0.8125 - val_loss: 1.4473 - val_acc: 0.6203\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6733 - acc: 0.8750 - val_loss: 1.4519 - val_acc: 0.6208\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5417 - acc: 0.9062 - val_loss: 1.4722 - val_acc: 0.6170\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4606 - acc: 0.9062 - val_loss: 1.5056 - val_acc: 0.6109\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4995 - acc: 0.9375 - val_loss: 1.5669 - val_acc: 0.6005\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3769 - acc: 0.9375 - val_loss: 1.6389 - val_acc: 0.5894\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3572 - acc: 0.9688 - val_loss: 1.7155 - val_acc: 0.5766\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2818 - acc: 1.0000 - val_loss: 1.7924 - val_acc: 0.5684\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2905 - acc: 1.0000 - val_loss: 1.8605 - val_acc: 0.5582\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2523 - acc: 1.0000 - val_loss: 1.9232 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "38 [supervised loss: 0.2523, acc: 100.00%] [unsupervised loss: 2.3218, acc: 84.38%] [labeled consistency loss: 1.4522, acc:acc: 81.25%] [unlabeled consistency loss: 0.8696, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.0871 - acc: 0.6875 - val_loss: 2.0789 - val_acc: 0.5342\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8338 - acc: 0.7812 - val_loss: 2.0273 - val_acc: 0.5458\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8738 - acc: 0.7500 - val_loss: 1.8839 - val_acc: 0.5679\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7748 - acc: 0.7500 - val_loss: 1.7491 - val_acc: 0.5925\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4353 - acc: 0.9688 - val_loss: 1.6753 - val_acc: 0.6036\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3485 - acc: 1.0000 - val_loss: 1.6738 - val_acc: 0.6050\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3197 - acc: 1.0000 - val_loss: 1.7342 - val_acc: 0.5928\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2606 - acc: 1.0000 - val_loss: 1.8491 - val_acc: 0.5711\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2685 - acc: 1.0000 - val_loss: 2.0030 - val_acc: 0.5456\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3100 - acc: 0.9688 - val_loss: 2.1878 - val_acc: 0.5146\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "39 [supervised loss: 0.3100, acc: 96.88%] [unsupervised loss: 4.3517, acc: 53.12%] [labeled consistency loss: 2.4756, acc:acc: 53.12%] [unlabeled consistency loss: 1.8761, acc: 53.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5708 - acc: 0.9062 - val_loss: 2.4330 - val_acc: 0.4813\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8102 - acc: 0.8438 - val_loss: 2.4788 - val_acc: 0.4747\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4687 - acc: 0.9375 - val_loss: 2.5111 - val_acc: 0.4702\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4832 - acc: 0.9375 - val_loss: 2.5397 - val_acc: 0.4661\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3751 - acc: 0.9688 - val_loss: 2.5644 - val_acc: 0.4639\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4503 - acc: 0.9688 - val_loss: 2.5594 - val_acc: 0.4645\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5006 - acc: 0.9062 - val_loss: 2.5351 - val_acc: 0.4672\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4022 - acc: 0.9688 - val_loss: 2.4973 - val_acc: 0.4733\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3117 - acc: 1.0000 - val_loss: 2.4673 - val_acc: 0.4773\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3074 - acc: 1.0000 - val_loss: 2.4381 - val_acc: 0.4822\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "40 [supervised loss: 0.3074, acc: 100.00%] [unsupervised loss: 2.3945, acc: 84.38%] [labeled consistency loss: 0.9276, acc:acc: 93.75%] [unlabeled consistency loss: 1.4669, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8816 - acc: 0.7500 - val_loss: 2.3467 - val_acc: 0.4985\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.9269 - acc: 0.7500 - val_loss: 2.1982 - val_acc: 0.5166\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.8099 - acc: 0.8438 - val_loss: 2.0453 - val_acc: 0.5366\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6511 - acc: 0.8750 - val_loss: 1.8953 - val_acc: 0.5562\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5694 - acc: 0.9375 - val_loss: 1.7709 - val_acc: 0.5762\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5537 - acc: 0.9062 - val_loss: 1.6822 - val_acc: 0.5902\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5205 - acc: 0.9375 - val_loss: 1.6297 - val_acc: 0.5980\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3693 - acc: 0.9688 - val_loss: 1.5917 - val_acc: 0.6042\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2874 - acc: 1.0000 - val_loss: 1.5677 - val_acc: 0.6063\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3210 - acc: 0.9688 - val_loss: 1.5573 - val_acc: 0.6079\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "41 [supervised loss: 0.3210, acc: 96.88%] [unsupervised loss: 3.9964, acc: 68.75%] [labeled consistency loss: 2.2017, acc:acc: 65.62%] [unlabeled consistency loss: 1.7946, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7238 - acc: 0.8438 - val_loss: 1.6529 - val_acc: 0.5934\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6440 - acc: 0.8125 - val_loss: 1.6591 - val_acc: 0.5928\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6302 - acc: 0.9062 - val_loss: 1.6540 - val_acc: 0.5947\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5511 - acc: 0.8750 - val_loss: 1.6351 - val_acc: 0.6012\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4323 - acc: 0.9375 - val_loss: 1.6141 - val_acc: 0.6084\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4318 - acc: 0.9062 - val_loss: 1.5917 - val_acc: 0.6130\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3945 - acc: 0.9062 - val_loss: 1.5700 - val_acc: 0.6174\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3438 - acc: 1.0000 - val_loss: 1.5584 - val_acc: 0.6200\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3486 - acc: 0.9688 - val_loss: 1.5533 - val_acc: 0.6200\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3062 - acc: 0.9688 - val_loss: 1.5502 - val_acc: 0.6206\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "42 [supervised loss: 0.3062, acc: 96.88%] [unsupervised loss: 3.1679, acc: 73.44%] [labeled consistency loss: 1.5627, acc:acc: 71.88%] [unlabeled consistency loss: 1.6051, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.9212 - acc: 0.7500 - val_loss: 1.6019 - val_acc: 0.6132\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7931 - acc: 0.7188 - val_loss: 1.5492 - val_acc: 0.6249\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6061 - acc: 0.8750 - val_loss: 1.4781 - val_acc: 0.6407\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5965 - acc: 0.8438 - val_loss: 1.3885 - val_acc: 0.6620\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5695 - acc: 0.9062 - val_loss: 1.2956 - val_acc: 0.6791\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4523 - acc: 0.9062 - val_loss: 1.2202 - val_acc: 0.6961\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4549 - acc: 0.9062 - val_loss: 1.1627 - val_acc: 0.7072\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3712 - acc: 0.9688 - val_loss: 1.1173 - val_acc: 0.7156\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3383 - acc: 1.0000 - val_loss: 1.0868 - val_acc: 0.7229\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3259 - acc: 1.0000 - val_loss: 1.0696 - val_acc: 0.7244\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "43 [supervised loss: 0.3259, acc: 100.00%] [unsupervised loss: 3.0676, acc: 71.88%] [labeled consistency loss: 1.7372, acc:acc: 65.62%] [unlabeled consistency loss: 1.3304, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.8792 - acc: 0.7188 - val_loss: 1.0750 - val_acc: 0.7220\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.8235 - acc: 0.8125 - val_loss: 1.0798 - val_acc: 0.7209\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7282 - acc: 0.7812 - val_loss: 1.0970 - val_acc: 0.7168\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6156 - acc: 0.8438 - val_loss: 1.1282 - val_acc: 0.7093\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4505 - acc: 0.9375 - val_loss: 1.1728 - val_acc: 0.6976\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4005 - acc: 0.9375 - val_loss: 1.2303 - val_acc: 0.6834\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3558 - acc: 0.9688 - val_loss: 1.3015 - val_acc: 0.6677\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3305 - acc: 1.0000 - val_loss: 1.3746 - val_acc: 0.6540\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2902 - acc: 1.0000 - val_loss: 1.4451 - val_acc: 0.6430\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3022 - acc: 1.0000 - val_loss: 1.5111 - val_acc: 0.6302\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "44 [supervised loss: 0.3022, acc: 100.00%] [unsupervised loss: 2.4973, acc: 82.81%] [labeled consistency loss: 1.4953, acc:acc: 78.12%] [unlabeled consistency loss: 1.0020, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6395 - acc: 0.8438 - val_loss: 1.6013 - val_acc: 0.6169\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7711 - acc: 0.8750 - val_loss: 1.6093 - val_acc: 0.6151\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7612 - acc: 0.8438 - val_loss: 1.5857 - val_acc: 0.6201\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4860 - acc: 0.8750 - val_loss: 1.5508 - val_acc: 0.6262\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5307 - acc: 0.8750 - val_loss: 1.5109 - val_acc: 0.6338\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4502 - acc: 0.9688 - val_loss: 1.4605 - val_acc: 0.6405\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3698 - acc: 0.9688 - val_loss: 1.4294 - val_acc: 0.6473\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.3634 - acc: 1.0000 - val_loss: 1.4122 - val_acc: 0.6505\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2692 - acc: 1.0000 - val_loss: 1.4028 - val_acc: 0.6488\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2903 - acc: 0.9688 - val_loss: 1.4027 - val_acc: 0.6508\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "45 [supervised loss: 0.2903, acc: 96.88%] [unsupervised loss: 2.3876, acc: 79.69%] [labeled consistency loss: 1.1591, acc:acc: 84.38%] [unlabeled consistency loss: 1.2286, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5035 - acc: 0.9375 - val_loss: 1.4335 - val_acc: 0.6427\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5441 - acc: 0.8750 - val_loss: 1.4368 - val_acc: 0.6430\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4959 - acc: 0.9375 - val_loss: 1.4365 - val_acc: 0.6416\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.5441 - acc: 0.9375 - val_loss: 1.4233 - val_acc: 0.6418\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4681 - acc: 0.9375 - val_loss: 1.3920 - val_acc: 0.6472\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3684 - acc: 0.9375 - val_loss: 1.3646 - val_acc: 0.6539\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3344 - acc: 0.9688 - val_loss: 1.3358 - val_acc: 0.6594\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2774 - acc: 1.0000 - val_loss: 1.3112 - val_acc: 0.6647\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2650 - acc: 1.0000 - val_loss: 1.2927 - val_acc: 0.6698\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2616 - acc: 1.0000 - val_loss: 1.2810 - val_acc: 0.6738\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "46 [supervised loss: 0.2616, acc: 100.00%] [unsupervised loss: 2.2511, acc: 82.81%] [labeled consistency loss: 1.2858, acc:acc: 84.38%] [unlabeled consistency loss: 0.9653, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6798 - acc: 0.7500 - val_loss: 1.2758 - val_acc: 0.6758\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5528 - acc: 0.8750 - val_loss: 1.2668 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4536 - acc: 0.9375 - val_loss: 1.2519 - val_acc: 0.6797\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4345 - acc: 0.9375 - val_loss: 1.2503 - val_acc: 0.6782\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3488 - acc: 0.9688 - val_loss: 1.2586 - val_acc: 0.6776\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2830 - acc: 1.0000 - val_loss: 1.2753 - val_acc: 0.6755\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3089 - acc: 1.0000 - val_loss: 1.2974 - val_acc: 0.6706\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2590 - acc: 0.9688 - val_loss: 1.3228 - val_acc: 0.6680\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2367 - acc: 1.0000 - val_loss: 1.3544 - val_acc: 0.6638\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2520 - acc: 1.0000 - val_loss: 1.3929 - val_acc: 0.6586\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "47 [supervised loss: 0.2520, acc: 100.00%] [unsupervised loss: 2.7451, acc: 75.00%] [labeled consistency loss: 1.8096, acc:acc: 68.75%] [unlabeled consistency loss: 0.9355, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4232 - acc: 0.9062 - val_loss: 1.4413 - val_acc: 0.6488\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3780 - acc: 0.9375 - val_loss: 1.4455 - val_acc: 0.6487\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4540 - acc: 0.8750 - val_loss: 1.4414 - val_acc: 0.6496\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3657 - acc: 0.9375 - val_loss: 1.4424 - val_acc: 0.6481\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3803 - acc: 0.9375 - val_loss: 1.4518 - val_acc: 0.6458\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3488 - acc: 0.9375 - val_loss: 1.4662 - val_acc: 0.6413\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2670 - acc: 1.0000 - val_loss: 1.4848 - val_acc: 0.6370\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2950 - acc: 0.9688 - val_loss: 1.5112 - val_acc: 0.6304\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2551 - acc: 1.0000 - val_loss: 1.5375 - val_acc: 0.6246\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2508 - acc: 1.0000 - val_loss: 1.5649 - val_acc: 0.6186\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "48 [supervised loss: 0.2508, acc: 100.00%] [unsupervised loss: 2.1159, acc: 82.81%] [labeled consistency loss: 1.1126, acc:acc: 87.50%] [unlabeled consistency loss: 1.0033, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4870 - acc: 0.9375 - val_loss: 1.6024 - val_acc: 0.6082\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4628 - acc: 0.9688 - val_loss: 1.5754 - val_acc: 0.6116\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4182 - acc: 0.9688 - val_loss: 1.5339 - val_acc: 0.6182\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3128 - acc: 0.9688 - val_loss: 1.4977 - val_acc: 0.6227\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3265 - acc: 0.9688 - val_loss: 1.4791 - val_acc: 0.6279\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3959 - acc: 0.9375 - val_loss: 1.4687 - val_acc: 0.6309\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.3644 - acc: 0.9375 - val_loss: 1.4758 - val_acc: 0.6309\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2658 - acc: 1.0000 - val_loss: 1.5091 - val_acc: 0.6251\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2456 - acc: 1.0000 - val_loss: 1.5636 - val_acc: 0.6161\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3067 - acc: 0.9688 - val_loss: 1.6367 - val_acc: 0.6025\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "49 [supervised loss: 0.3067, acc: 96.88%] [unsupervised loss: 2.6177, acc: 76.56%] [labeled consistency loss: 1.5625, acc:acc: 71.88%] [unlabeled consistency loss: 1.0552, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7330 - acc: 0.8125 - val_loss: 1.6893 - val_acc: 0.5927\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87200\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6506 - acc: 0.8438 - val_loss: 1.6581 - val_acc: 0.5955\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87200\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6763 - acc: 0.8750 - val_loss: 1.6266 - val_acc: 0.6012\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87200\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5756 - acc: 0.8750 - val_loss: 1.5882 - val_acc: 0.6069\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87200\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5048 - acc: 0.9062 - val_loss: 1.5730 - val_acc: 0.6110\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87200\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.4347 - acc: 0.9375 - val_loss: 1.5681 - val_acc: 0.6111\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87200\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.3611 - acc: 0.9688 - val_loss: 1.5810 - val_acc: 0.6103\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87200\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2983 - acc: 1.0000 - val_loss: 1.6070 - val_acc: 0.6055\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87200\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2796 - acc: 1.0000 - val_loss: 1.6399 - val_acc: 0.5996\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87200\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2612 - acc: 1.0000 - val_loss: 1.6738 - val_acc: 0.5912\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87200\n",
            "50 [supervised loss: 0.2612, acc: 100.00%] [unsupervised loss: 1.9825, acc: 79.69%] [labeled consistency loss: 1.1464, acc:acc: 78.12%] [unlabeled consistency loss: 0.8361, acc: 81.25%]\n",
            "Training time: 2566.5759s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ndAup4NMwkj",
        "colab_type": "code",
        "outputId": "bcedaef7-c809-43c5-d5f8-e76843b79edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "plot_supervised_losses = np.array(supervised_losses)\n",
        "plot_unsupervised_losses = np.array(unsupervised_losses)\n",
        "plot_labeled_consistency_costs = np.array(labeled_consistency_costs)\n",
        "plot_unlabeled_consistency_costs = np.array(unlabeled_consistency_costs)\n",
        "plot_all_losses = np.array(supervised_losses)+np.array(unsupervised_losses)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, plot_all_losses, label=\"All loss\", color='black')\n",
        "plt.plot(iteration_checkpoints, plot_supervised_losses, label=\"Supervised loss\", color='tab:blue')\n",
        "plt.plot(iteration_checkpoints, plot_unsupervised_losses, label=\"Unsupervised loss\", color='tab:green')\n",
        "plt.plot(iteration_checkpoints, plot_labeled_consistency_costs, label=\"Labeled consistency loss\", color='tab:red', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, plot_unlabeled_consistency_costs, label=\"Unlabeled consistency loss\", color='tab:orange', linestyle='dashed')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Mean Teacher's Supervised and Unsupervised Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11e8dbe550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAFPCAYAAAA1Pp3dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAiCIAiCIOQ5YrgJgiAIgiAIgiDkOf8Pbcho75tVJ/IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omcpkiBMwiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-8000.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftgecc1jMwfq",
        "colab_type": "code",
        "outputId": "b5851945-3ff9-4407-ba28-3d0f65fd8172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-6100.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 515us/step\n",
            "Test Accuracy: 12.79%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDJmNbdMwce",
        "colab_type": "code",
        "outputId": "f8c8bae8-63cd-4573-9f94-28ecfc9388fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "accs = []\n",
        "tx = [x for x in range(1,51,1)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"Mean Teacher's accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 684us/step\n",
            "10000/10000 [==============================] - 7s 702us/step\n",
            "10000/10000 [==============================] - 7s 691us/step\n",
            "10000/10000 [==============================] - 7s 675us/step\n",
            "10000/10000 [==============================] - 7s 692us/step\n",
            "10000/10000 [==============================] - 7s 694us/step\n",
            "10000/10000 [==============================] - 7s 687us/step\n",
            "10000/10000 [==============================] - 7s 693us/step\n",
            "10000/10000 [==============================] - 7s 683us/step\n",
            "10000/10000 [==============================] - 7s 678us/step\n",
            "10000/10000 [==============================] - 7s 660us/step\n",
            "10000/10000 [==============================] - 7s 660us/step\n",
            "10000/10000 [==============================] - 6s 631us/step\n",
            "10000/10000 [==============================] - 7s 665us/step\n",
            "10000/10000 [==============================] - 7s 682us/step\n",
            "10000/10000 [==============================] - 7s 675us/step\n",
            "10000/10000 [==============================] - 6s 634us/step\n",
            "10000/10000 [==============================] - 6s 616us/step\n",
            "10000/10000 [==============================] - 7s 671us/step\n",
            "10000/10000 [==============================] - 7s 700us/step\n",
            "10000/10000 [==============================] - 7s 717us/step\n",
            "10000/10000 [==============================] - 7s 686us/step\n",
            "10000/10000 [==============================] - 7s 690us/step\n",
            "10000/10000 [==============================] - 7s 699us/step\n",
            "10000/10000 [==============================] - 7s 695us/step\n",
            "10000/10000 [==============================] - 7s 671us/step\n",
            "10000/10000 [==============================] - 7s 696us/step\n",
            "10000/10000 [==============================] - 7s 672us/step\n",
            "10000/10000 [==============================] - 6s 639us/step\n",
            "10000/10000 [==============================] - 6s 626us/step\n",
            "10000/10000 [==============================] - 7s 660us/step\n",
            "10000/10000 [==============================] - 7s 691us/step\n",
            "10000/10000 [==============================] - 7s 679us/step\n",
            "10000/10000 [==============================] - 7s 684us/step\n",
            "10000/10000 [==============================] - 7s 688us/step\n",
            "10000/10000 [==============================] - 7s 686us/step\n",
            "10000/10000 [==============================] - 7s 661us/step\n",
            "10000/10000 [==============================] - 6s 640us/step\n",
            "10000/10000 [==============================] - 7s 703us/step\n",
            "10000/10000 [==============================] - 7s 699us/step\n",
            "10000/10000 [==============================] - 7s 694us/step\n",
            "10000/10000 [==============================] - 7s 708us/step\n",
            "10000/10000 [==============================] - 7s 708us/step\n",
            "10000/10000 [==============================] - 7s 669us/step\n",
            "10000/10000 [==============================] - 6s 633us/step\n",
            "10000/10000 [==============================] - 7s 693us/step\n",
            "10000/10000 [==============================] - 6s 649us/step\n",
            "10000/10000 [==============================] - 7s 700us/step\n",
            "10000/10000 [==============================] - 7s 697us/step\n",
            "10000/10000 [==============================] - 7s 695us/step\n",
            "0.8021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11e7683ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFPCAYAAAASkBw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2feQLXtMwY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47668922-9a80-4ffd-da23-e00ab4dea074"
      },
      "source": [
        "print(max(accs[12:]))\n",
        "# for acc in accs:\n",
        "#   print(acc)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UrCrrObhruT",
        "colab_type": "text"
      },
      "source": [
        "# To be deleted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N7bCb6u7h7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-eb3VpX7mRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'useless_models')\n",
        "model_name = 'cifar10_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.accs = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.accs.append(logs.get('acc'))\n",
        "\n",
        "history = LossHistory()\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0bnB6CI_zts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik5uq5X3Rbi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations, batch_size, iter_epochs, save_interval):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        imgs, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Compute quantities required for featurewise normalization (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(imgs)\n",
        "        model.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        \n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "            losses.append(history.losses[-1])\n",
        "            accs.append(history.accs[-1])\n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "            discriminator_supervised.save(\"./models/model-\" + str(iteration+1) + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr0Q7EaWOar9",
        "colab_type": "code",
        "outputId": "db7014bf-2598-4046-ab57-4e80f44640dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iterations = 5\n",
        "iter_epochs = 10\n",
        "save_interval = 10\n",
        "batch_size = 32\n",
        "num_labeled = 100\n",
        "losses = []\n",
        "accs = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "dataset = Dataset_CIFAR10(num_labeled)\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "train(iterations, batch_size, iter_epochs, save_interval)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Use time:\" + str(endtime-starttime) + \"s\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2716 - acc: 1.0000 - val_loss: 1.5633 - val_acc: 0.7275\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73420\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2799 - acc: 1.0000 - val_loss: 1.5599 - val_acc: 0.7273\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73420\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2757 - acc: 1.0000 - val_loss: 1.5565 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73420\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2784 - acc: 1.0000 - val_loss: 1.5532 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73420\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2789 - acc: 1.0000 - val_loss: 1.5482 - val_acc: 0.7278\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73420\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2742 - acc: 1.0000 - val_loss: 1.5430 - val_acc: 0.7291\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73420\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2996 - acc: 1.0000 - val_loss: 1.5316 - val_acc: 0.7308\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73420\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2808 - acc: 1.0000 - val_loss: 1.5165 - val_acc: 0.7327\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73420\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 1.5049 - val_acc: 0.7339\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73420\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2722 - acc: 1.0000 - val_loss: 1.4943 - val_acc: 0.7351\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.73420 to 0.73510, saving model to /content/saved_models/cifar10_ResNet20v1_model.010.h5\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3956 - acc: 0.9688 - val_loss: 1.4862 - val_acc: 0.7348\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73510\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3032 - acc: 1.0000 - val_loss: 1.4795 - val_acc: 0.7346\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73510\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3062 - acc: 1.0000 - val_loss: 1.4726 - val_acc: 0.7359\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.73510 to 0.73590, saving model to /content/saved_models/cifar10_ResNet20v1_model.003.h5\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2877 - acc: 1.0000 - val_loss: 1.4676 - val_acc: 0.7361\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.73590 to 0.73610, saving model to /content/saved_models/cifar10_ResNet20v1_model.004.h5\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3063 - acc: 0.9688 - val_loss: 1.4657 - val_acc: 0.7368\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.73610 to 0.73680, saving model to /content/saved_models/cifar10_ResNet20v1_model.005.h5\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3436 - acc: 0.9688 - val_loss: 1.4577 - val_acc: 0.7382\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.73680 to 0.73820, saving model to /content/saved_models/cifar10_ResNet20v1_model.006.h5\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2751 - acc: 1.0000 - val_loss: 1.4565 - val_acc: 0.7365\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3166 - acc: 0.9688 - val_loss: 1.4655 - val_acc: 0.7338\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2734 - acc: 1.0000 - val_loss: 1.4773 - val_acc: 0.7321\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3418 - acc: 0.9688 - val_loss: 1.4931 - val_acc: 0.7280\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2748 - acc: 1.0000 - val_loss: 1.5108 - val_acc: 0.7239\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73820\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2711 - acc: 1.0000 - val_loss: 1.5287 - val_acc: 0.7225\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73820\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.5459 - val_acc: 0.7203\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73820\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2706 - acc: 1.0000 - val_loss: 1.5630 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73820\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2713 - acc: 1.0000 - val_loss: 1.5802 - val_acc: 0.7174\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2733 - acc: 1.0000 - val_loss: 1.5959 - val_acc: 0.7148\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2782 - acc: 1.0000 - val_loss: 1.6087 - val_acc: 0.7132\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2755 - acc: 1.0000 - val_loss: 1.6190 - val_acc: 0.7111\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2965 - acc: 1.0000 - val_loss: 1.6283 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.6368 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2950 - acc: 0.9688 - val_loss: 1.6413 - val_acc: 0.7072\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73820\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2727 - acc: 1.0000 - val_loss: 1.6451 - val_acc: 0.7056\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73820\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2873 - acc: 1.0000 - val_loss: 1.6464 - val_acc: 0.7052\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73820\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2858 - acc: 1.0000 - val_loss: 1.6465 - val_acc: 0.7047\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73820\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2697 - acc: 1.0000 - val_loss: 1.6467 - val_acc: 0.7043\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2707 - acc: 1.0000 - val_loss: 1.6466 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4211 - acc: 0.9688 - val_loss: 1.6265 - val_acc: 0.7052\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2730 - acc: 1.0000 - val_loss: 1.6096 - val_acc: 0.7068\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2732 - acc: 1.0000 - val_loss: 1.5967 - val_acc: 0.7071\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2811 - acc: 1.0000 - val_loss: 1.5849 - val_acc: 0.7078\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2735 - acc: 1.0000 - val_loss: 1.5728 - val_acc: 0.7091\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.73820\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2724 - acc: 1.0000 - val_loss: 1.5632 - val_acc: 0.7099\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.73820\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.5545 - val_acc: 0.7106\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.73820\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2763 - acc: 1.0000 - val_loss: 1.5466 - val_acc: 0.7116\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73820\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2727 - acc: 1.0000 - val_loss: 1.5397 - val_acc: 0.7119\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73820\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2865 - acc: 1.0000 - val_loss: 1.5371 - val_acc: 0.7122\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73820\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 1.5350 - val_acc: 0.7128\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.73820\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2737 - acc: 1.0000 - val_loss: 1.5334 - val_acc: 0.7139\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.73820\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2711 - acc: 1.0000 - val_loss: 1.5314 - val_acc: 0.7140\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.73820\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2711 - acc: 1.0000 - val_loss: 1.5306 - val_acc: 0.7148\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.73820\n",
            "[0.27351266, 0.27242652, 0.27099964, 0.27627712, 0.27269456, 0.286533, 0.2704616, 0.27368137, 0.27110267, 0.27113327]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAIJ9nrnaNjk",
        "colab_type": "code",
        "outputId": "a194db45-69be-467c-8e6c-d8c1e1fef82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Score trained model.\n",
        "x_test, y_test = dataset.test_set()\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 315us/step\n",
            "Test loss: 1.5306017761230468\n",
            "Test accuracy: 0.7148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-sKr5pbY390",
        "colab_type": "code",
        "outputId": "1af8d53b-fb53-4776-c5d6-6da612570555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(losses)\n",
        "print(accs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Y7noknSmXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}