{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xDiTP_67MV",
        "colab_type": "code",
        "outputId": "cc526568-83b2-4ce2-b1ad-747de3760b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import (Dense, Conv2D, BatchNormalization, Activation, \n",
        "                          AveragePooling2D, Input, Flatten, \n",
        "                          Concatenate, Dropout, Lambda, \n",
        "                          Reshape, Embedding, Multiply)\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, Callback\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from __future__ import print_function\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSaEOfjG6ztu",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_hPw_jy8jea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset_CIFAR10:\n",
        "    def __init__(self, num_labeled):\n",
        "\n",
        "        def preprocess_imgs(x):\n",
        "            # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "            return x\n",
        "\n",
        "        def preprocess_labels(y):\n",
        "            y = y.reshape(-1, 1)\n",
        "            y = to_categorical(y, num_classes = 10)\n",
        "            return y\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train = preprocess_imgs(x_train)\n",
        "        y_train = preprocess_labels(y_train)\n",
        "        x_test = preprocess_imgs(x_test)\n",
        "        y_test = preprocess_labels(y_test)\n",
        "\n",
        "        # Number labeled examples to use for training\n",
        "        self.num_labeled = num_labeled\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        del x_train, y_train, x_test, y_test\n",
        "\n",
        "    def batch_labeled(self, batch_size):\n",
        "        # Get a random batch of labeled images and their labels\n",
        "        idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        labels = self.y_train[idx]\n",
        "        return imgs, labels\n",
        "\n",
        "    def batch_unlabeled(self, batch_size):\n",
        "        # Get a random batch of unlabeled images\n",
        "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        return imgs\n",
        "\n",
        "    def training_set(self):\n",
        "        return self.x_train, self.y_train\n",
        "\n",
        "    def test_set(self):\n",
        "        return self.x_test, self.y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5nWXxcZdgQb",
        "colab_type": "text"
      },
      "source": [
        "## Check the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5jJexNdigk",
        "colab_type": "code",
        "outputId": "80c9c905-369a-40b7-e388-6b7a3d1d67c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Training set\n",
        "d_ytrain = {}\n",
        "for i in range(10):\n",
        "  d_ytrain[i] = 0\n",
        "for i in range(len(y_train)):\n",
        "  d_ytrain[y_train[i][0]] = d_ytrain.get(y_train[i][0]) + 1\n",
        "print(\"CIFAR-10 training set:\")\n",
        "print(d_ytrain)\n",
        "\n",
        "# Test set\n",
        "d_ytest = {}\n",
        "for i in range(10):\n",
        "  d_ytest[i] = 0\n",
        "for i in range(len(y_test)):\n",
        "  d_ytest[y_test[i][0]] = d_ytest.get(y_test[i][0]) + 1\n",
        "print(\"CIFAR-10 test set:\")\n",
        "print(d_ytest)\n",
        "\n",
        "del x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "CIFAR-10 training set:\n",
            "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n",
            "CIFAR-10 test set:\n",
            "{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRSrTM11pJtd",
        "colab_type": "code",
        "outputId": "73b65b08-f8cb-4eed-96c1-278990dfa0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "d_imgs = {}\n",
        "num_imgs = 0\n",
        "for i in range(1000):\n",
        "  if y_train[i][0] not in d_imgs:\n",
        "    d_imgs[y_train[i][0]] = x_train[i]\n",
        "    num_imgs += 1\n",
        "  if num_imgs == 10: break\n",
        "\n",
        "d_name = {0:\"airplane\", 1:\"automobile\", 2:\"bird\", 3:\"cat\", 4:\"deer\", 5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9:\"truck\"}\t\n",
        "\n",
        "# Set image grid\n",
        "fig, axs = plt.subplots(2,\n",
        "                        5,\n",
        "                        figsize=(10, 4),\n",
        "                        sharey=True,\n",
        "                        sharex=True)\n",
        "\n",
        "cnt = 0\n",
        "for i in range(2):\n",
        "    for j in range(5):\n",
        "        # Output a grid of images\n",
        "        axs[i, j].imshow(d_imgs[cnt])\n",
        "        axs[i, j].axis('off')\n",
        "        axs[i, j].set_title(\"Class: \" + str(d_name[cnt]))\n",
        "        cnt += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAD1CAYAAABUdy/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrpoGSKdmrp",
        "colab_type": "text"
      },
      "source": [
        "# Number of labeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvYSG5vKdoSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labeled = 1500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjhxJulqdyFs",
        "colab_type": "text"
      },
      "source": [
        "# SCGAN-2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcTHBiShR9m",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA0DtW_2d14g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "input_shape = img_shape = (32, 32, 3)\n",
        "num_classes = 10\n",
        "z_dim = 100   # Size of the noise vector, used as input to the Generator\n",
        "n = 3\n",
        "depth = n * 6 + 2   # Depth of ResNet model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgzlORUA7eUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eZ2B42_7fiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abhpWtyBfaxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "def build_generator(z_dim):\n",
        "  \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ), dtype='float32')\n",
        "    label_embedding = Dense(z_dim, input_dim=num_classes)(label)\n",
        "    joined_representation = Multiply()([z, label_embedding])\n",
        "    \n",
        "#     model = Sequential()\n",
        "\n",
        "    # Reshape input into 8x8x256 tensor via a fully connected layer\n",
        "    model = Dense(256 * 8 * 8, input_dim=z_dim)(joined_representation)\n",
        "    model = Reshape((8, 8, 256))(model)\n",
        "\n",
        "    # Transposed convolution layer, from 8x8x256 into 16x16x128 tensor\n",
        "    model = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(model)\n",
        "\n",
        "    # Batch normalization\n",
        "    model = BatchNormalization()(model)\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model = LeakyReLU(alpha=0.01)(model)\n",
        "\n",
        "    # Transposed convolution layer, from 16x16x128 to 16x16x64 tensor\n",
        "    model = Conv2DTranspose(64, kernel_size=3, strides=1, padding='same')(model)\n",
        "\n",
        "    # Batch normalization\n",
        "    model = BatchNormalization()(model)\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model = LeakyReLU(alpha=0.01)(model)\n",
        "\n",
        "    # Transposed convolution layer, from 16x16x64 to 32x32x3 tensor\n",
        "    model = Conv2DTranspose(3, kernel_size=3, strides=2, padding='same')(model)\n",
        "\n",
        "    # Output layer with tanh activation\n",
        "    conditioned_img = Activation('tanh')(model)\n",
        "    \n",
        "#     conditioned_img = model(joined_representation)\n",
        "\n",
        "    model = Model([z, label], conditioned_img)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ09_xGfffbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_net(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes, kernel_initializer='he_normal')(y)\n",
        "    # outputs = Dense(num_classes,\n",
        "    #                 activation='softmax',\n",
        "    #                 kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx6rxPK2hiqj",
        "colab_type": "code",
        "outputId": "84f14265-b68c-48cd-a4f8-9c437e806a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "build_discriminator_net(img_shape, depth).summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTS0nBXhMhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_supervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    # Softmax activation, giving predicted probability distribution over the real classes\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuxULq-IhOlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_unsupervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    def predict(x):\n",
        "        # Transform distribution over real classes into a binary real-vs-fake probability\n",
        "        prediction = 1.0 - (1.0 / (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
        "        return prediction\n",
        "\n",
        "    # 'Real-vs-fake' output neuron defined above\n",
        "    model.add(Lambda(predict))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3LcMzjZhQFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ))\n",
        "    img = generator([z, label])\n",
        "    output = discriminator(img)\n",
        "    model = Model([z, label], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X4FZirFhWA7",
        "colab_type": "code",
        "outputId": "615148a9-6cc5-4eb2-856e-f7b56a76d131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "# Core Discriminator network:\n",
        "# These layers are shared during supervised and unsupervised training\n",
        "\n",
        "discriminator_net = build_discriminator_net(input_shape=img_shape, depth=depth)\n",
        "\n",
        "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
        "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "# discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "#                                  metrics=['accuracy'],\n",
        "#                                  optimizer=Adam(lr=lr_schedule(0)))\n",
        "\n",
        "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
        "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "                                metrics=['accuracy'],\n",
        "                                optimizer=Adam())\n",
        "# discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "#                                 metrics=['accuracy'],\n",
        "#                                 optimizer=Adam(lr=lr_schedule(0)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1nVA_1egKoX",
        "colab_type": "code",
        "outputId": "e0e1d0ca-2aab-47c1-f2e7-0c807fc4c058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "discriminator_unsupervised.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_2 (Model)              (None, 10)                274442    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpsqRccThV02",
        "colab_type": "code",
        "outputId": "9708cb90-1f99-451e-db7e-84a9717a3297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Build the Generator\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "discriminator_supervised.trainable = False\n",
        "discriminator_unsupervised.trainable = False\n",
        "gan = build_gan(generator, discriminator_unsupervised)\n",
        "gan.compile(loss='binary_crossentropy', \n",
        "            metrics=['accuracy'], \n",
        "            optimizer=Adam())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yrZyUchhhER",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW25Txjghf7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir models\n",
        "%mkdir losses\n",
        "%mkdir models/models-label-1500\n",
        "%mkdir losses/losses-label-1500\n",
        "\n",
        "# if not os.path.isdir(save_dir):\n",
        "#     os.makedirs(save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNb1Q6IQkL-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'models')\n",
        "model_name = 'cifar10_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.accs = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.accs.append(logs.get('acc'))\n",
        "\n",
        "history = LossHistory()\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler, history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3q3OrbkUGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR2Hun3ChbSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain(iterations_pre, batch_size, save_interval, iter_epochs):\n",
        "  for iteration in range(iterations_pre):\n",
        "      imgs, labels = dataset.training_set()\n",
        "      # imgs, labels = dataset.batch_labeled(batch_size)\n",
        "      x_test, y_test = dataset.test_set()\n",
        "\n",
        "      # Compute quantities required for featurewise normalization (std, mean, and principal components if ZCA whitening is applied).\n",
        "      datagen.fit(imgs)\n",
        "      discriminator_supervised.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  epochs=iter_epochs, verbose=1, workers=4,\n",
        "                  callbacks=callbacks)\n",
        "      \n",
        "      if (iteration + 1) % save_interval == 0:\n",
        "          \n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [D loss class: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, history.losses[-1], 100 * history.accs[-1]))\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "          losses.append(history.losses[-1])\n",
        "          accs.append(history.accs[-1])\n",
        "          discriminator_supervised.save_weights(\"./models/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "          \n",
        "          # x, y = dataset.training_set()\n",
        "          # _, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "          # print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGIfXoRgW-0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def pretrain(iterations_pre, batch_size, save_interval):\n",
        "#   for iteration in range(iterations_pre):\n",
        "#       # imgs, labels = dataset.training_set()\n",
        "#       imgs, labels = dataset.batch_labeled(1000)\n",
        "      \n",
        "#       loss, acc = discriminator_supervised.train_on_batch(imgs, labels)\n",
        "      \n",
        "#       if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "#           losses.append(loss)\n",
        "#           accs.append(acc)\n",
        "#           iteration_checkpoints.append(iteration + 1)\n",
        "          \n",
        "#           # Output training progress\n",
        "#           print(\n",
        "#               \"%d [D loss class: %.4f, acc: %.2f%%]\"\n",
        "#               % (iteration + 1, loss, 100 * acc))\n",
        "#           discriminator_supervised.save(\"./models/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "          \n",
        "#           # x, y = dataset.training_set()\n",
        "#           # _, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "#           # print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nIEI7nOhsUa",
        "colab_type": "code",
        "outputId": "ca48b2a9-0e1f-4d94-c6e1-6cf0280e559f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Pretrain\n",
        "\n",
        "# Set hyperparameters\n",
        "iterations_pre = 1\n",
        "iter_epochs = 40    # 20\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "losses = []\n",
        "accs = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "dataset = Dataset_CIFAR10(num_labeled)\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "pretrain(iterations_pre, batch_size, save_interval\n",
        "         , iter_epochs\n",
        "         )\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Use time:\" + str(endtime-starttime) + \"s\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 103s 66ms/step - loss: 1.5808 - acc: 0.4826 - val_loss: 1.8150 - val_acc: 0.4500\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.45000, saving model to /content/models/cifar10_model.001.h5\n",
            "Epoch 2/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 1.1992 - acc: 0.6297 - val_loss: 1.0872 - val_acc: 0.6709\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.45000 to 0.67090, saving model to /content/models/cifar10_model.002.h5\n",
            "Epoch 3/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 57ms/step - loss: 1.0207 - acc: 0.7006 - val_loss: 1.2265 - val_acc: 0.6507\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.67090\n",
            "Epoch 4/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.9298 - acc: 0.7329 - val_loss: 1.2809 - val_acc: 0.6587\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.67090\n",
            "Epoch 5/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.8577 - acc: 0.7603 - val_loss: 1.0542 - val_acc: 0.7087\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.67090 to 0.70870, saving model to /content/models/cifar10_model.005.h5\n",
            "Epoch 6/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.8189 - acc: 0.7766 - val_loss: 1.0047 - val_acc: 0.7193\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.70870 to 0.71930, saving model to /content/models/cifar10_model.006.h5\n",
            "Epoch 7/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.7821 - acc: 0.7890 - val_loss: 0.8666 - val_acc: 0.7683\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.71930 to 0.76830, saving model to /content/models/cifar10_model.007.h5\n",
            "Epoch 8/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 57ms/step - loss: 0.7518 - acc: 0.8014 - val_loss: 0.8860 - val_acc: 0.7659\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.76830\n",
            "Epoch 9/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 95s 61ms/step - loss: 0.7288 - acc: 0.8092 - val_loss: 0.7375 - val_acc: 0.8129\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.76830 to 0.81290, saving model to /content/models/cifar10_model.009.h5\n",
            "Epoch 10/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.7015 - acc: 0.8188 - val_loss: 0.9818 - val_acc: 0.7465\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81290\n",
            "Epoch 11/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.6936 - acc: 0.8230 - val_loss: 0.9713 - val_acc: 0.7414\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.81290\n",
            "Epoch 12/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6778 - acc: 0.8303 - val_loss: 0.8673 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.81290\n",
            "Epoch 13/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.6644 - acc: 0.8354 - val_loss: 0.7908 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.81290\n",
            "Epoch 14/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.6521 - acc: 0.8389 - val_loss: 0.8083 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.81290\n",
            "Epoch 15/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6439 - acc: 0.8427 - val_loss: 0.7838 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.81290\n",
            "Epoch 16/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.6332 - acc: 0.8470 - val_loss: 1.1694 - val_acc: 0.7222\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.81290\n",
            "Epoch 17/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6233 - acc: 0.8512 - val_loss: 1.0825 - val_acc: 0.7278\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.81290\n",
            "Epoch 18/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.6197 - acc: 0.8527 - val_loss: 0.7515 - val_acc: 0.8146\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.81290 to 0.81460, saving model to /content/models/cifar10_model.018.h5\n",
            "Epoch 19/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.6082 - acc: 0.8566 - val_loss: 0.7396 - val_acc: 0.8215\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.81460 to 0.82150, saving model to /content/models/cifar10_model.019.h5\n",
            "Epoch 20/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 95s 61ms/step - loss: 0.6055 - acc: 0.8597 - val_loss: 0.9240 - val_acc: 0.7650\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.82150\n",
            "Epoch 21/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5980 - acc: 0.8601 - val_loss: 0.7379 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.82150 to 0.82640, saving model to /content/models/cifar10_model.021.h5\n",
            "Epoch 22/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5940 - acc: 0.8651 - val_loss: 0.7044 - val_acc: 0.8275\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.82640 to 0.82750, saving model to /content/models/cifar10_model.022.h5\n",
            "Epoch 23/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 59ms/step - loss: 0.5865 - acc: 0.8659 - val_loss: 0.8063 - val_acc: 0.8033\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.82750\n",
            "Epoch 24/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.5812 - acc: 0.8689 - val_loss: 0.8172 - val_acc: 0.8031\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.82750\n",
            "Epoch 25/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5747 - acc: 0.8701 - val_loss: 1.0520 - val_acc: 0.7609\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.82750\n",
            "Epoch 26/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5741 - acc: 0.8705 - val_loss: 0.8520 - val_acc: 0.7920\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.82750\n",
            "Epoch 27/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5712 - acc: 0.8721 - val_loss: 0.7520 - val_acc: 0.8223\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.82750\n",
            "Epoch 28/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 59ms/step - loss: 0.5697 - acc: 0.8739 - val_loss: 0.8162 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.82750\n",
            "Epoch 29/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5658 - acc: 0.8758 - val_loss: 0.7576 - val_acc: 0.8133\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.82750\n",
            "Epoch 30/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5635 - acc: 0.8748 - val_loss: 0.6934 - val_acc: 0.8381\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.82750 to 0.83810, saving model to /content/models/cifar10_model.030.h5\n",
            "Epoch 31/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 91s 58ms/step - loss: 0.5554 - acc: 0.8798 - val_loss: 0.8414 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.83810\n",
            "Epoch 32/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5618 - acc: 0.8757 - val_loss: 0.7020 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.83810\n",
            "Epoch 33/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5493 - acc: 0.8821 - val_loss: 0.7654 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.83810\n",
            "Epoch 34/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5496 - acc: 0.8815 - val_loss: 0.7557 - val_acc: 0.8285\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.83810\n",
            "Epoch 35/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5474 - acc: 0.8817 - val_loss: 0.6423 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.83810 to 0.85680, saving model to /content/models/cifar10_model.035.h5\n",
            "Epoch 36/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5437 - acc: 0.8820 - val_loss: 0.6972 - val_acc: 0.8447\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.85680\n",
            "Epoch 37/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 93s 59ms/step - loss: 0.5420 - acc: 0.8823 - val_loss: 0.8670 - val_acc: 0.8009\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.85680\n",
            "Epoch 38/40\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 92s 59ms/step - loss: 0.5417 - acc: 0.8846 - val_loss: 0.7269 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.85680\n",
            "Epoch 39/40\n",
            "Learning rate:  0.001\n",
            "1462/1563 [===========================>..] - ETA: 5s - loss: 0.5345 - acc: 0.8864Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlf871AuWzWb",
        "colab_type": "code",
        "outputId": "ab7a7c08-a839-40b9-ceac-d447bfe1881f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "%ls models"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar10_model.001.h5  cifar10_model.009.h5  cifar10_model.030.h5\n",
            "cifar10_model.002.h5  cifar10_model.018.h5  cifar10_model.035.h5\n",
            "cifar10_model.005.h5  cifar10_model.019.h5  discriminator_supervised-1.h5\n",
            "cifar10_model.006.h5  cifar10_model.021.h5  \u001b[0m\u001b[01;34mmodels-label-1500\u001b[0m/\n",
            "cifar10_model.007.h5  cifar10_model.022.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lzo6IL5z9wE",
        "colab_type": "code",
        "outputId": "83039db5-41a9-4613-c9e6-be3ac5283c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "# tmodel.load_weights(\"./models/discriminator_supervised-2000.h5\", by_name=False)\n",
        "tmodel.load_weights(\"./models/cifar10_model.035.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 18s 360us/step\n",
            "Training Accuracy: 88.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etkdmP6Ez7Jy",
        "colab_type": "code",
        "outputId": "8cbe8134-1e7f-40a1-cbaf-036b02f1048d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/cifar10_model.035.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 352us/step\n",
            "Test Accuracy: 85.68%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xEVpUqmWb90",
        "colab_type": "code",
        "outputId": "50301db8-a35f-45bf-c533-43efb143ce38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "div = 1000\n",
        "ty = [history.accs[i*div] for i in range(0, len(history.accs)//div)]\n",
        "tx = [x for x in range(1*div, (len(ty)+1)*div, div)]\n",
        "print(max(ty))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, ty, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.96875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29d93d3cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFiCAYAAACkvHqaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTwghhBBCCCFmCP8f1huuQNEQc/sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOHm2K6nn-Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accs = []\n",
        "# tx = [x for x in range(100,2100,100)]\n",
        "# acc_max = [0,0]\n",
        "\n",
        "# x, y = dataset.test_set()\n",
        "\n",
        "# tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "# tmodel.compile(loss='categorical_crossentropy',\n",
        "#                          metrics=['accuracy'],\n",
        "#                          optimizer=Adam())\n",
        "\n",
        "# for e in tx:\n",
        "#   tmodel.load_weights(\"./models/discriminator_supervised-\"+ str(e) +\".h5\", by_name=False)\n",
        "#   _, acc = tmodel.evaluate(x, y)\n",
        "#   accs.append(acc)\n",
        "# print(max(accs))\n",
        "\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "# plt.xticks(tx, rotation=90)\n",
        "# plt.title(\"accs with epoch\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.ylabel(\"accs\")\n",
        "# plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSbSVx1khsOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations, batch_size, save_interval, iter_epochs, k):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    # Labels for real images: all ones\n",
        "    real = np.ones((batch_size, 1))\n",
        "\n",
        "    # Labels for fake images: all zeros\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        for _ in range(k):\n",
        "\n",
        "            # -------------------------\n",
        "            #  Train the Discriminator\n",
        "            # -------------------------\n",
        "\n",
        "            # Get labeled and unlabeled examples\n",
        "            imgs, labels = dataset.batch_labeled(batch_size)\n",
        "            imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "            # Generate a batch of fake images\n",
        "            z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "            fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "            fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "            gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "            discriminator_supervised.trainable = True\n",
        "            discriminator_unsupervised.trainable = True\n",
        "\n",
        "            # Train on real labeled examples\n",
        "            datagen.fit(imgs)\n",
        "            discriminator_supervised.fit_generator(datagen.flow(imgs, labels, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=iter_epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "            loss_d_supervised, acc_d_supervised = history.losses[-1], history.accs[-1]\n",
        "\n",
        "            # Train on real unlabeled examples\n",
        "            # Error\n",
        "            # datagen.fit(imgs_unlabeled)\n",
        "            # discriminator_unsupervised.fit_generator(datagen.flow(imgs_unlabeled, real, batch_size=batch_size),\n",
        "            #             validation_data=(x_test, np.ones((len(x_test), 1))),\n",
        "            #             epochs=iter_epochs, verbose=1, workers=4,\n",
        "            #             callbacks=callbacks)\n",
        "            # loss_d_unsupervised_real, acc_d_unsupervised_real = history.losses[-1], history.accs[-1]\n",
        "            loss_d_unsupervised_real, acc_d_unsupervised_real = discriminator_unsupervised.train_on_batch(imgs_unlabeled, real)\n",
        "\n",
        "            # Train on fake examples\n",
        "            loss_d_unsupervised_fake, acc_d_unsupervised_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "            # Calculate loss and acc\n",
        "            loss_d_unsupervised = 0.5 * np.add(loss_d_unsupervised_real, loss_d_unsupervised_fake)\n",
        "            loss_d = np.add(loss_d_supervised, loss_d_unsupervised)\n",
        "            acc_d_unsupervised = 0.5 * np.add(acc_d_unsupervised_real, acc_d_unsupervised_fake)\n",
        "            acc_d = np.add(acc_d_supervised, acc_d_unsupervised)\n",
        "        \n",
        "        # ---------------------\n",
        "        #  Train the Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "        fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "        gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "        discriminator_supervised.trainable = False\n",
        "        discriminator_unsupervised.trainable = False\n",
        "\n",
        "        # Train Generator\n",
        "        loss_g_unsupervised, acc_g_unsupervised = gan.train_on_batch([z,labels], real)\n",
        "\n",
        "        # Calculate loss and acc\n",
        "        loss_g = loss_g_unsupervised\n",
        "        acc_g = acc_g_unsupervised\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "            # Save losses to be plotted after training\n",
        "            losses_d_supervised.append(loss_d_supervised)\n",
        "            losses_d_unsupervised.append(loss_d_unsupervised)\n",
        "            losses_d_unsupervised_real.append(loss_d_unsupervised_real)\n",
        "            losses_d_unsupervised_fake.append(loss_d_unsupervised_fake)\n",
        "            losses_d.append(loss_d)\n",
        "            losses_g.append(loss_g)\n",
        "            \n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # Output training progress\n",
        "            print(\n",
        "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%]\"\n",
        "                % (iteration + 1, \n",
        "                   loss_d_supervised, 100 * acc_d_supervised,\n",
        "                   loss_d_unsupervised, 100 * acc_d_unsupervised, \n",
        "                   loss_g, 100 * acc_g))\n",
        "            \n",
        "            discriminator_supervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "            discriminator_unsupervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_unsupervised-\" + str(iteration+1) + \".h5\")\n",
        "            generator.save(\"./models/models-label-\" + str(num_labeled) + \"/generator-\" + str(iteration+1) + \".h5\")\n",
        "            file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_d_supervised.json\"\n",
        "            file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_d_unsupervised.json\"\n",
        "            file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/losses_g.json\"\n",
        "            with open(file1, 'w') as json_file:\n",
        "                  json.dump(str(losses_d_supervised), json_file)\n",
        "            with open(file2, 'w') as json_file:\n",
        "                  json.dump(str(losses_d_unsupervised), json_file)\n",
        "            with open(file3, 'w') as json_file:\n",
        "                  json.dump(str(losses_g), json_file)\n",
        "\n",
        "            # x,y = dataset.training_set()\n",
        "            # _, acc = discriminator_supervised.evaluate(x,y)\n",
        "            # print(str(100*acc)+\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__5V6FJn6xB",
        "colab_type": "code",
        "outputId": "563d3ac9-16bf-4e27-8542-2d9ea17d6d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 500 # 500\n",
        "iter_epochs = 1 # 1\n",
        "batch_size = 32 # 10\n",
        "save_interval = 10\n",
        "k = 1 # iteration of Discriminator\n",
        "\n",
        "losses_d_supervised = []\n",
        "losses_d_unsupervised = []\n",
        "losses_d_unsupervised_real = []\n",
        "losses_d_unsupervised_fake = []\n",
        "losses_d = []\n",
        "losses_g = []\n",
        "\n",
        "iteration_checkpoints = []\n",
        "\n",
        "# discriminator_supervised = load_model(\"./models/discriminator_supervised-1200.h5\")\n",
        "discriminator_supervised = load_model(\"./models/cifar10_model.035.h5\")\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the SCGAN-2D for the specified number of iterations\n",
        "train(iterations, batch_size, save_interval, iter_epochs, k)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.5412 - acc: 0.8438 - val_loss: 0.5922 - val_acc: 0.8710\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.86680 to 0.87100, saving model to /content/models/cifar10_model.001.h5\n",
            "1 [D loss supervised: 0.5412, acc.: 84.38%] [D loss unsupervised: 0.2966, acc.: 100.00%] [G loss: 1.467857, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4403 - acc: 0.8750 - val_loss: 0.6075 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87100\n",
            "2 [D loss supervised: 0.4403, acc.: 87.50%] [D loss unsupervised: 0.3055, acc.: 95.31%] [G loss: 1.847920, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4125 - acc: 0.9375 - val_loss: 0.5997 - val_acc: 0.8712\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.87100 to 0.87120, saving model to /content/models/cifar10_model.001.h5\n",
            "3 [D loss supervised: 0.4125, acc.: 93.75%] [D loss unsupervised: 0.2800, acc.: 98.44%] [G loss: 1.937941, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6461 - acc: 0.8438 - val_loss: 0.6016 - val_acc: 0.8712\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87120\n",
            "4 [D loss supervised: 0.6461, acc.: 84.38%] [D loss unsupervised: 0.2759, acc.: 98.44%] [G loss: 1.776596, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3852 - acc: 0.9375 - val_loss: 0.5820 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.87120 to 0.87620, saving model to /content/models/cifar10_model.001.h5\n",
            "5 [D loss supervised: 0.3852, acc.: 93.75%] [D loss unsupervised: 0.2778, acc.: 100.00%] [G loss: 2.074960, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5386 - acc: 0.9062 - val_loss: 0.5823 - val_acc: 0.8759\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87620\n",
            "6 [D loss supervised: 0.5386, acc.: 90.62%] [D loss unsupervised: 0.2885, acc.: 100.00%] [G loss: 2.313745, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8502 - acc: 0.7500 - val_loss: 0.5816 - val_acc: 0.8768\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.87620 to 0.87680, saving model to /content/models/cifar10_model.001.h5\n",
            "7 [D loss supervised: 0.8502, acc.: 75.00%] [D loss unsupervised: 0.2650, acc.: 100.00%] [G loss: 2.742483, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4040 - acc: 0.9062 - val_loss: 0.5818 - val_acc: 0.8767\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "8 [D loss supervised: 0.4040, acc.: 90.62%] [D loss unsupervised: 0.3039, acc.: 95.31%] [G loss: 2.885997, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4880 - acc: 0.9062 - val_loss: 0.5840 - val_acc: 0.8763\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "9 [D loss supervised: 0.4880, acc.: 90.62%] [D loss unsupervised: 0.2845, acc.: 96.88%] [G loss: 3.023329, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5519 - acc: 0.8438 - val_loss: 0.5919 - val_acc: 0.8740\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "10 [D loss supervised: 0.5519, acc.: 84.38%] [D loss unsupervised: 0.2924, acc.: 96.88%] [G loss: 3.060005, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6786 - acc: 0.8750 - val_loss: 0.5933 - val_acc: 0.8738\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "11 [D loss supervised: 0.6786, acc.: 87.50%] [D loss unsupervised: 0.2811, acc.: 98.44%] [G loss: 3.202062, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4929 - acc: 0.9062 - val_loss: 0.5975 - val_acc: 0.8723\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "12 [D loss supervised: 0.4929, acc.: 90.62%] [D loss unsupervised: 0.2750, acc.: 98.44%] [G loss: 3.214714, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4662 - acc: 0.9062 - val_loss: 0.6052 - val_acc: 0.8699\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "13 [D loss supervised: 0.4662, acc.: 90.62%] [D loss unsupervised: 0.2734, acc.: 98.44%] [G loss: 2.979795, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4919 - acc: 0.9062 - val_loss: 0.6125 - val_acc: 0.8667\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "14 [D loss supervised: 0.4919, acc.: 90.62%] [D loss unsupervised: 0.2585, acc.: 98.44%] [G loss: 2.809040, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6754 - acc: 0.8438 - val_loss: 0.6155 - val_acc: 0.8649\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "15 [D loss supervised: 0.6754, acc.: 84.38%] [D loss unsupervised: 0.2473, acc.: 100.00%] [G loss: 2.770586, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3444 - acc: 0.9688 - val_loss: 0.6214 - val_acc: 0.8642\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "16 [D loss supervised: 0.3444, acc.: 96.88%] [D loss unsupervised: 0.2558, acc.: 100.00%] [G loss: 2.677388, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3676 - acc: 0.9375 - val_loss: 0.6306 - val_acc: 0.8618\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "17 [D loss supervised: 0.3676, acc.: 93.75%] [D loss unsupervised: 0.2545, acc.: 98.44%] [G loss: 2.710370, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4790 - acc: 0.8438 - val_loss: 0.6401 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "18 [D loss supervised: 0.4790, acc.: 84.38%] [D loss unsupervised: 0.2457, acc.: 100.00%] [G loss: 2.829528, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4061 - acc: 0.9375 - val_loss: 0.6502 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "19 [D loss supervised: 0.4061, acc.: 93.75%] [D loss unsupervised: 0.2506, acc.: 100.00%] [G loss: 2.792872, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3459 - acc: 0.9688 - val_loss: 0.6618 - val_acc: 0.8570\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "20 [D loss supervised: 0.3459, acc.: 96.88%] [D loss unsupervised: 0.2503, acc.: 100.00%] [G loss: 2.652853, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5463 - acc: 0.8750 - val_loss: 0.6679 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "21 [D loss supervised: 0.5463, acc.: 87.50%] [D loss unsupervised: 0.2639, acc.: 100.00%] [G loss: 2.879297, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3067 - acc: 0.9688 - val_loss: 0.6730 - val_acc: 0.8506\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "22 [D loss supervised: 0.3067, acc.: 96.88%] [D loss unsupervised: 0.2404, acc.: 100.00%] [G loss: 3.007036, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5239 - acc: 0.9375 - val_loss: 0.6751 - val_acc: 0.8501\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "23 [D loss supervised: 0.5239, acc.: 93.75%] [D loss unsupervised: 0.2663, acc.: 98.44%] [G loss: 2.940410, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3966 - acc: 0.9062 - val_loss: 0.6819 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "24 [D loss supervised: 0.3966, acc.: 90.62%] [D loss unsupervised: 0.2643, acc.: 100.00%] [G loss: 3.038638, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4977 - acc: 0.9375 - val_loss: 0.6807 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "25 [D loss supervised: 0.4977, acc.: 93.75%] [D loss unsupervised: 0.3124, acc.: 98.44%] [G loss: 2.881484, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4359 - acc: 0.9375 - val_loss: 0.6841 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "26 [D loss supervised: 0.4359, acc.: 93.75%] [D loss unsupervised: 0.2380, acc.: 100.00%] [G loss: 2.972910, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5997 - acc: 0.8438 - val_loss: 0.6922 - val_acc: 0.8449\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "27 [D loss supervised: 0.5997, acc.: 84.38%] [D loss unsupervised: 0.2348, acc.: 100.00%] [G loss: 3.014970, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4799 - acc: 0.9062 - val_loss: 0.6998 - val_acc: 0.8435\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "28 [D loss supervised: 0.4799, acc.: 90.62%] [D loss unsupervised: 0.2618, acc.: 96.88%] [G loss: 3.371656, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5829 - acc: 0.8750 - val_loss: 0.7101 - val_acc: 0.8398\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "29 [D loss supervised: 0.5829, acc.: 87.50%] [D loss unsupervised: 0.2311, acc.: 100.00%] [G loss: 3.457724, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2912 - acc: 1.0000 - val_loss: 0.7214 - val_acc: 0.8381\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "30 [D loss supervised: 0.2912, acc.: 100.00%] [D loss unsupervised: 0.2491, acc.: 100.00%] [G loss: 3.547090, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3684 - acc: 0.9375 - val_loss: 0.7339 - val_acc: 0.8345\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "31 [D loss supervised: 0.3684, acc.: 93.75%] [D loss unsupervised: 0.2562, acc.: 100.00%] [G loss: 3.359709, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3665 - acc: 0.9375 - val_loss: 0.7434 - val_acc: 0.8307\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "32 [D loss supervised: 0.3665, acc.: 93.75%] [D loss unsupervised: 0.2385, acc.: 100.00%] [G loss: 3.563681, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4998 - acc: 0.8438 - val_loss: 0.7488 - val_acc: 0.8288\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "33 [D loss supervised: 0.4998, acc.: 84.38%] [D loss unsupervised: 0.2882, acc.: 100.00%] [G loss: 3.271981, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5267 - acc: 0.8438 - val_loss: 0.7553 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "34 [D loss supervised: 0.5267, acc.: 84.38%] [D loss unsupervised: 0.2389, acc.: 100.00%] [G loss: 3.354815, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3029 - acc: 0.9688 - val_loss: 0.7611 - val_acc: 0.8249\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "35 [D loss supervised: 0.3029, acc.: 96.88%] [D loss unsupervised: 0.2297, acc.: 100.00%] [G loss: 3.283721, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3589 - acc: 0.9375 - val_loss: 0.7666 - val_acc: 0.8235\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "36 [D loss supervised: 0.3589, acc.: 93.75%] [D loss unsupervised: 0.2291, acc.: 100.00%] [G loss: 3.184906, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5882 - acc: 0.9062 - val_loss: 0.7770 - val_acc: 0.8210\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "37 [D loss supervised: 0.5882, acc.: 90.62%] [D loss unsupervised: 0.2596, acc.: 98.44%] [G loss: 2.819465, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4233 - acc: 0.9062 - val_loss: 0.7897 - val_acc: 0.8200\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "38 [D loss supervised: 0.4233, acc.: 90.62%] [D loss unsupervised: 0.2502, acc.: 98.44%] [G loss: 2.412357, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5395 - acc: 0.8750 - val_loss: 0.8009 - val_acc: 0.8187\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "39 [D loss supervised: 0.5395, acc.: 87.50%] [D loss unsupervised: 0.2354, acc.: 100.00%] [G loss: 1.752187, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3043 - acc: 0.9688 - val_loss: 0.8028 - val_acc: 0.8190\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "40 [D loss supervised: 0.3043, acc.: 96.88%] [D loss unsupervised: 0.2488, acc.: 100.00%] [G loss: 1.460318, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4471 - acc: 0.9062 - val_loss: 0.8039 - val_acc: 0.8193\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "41 [D loss supervised: 0.4471, acc.: 90.62%] [D loss unsupervised: 0.2656, acc.: 100.00%] [G loss: 1.768502, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3295 - acc: 0.9375 - val_loss: 0.8081 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "42 [D loss supervised: 0.3295, acc.: 93.75%] [D loss unsupervised: 0.2483, acc.: 100.00%] [G loss: 2.395546, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3427 - acc: 0.9688 - val_loss: 0.8156 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "43 [D loss supervised: 0.3427, acc.: 96.88%] [D loss unsupervised: 0.2479, acc.: 100.00%] [G loss: 2.637100, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5414 - acc: 0.9375 - val_loss: 0.8191 - val_acc: 0.8169\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "44 [D loss supervised: 0.5414, acc.: 93.75%] [D loss unsupervised: 0.2302, acc.: 100.00%] [G loss: 2.848704, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7280 - acc: 0.7812 - val_loss: 0.8291 - val_acc: 0.8140\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "45 [D loss supervised: 0.7280, acc.: 78.12%] [D loss unsupervised: 0.2346, acc.: 100.00%] [G loss: 2.688225, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3684 - acc: 0.9375 - val_loss: 0.8321 - val_acc: 0.8137\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "46 [D loss supervised: 0.3684, acc.: 93.75%] [D loss unsupervised: 0.2313, acc.: 100.00%] [G loss: 1.767336, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5553 - acc: 0.8750 - val_loss: 0.8340 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "47 [D loss supervised: 0.5553, acc.: 87.50%] [D loss unsupervised: 0.2451, acc.: 100.00%] [G loss: 1.347567, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5679 - acc: 0.9062 - val_loss: 0.8328 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "48 [D loss supervised: 0.5679, acc.: 90.62%] [D loss unsupervised: 0.2346, acc.: 100.00%] [G loss: 1.783210, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3857 - acc: 0.9062 - val_loss: 0.8291 - val_acc: 0.8134\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "49 [D loss supervised: 0.3857, acc.: 90.62%] [D loss unsupervised: 0.2338, acc.: 100.00%] [G loss: 2.011524, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3763 - acc: 0.9062 - val_loss: 0.8255 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "50 [D loss supervised: 0.3763, acc.: 90.62%] [D loss unsupervised: 0.2659, acc.: 98.44%] [G loss: 1.967727, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4267 - acc: 0.9375 - val_loss: 0.8222 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "51 [D loss supervised: 0.4267, acc.: 93.75%] [D loss unsupervised: 0.2539, acc.: 98.44%] [G loss: 2.368564, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5994 - acc: 0.8750 - val_loss: 0.8252 - val_acc: 0.8146\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "52 [D loss supervised: 0.5994, acc.: 87.50%] [D loss unsupervised: 0.2496, acc.: 98.44%] [G loss: 1.481684, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6179 - acc: 0.8125 - val_loss: 0.8336 - val_acc: 0.8141\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "53 [D loss supervised: 0.6179, acc.: 81.25%] [D loss unsupervised: 0.2510, acc.: 98.44%] [G loss: 1.813754, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3986 - acc: 0.9375 - val_loss: 0.8414 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "54 [D loss supervised: 0.3986, acc.: 93.75%] [D loss unsupervised: 0.2216, acc.: 100.00%] [G loss: 1.867898, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6275 - acc: 0.8438 - val_loss: 0.8298 - val_acc: 0.8129\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "55 [D loss supervised: 0.6275, acc.: 84.38%] [D loss unsupervised: 0.2331, acc.: 100.00%] [G loss: 2.000415, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5955 - acc: 0.9062 - val_loss: 0.8167 - val_acc: 0.8159\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "56 [D loss supervised: 0.5955, acc.: 90.62%] [D loss unsupervised: 0.2173, acc.: 100.00%] [G loss: 2.109390, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5838 - acc: 0.8438 - val_loss: 0.7960 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "57 [D loss supervised: 0.5838, acc.: 84.38%] [D loss unsupervised: 0.2125, acc.: 100.00%] [G loss: 1.754953, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3791 - acc: 0.9375 - val_loss: 0.7748 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "58 [D loss supervised: 0.3791, acc.: 93.75%] [D loss unsupervised: 0.2260, acc.: 100.00%] [G loss: 2.019485, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5186 - acc: 0.9062 - val_loss: 0.7554 - val_acc: 0.8293\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "59 [D loss supervised: 0.5186, acc.: 90.62%] [D loss unsupervised: 0.2299, acc.: 100.00%] [G loss: 1.532098, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4511 - acc: 0.9062 - val_loss: 0.7400 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "60 [D loss supervised: 0.4511, acc.: 90.62%] [D loss unsupervised: 0.2149, acc.: 100.00%] [G loss: 1.648492, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6258 - acc: 0.8750 - val_loss: 0.7235 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "61 [D loss supervised: 0.6258, acc.: 87.50%] [D loss unsupervised: 0.2139, acc.: 100.00%] [G loss: 1.084772, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5786 - acc: 0.8438 - val_loss: 0.7057 - val_acc: 0.8422\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "62 [D loss supervised: 0.5786, acc.: 84.38%] [D loss unsupervised: 0.2248, acc.: 100.00%] [G loss: 1.194712, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3326 - acc: 0.9375 - val_loss: 0.6933 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "63 [D loss supervised: 0.3326, acc.: 93.75%] [D loss unsupervised: 0.2162, acc.: 100.00%] [G loss: 0.987396, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3790 - acc: 0.9688 - val_loss: 0.6866 - val_acc: 0.8466\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "64 [D loss supervised: 0.3790, acc.: 96.88%] [D loss unsupervised: 0.2172, acc.: 100.00%] [G loss: 0.705806, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3564 - acc: 0.9375 - val_loss: 0.6830 - val_acc: 0.8469\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "65 [D loss supervised: 0.3564, acc.: 93.75%] [D loss unsupervised: 0.2164, acc.: 100.00%] [G loss: 0.822104, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3314 - acc: 0.9688 - val_loss: 0.6808 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "66 [D loss supervised: 0.3314, acc.: 96.88%] [D loss unsupervised: 0.2164, acc.: 100.00%] [G loss: 1.514272, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4638 - acc: 0.9062 - val_loss: 0.6797 - val_acc: 0.8482\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "67 [D loss supervised: 0.4638, acc.: 90.62%] [D loss unsupervised: 0.2141, acc.: 100.00%] [G loss: 0.785259, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3973 - acc: 0.9062 - val_loss: 0.6814 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "68 [D loss supervised: 0.3973, acc.: 90.62%] [D loss unsupervised: 0.2100, acc.: 100.00%] [G loss: 0.999223, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4036 - acc: 0.9062 - val_loss: 0.6835 - val_acc: 0.8463\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "69 [D loss supervised: 0.4036, acc.: 90.62%] [D loss unsupervised: 0.2098, acc.: 100.00%] [G loss: 1.022411, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4470 - acc: 0.9375 - val_loss: 0.6895 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "70 [D loss supervised: 0.4470, acc.: 93.75%] [D loss unsupervised: 0.2084, acc.: 100.00%] [G loss: 0.963734, acc.: 56.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3788 - acc: 0.9062 - val_loss: 0.6963 - val_acc: 0.8437\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "71 [D loss supervised: 0.3788, acc.: 90.62%] [D loss unsupervised: 0.2054, acc.: 100.00%] [G loss: 0.731563, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4456 - acc: 0.9062 - val_loss: 0.7073 - val_acc: 0.8408\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "72 [D loss supervised: 0.4456, acc.: 90.62%] [D loss unsupervised: 0.2125, acc.: 100.00%] [G loss: 0.673303, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3289 - acc: 0.9688 - val_loss: 0.7165 - val_acc: 0.8384\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "73 [D loss supervised: 0.3289, acc.: 96.88%] [D loss unsupervised: 0.2225, acc.: 98.44%] [G loss: 0.834085, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3970 - acc: 0.9062 - val_loss: 0.7239 - val_acc: 0.8370\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "74 [D loss supervised: 0.3970, acc.: 90.62%] [D loss unsupervised: 0.2053, acc.: 100.00%] [G loss: 0.649260, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3872 - acc: 0.9375 - val_loss: 0.7356 - val_acc: 0.8357\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "75 [D loss supervised: 0.3872, acc.: 93.75%] [D loss unsupervised: 0.2126, acc.: 100.00%] [G loss: 0.595431, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3982 - acc: 0.9062 - val_loss: 0.7419 - val_acc: 0.8350\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "76 [D loss supervised: 0.3982, acc.: 90.62%] [D loss unsupervised: 0.2113, acc.: 100.00%] [G loss: 0.582028, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4159 - acc: 0.9062 - val_loss: 0.7338 - val_acc: 0.8371\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "77 [D loss supervised: 0.4159, acc.: 90.62%] [D loss unsupervised: 0.2141, acc.: 100.00%] [G loss: 0.450163, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2562 - acc: 1.0000 - val_loss: 0.7272 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "78 [D loss supervised: 0.2562, acc.: 100.00%] [D loss unsupervised: 0.2133, acc.: 100.00%] [G loss: 0.482822, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4406 - acc: 0.9062 - val_loss: 0.7235 - val_acc: 0.8381\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "79 [D loss supervised: 0.4406, acc.: 90.62%] [D loss unsupervised: 0.2100, acc.: 100.00%] [G loss: 0.348407, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4112 - acc: 0.9375 - val_loss: 0.7143 - val_acc: 0.8389\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "80 [D loss supervised: 0.4112, acc.: 93.75%] [D loss unsupervised: 0.2127, acc.: 100.00%] [G loss: 0.534153, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4056 - acc: 0.9375 - val_loss: 0.7027 - val_acc: 0.8412\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "81 [D loss supervised: 0.4056, acc.: 93.75%] [D loss unsupervised: 0.2076, acc.: 100.00%] [G loss: 0.334237, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4704 - acc: 0.9375 - val_loss: 0.6908 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "82 [D loss supervised: 0.4704, acc.: 93.75%] [D loss unsupervised: 0.2107, acc.: 100.00%] [G loss: 0.417596, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5775 - acc: 0.8125 - val_loss: 0.6759 - val_acc: 0.8492\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "83 [D loss supervised: 0.5775, acc.: 81.25%] [D loss unsupervised: 0.2404, acc.: 98.44%] [G loss: 0.255151, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3191 - acc: 0.9688 - val_loss: 0.6671 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "84 [D loss supervised: 0.3191, acc.: 96.88%] [D loss unsupervised: 0.2054, acc.: 100.00%] [G loss: 0.304271, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6767 - acc: 0.8438 - val_loss: 0.6640 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "85 [D loss supervised: 0.6767, acc.: 84.38%] [D loss unsupervised: 0.2104, acc.: 100.00%] [G loss: 0.313980, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4829 - acc: 0.8750 - val_loss: 0.6652 - val_acc: 0.8519\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "86 [D loss supervised: 0.4829, acc.: 87.50%] [D loss unsupervised: 0.2065, acc.: 100.00%] [G loss: 0.272075, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4375 - acc: 0.8750 - val_loss: 0.6667 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "87 [D loss supervised: 0.4375, acc.: 87.50%] [D loss unsupervised: 0.2078, acc.: 100.00%] [G loss: 0.303854, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3365 - acc: 0.9688 - val_loss: 0.6694 - val_acc: 0.8512\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "88 [D loss supervised: 0.3365, acc.: 96.88%] [D loss unsupervised: 0.2033, acc.: 100.00%] [G loss: 0.249633, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4010 - acc: 0.9375 - val_loss: 0.6717 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "89 [D loss supervised: 0.4010, acc.: 93.75%] [D loss unsupervised: 0.2057, acc.: 100.00%] [G loss: 0.279845, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4654 - acc: 0.9375 - val_loss: 0.6753 - val_acc: 0.8506\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "90 [D loss supervised: 0.4654, acc.: 93.75%] [D loss unsupervised: 0.2090, acc.: 100.00%] [G loss: 0.368382, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4061 - acc: 0.9375 - val_loss: 0.6834 - val_acc: 0.8476\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "91 [D loss supervised: 0.4061, acc.: 93.75%] [D loss unsupervised: 0.2010, acc.: 100.00%] [G loss: 0.308264, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4945 - acc: 0.9062 - val_loss: 0.6874 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "92 [D loss supervised: 0.4945, acc.: 90.62%] [D loss unsupervised: 0.2029, acc.: 100.00%] [G loss: 0.359028, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4439 - acc: 0.8750 - val_loss: 0.6924 - val_acc: 0.8446\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "93 [D loss supervised: 0.4439, acc.: 87.50%] [D loss unsupervised: 0.2046, acc.: 100.00%] [G loss: 0.330778, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3566 - acc: 0.9375 - val_loss: 0.6983 - val_acc: 0.8430\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "94 [D loss supervised: 0.3566, acc.: 93.75%] [D loss unsupervised: 0.2034, acc.: 100.00%] [G loss: 0.279690, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3906 - acc: 0.9062 - val_loss: 0.7040 - val_acc: 0.8428\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "95 [D loss supervised: 0.3906, acc.: 90.62%] [D loss unsupervised: 0.2004, acc.: 100.00%] [G loss: 0.295281, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3406 - acc: 0.9375 - val_loss: 0.7121 - val_acc: 0.8404\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "96 [D loss supervised: 0.3406, acc.: 93.75%] [D loss unsupervised: 0.2026, acc.: 100.00%] [G loss: 0.275100, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3651 - acc: 0.9688 - val_loss: 0.7189 - val_acc: 0.8391\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "97 [D loss supervised: 0.3651, acc.: 96.88%] [D loss unsupervised: 0.2045, acc.: 100.00%] [G loss: 0.299148, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3962 - acc: 0.9375 - val_loss: 0.7234 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "98 [D loss supervised: 0.3962, acc.: 93.75%] [D loss unsupervised: 0.2045, acc.: 100.00%] [G loss: 0.255254, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4212 - acc: 0.9375 - val_loss: 0.7246 - val_acc: 0.8376\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "99 [D loss supervised: 0.4212, acc.: 93.75%] [D loss unsupervised: 0.1991, acc.: 100.00%] [G loss: 0.302656, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7373 - acc: 0.8438 - val_loss: 0.7324 - val_acc: 0.8356\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "100 [D loss supervised: 0.7373, acc.: 84.38%] [D loss unsupervised: 0.1997, acc.: 100.00%] [G loss: 0.319725, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3458 - acc: 0.9688 - val_loss: 0.7409 - val_acc: 0.8325\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "101 [D loss supervised: 0.3458, acc.: 96.88%] [D loss unsupervised: 0.2005, acc.: 100.00%] [G loss: 0.248603, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5539 - acc: 0.8438 - val_loss: 0.7441 - val_acc: 0.8309\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "102 [D loss supervised: 0.5539, acc.: 84.38%] [D loss unsupervised: 0.1996, acc.: 100.00%] [G loss: 0.245983, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4667 - acc: 0.9062 - val_loss: 0.7504 - val_acc: 0.8285\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "103 [D loss supervised: 0.4667, acc.: 90.62%] [D loss unsupervised: 0.1998, acc.: 100.00%] [G loss: 0.271288, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4336 - acc: 0.9062 - val_loss: 0.7511 - val_acc: 0.8287\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "104 [D loss supervised: 0.4336, acc.: 90.62%] [D loss unsupervised: 0.2053, acc.: 100.00%] [G loss: 0.353889, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5071 - acc: 0.8750 - val_loss: 0.7443 - val_acc: 0.8283\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "105 [D loss supervised: 0.5071, acc.: 87.50%] [D loss unsupervised: 0.2021, acc.: 100.00%] [G loss: 0.250616, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3556 - acc: 0.9375 - val_loss: 0.7393 - val_acc: 0.8282\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "106 [D loss supervised: 0.3556, acc.: 93.75%] [D loss unsupervised: 0.2128, acc.: 100.00%] [G loss: 0.296752, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4795 - acc: 0.8438 - val_loss: 0.7337 - val_acc: 0.8285\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "107 [D loss supervised: 0.4795, acc.: 84.38%] [D loss unsupervised: 0.2065, acc.: 100.00%] [G loss: 0.243734, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5070 - acc: 0.8750 - val_loss: 0.7206 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "108 [D loss supervised: 0.5070, acc.: 87.50%] [D loss unsupervised: 0.2069, acc.: 100.00%] [G loss: 0.275559, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4149 - acc: 0.9688 - val_loss: 0.7086 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "109 [D loss supervised: 0.4149, acc.: 96.88%] [D loss unsupervised: 0.1985, acc.: 100.00%] [G loss: 0.335974, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4551 - acc: 0.8438 - val_loss: 0.6990 - val_acc: 0.8398\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "110 [D loss supervised: 0.4551, acc.: 84.38%] [D loss unsupervised: 0.1997, acc.: 100.00%] [G loss: 0.244767, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3797 - acc: 0.9688 - val_loss: 0.6902 - val_acc: 0.8436\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "111 [D loss supervised: 0.3797, acc.: 96.88%] [D loss unsupervised: 0.1982, acc.: 100.00%] [G loss: 0.255556, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3635 - acc: 0.9688 - val_loss: 0.6843 - val_acc: 0.8459\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "112 [D loss supervised: 0.3635, acc.: 96.88%] [D loss unsupervised: 0.2004, acc.: 100.00%] [G loss: 0.308300, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3744 - acc: 0.9375 - val_loss: 0.6802 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "113 [D loss supervised: 0.3744, acc.: 93.75%] [D loss unsupervised: 0.2015, acc.: 100.00%] [G loss: 0.276462, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3383 - acc: 0.9375 - val_loss: 0.6789 - val_acc: 0.8483\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "114 [D loss supervised: 0.3383, acc.: 93.75%] [D loss unsupervised: 0.1971, acc.: 100.00%] [G loss: 0.339244, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4243 - acc: 0.9062 - val_loss: 0.6795 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "115 [D loss supervised: 0.4243, acc.: 90.62%] [D loss unsupervised: 0.2055, acc.: 100.00%] [G loss: 0.396164, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3675 - acc: 0.9375 - val_loss: 0.6790 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "116 [D loss supervised: 0.3675, acc.: 93.75%] [D loss unsupervised: 0.2091, acc.: 100.00%] [G loss: 0.481619, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6154 - acc: 0.8438 - val_loss: 0.6726 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "117 [D loss supervised: 0.6154, acc.: 84.38%] [D loss unsupervised: 0.1942, acc.: 100.00%] [G loss: 0.584723, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2889 - acc: 0.9688 - val_loss: 0.6695 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "118 [D loss supervised: 0.2889, acc.: 96.88%] [D loss unsupervised: 0.2108, acc.: 100.00%] [G loss: 0.359756, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4314 - acc: 0.9688 - val_loss: 0.6659 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "119 [D loss supervised: 0.4314, acc.: 96.88%] [D loss unsupervised: 0.2001, acc.: 100.00%] [G loss: 0.458572, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3221 - acc: 0.9688 - val_loss: 0.6623 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "120 [D loss supervised: 0.3221, acc.: 96.88%] [D loss unsupervised: 0.1990, acc.: 100.00%] [G loss: 0.314739, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3814 - acc: 0.9062 - val_loss: 0.6567 - val_acc: 0.8492\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "121 [D loss supervised: 0.3814, acc.: 90.62%] [D loss unsupervised: 0.2017, acc.: 100.00%] [G loss: 0.328498, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3891 - acc: 0.8438 - val_loss: 0.6481 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "122 [D loss supervised: 0.3891, acc.: 84.38%] [D loss unsupervised: 0.1959, acc.: 100.00%] [G loss: 0.286401, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3348 - acc: 1.0000 - val_loss: 0.6405 - val_acc: 0.8534\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "123 [D loss supervised: 0.3348, acc.: 100.00%] [D loss unsupervised: 0.2152, acc.: 98.44%] [G loss: 0.279672, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3805 - acc: 0.9375 - val_loss: 0.6331 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "124 [D loss supervised: 0.3805, acc.: 93.75%] [D loss unsupervised: 0.1967, acc.: 100.00%] [G loss: 0.228480, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2982 - acc: 1.0000 - val_loss: 0.6264 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "125 [D loss supervised: 0.2982, acc.: 100.00%] [D loss unsupervised: 0.1993, acc.: 100.00%] [G loss: 0.258581, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4054 - acc: 0.9375 - val_loss: 0.6216 - val_acc: 0.8602\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "126 [D loss supervised: 0.4054, acc.: 93.75%] [D loss unsupervised: 0.2000, acc.: 100.00%] [G loss: 0.259475, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3528 - acc: 0.9375 - val_loss: 0.6178 - val_acc: 0.8588\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "127 [D loss supervised: 0.3528, acc.: 93.75%] [D loss unsupervised: 0.1982, acc.: 100.00%] [G loss: 0.231461, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3789 - acc: 0.9688 - val_loss: 0.6159 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "128 [D loss supervised: 0.3789, acc.: 96.88%] [D loss unsupervised: 0.1940, acc.: 100.00%] [G loss: 0.255032, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3868 - acc: 0.9375 - val_loss: 0.6151 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "129 [D loss supervised: 0.3868, acc.: 93.75%] [D loss unsupervised: 0.1964, acc.: 100.00%] [G loss: 0.242829, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3896 - acc: 0.9688 - val_loss: 0.6153 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "130 [D loss supervised: 0.3896, acc.: 96.88%] [D loss unsupervised: 0.1929, acc.: 100.00%] [G loss: 0.265448, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2778 - acc: 1.0000 - val_loss: 0.6169 - val_acc: 0.8616\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "131 [D loss supervised: 0.2778, acc.: 100.00%] [D loss unsupervised: 0.1965, acc.: 100.00%] [G loss: 0.325961, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3183 - acc: 0.9375 - val_loss: 0.6181 - val_acc: 0.8619\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "132 [D loss supervised: 0.3183, acc.: 93.75%] [D loss unsupervised: 0.1971, acc.: 100.00%] [G loss: 0.237766, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2879 - acc: 1.0000 - val_loss: 0.6203 - val_acc: 0.8617\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "133 [D loss supervised: 0.2879, acc.: 100.00%] [D loss unsupervised: 0.2004, acc.: 100.00%] [G loss: 0.336588, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2837 - acc: 1.0000 - val_loss: 0.6231 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "134 [D loss supervised: 0.2837, acc.: 100.00%] [D loss unsupervised: 0.1935, acc.: 100.00%] [G loss: 0.383740, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3477 - acc: 0.9375 - val_loss: 0.6249 - val_acc: 0.8618\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "135 [D loss supervised: 0.3477, acc.: 93.75%] [D loss unsupervised: 0.1920, acc.: 100.00%] [G loss: 0.435940, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2544 - acc: 1.0000 - val_loss: 0.6257 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "136 [D loss supervised: 0.2544, acc.: 100.00%] [D loss unsupervised: 0.1953, acc.: 100.00%] [G loss: 0.382984, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4684 - acc: 0.8750 - val_loss: 0.6282 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "137 [D loss supervised: 0.4684, acc.: 87.50%] [D loss unsupervised: 0.1976, acc.: 100.00%] [G loss: 0.427846, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3080 - acc: 0.9688 - val_loss: 0.6307 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "138 [D loss supervised: 0.3080, acc.: 96.88%] [D loss unsupervised: 0.1958, acc.: 100.00%] [G loss: 0.397323, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3093 - acc: 0.9688 - val_loss: 0.6342 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "139 [D loss supervised: 0.3093, acc.: 96.88%] [D loss unsupervised: 0.2162, acc.: 100.00%] [G loss: 0.252274, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3567 - acc: 0.9375 - val_loss: 0.6381 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "140 [D loss supervised: 0.3567, acc.: 93.75%] [D loss unsupervised: 0.2023, acc.: 100.00%] [G loss: 0.322514, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3004 - acc: 1.0000 - val_loss: 0.6415 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "141 [D loss supervised: 0.3004, acc.: 100.00%] [D loss unsupervised: 0.1968, acc.: 100.00%] [G loss: 0.612753, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3711 - acc: 0.9062 - val_loss: 0.6432 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "142 [D loss supervised: 0.3711, acc.: 90.62%] [D loss unsupervised: 0.1919, acc.: 100.00%] [G loss: 1.292829, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4939 - acc: 0.9062 - val_loss: 0.6379 - val_acc: 0.8582\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "143 [D loss supervised: 0.4939, acc.: 90.62%] [D loss unsupervised: 0.1931, acc.: 100.00%] [G loss: 0.810477, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2656 - acc: 0.9688 - val_loss: 0.6348 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "144 [D loss supervised: 0.2656, acc.: 96.88%] [D loss unsupervised: 0.2036, acc.: 100.00%] [G loss: 0.453584, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4490 - acc: 0.9375 - val_loss: 0.6342 - val_acc: 0.8556\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "145 [D loss supervised: 0.4490, acc.: 93.75%] [D loss unsupervised: 0.1991, acc.: 100.00%] [G loss: 0.728045, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4761 - acc: 0.9375 - val_loss: 0.6360 - val_acc: 0.8561\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "146 [D loss supervised: 0.4761, acc.: 93.75%] [D loss unsupervised: 0.1939, acc.: 100.00%] [G loss: 1.080369, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2963 - acc: 0.9688 - val_loss: 0.6402 - val_acc: 0.8566\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "147 [D loss supervised: 0.2963, acc.: 96.88%] [D loss unsupervised: 0.1921, acc.: 100.00%] [G loss: 1.166160, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3404 - acc: 0.9688 - val_loss: 0.6440 - val_acc: 0.8545\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "148 [D loss supervised: 0.3404, acc.: 96.88%] [D loss unsupervised: 0.2031, acc.: 100.00%] [G loss: 1.266680, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2512 - acc: 1.0000 - val_loss: 0.6466 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "149 [D loss supervised: 0.2512, acc.: 100.00%] [D loss unsupervised: 0.1971, acc.: 100.00%] [G loss: 0.771612, acc.: 65.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3129 - acc: 0.9688 - val_loss: 0.6453 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "150 [D loss supervised: 0.3129, acc.: 96.88%] [D loss unsupervised: 0.2248, acc.: 98.44%] [G loss: 0.381371, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3201 - acc: 0.9375 - val_loss: 0.6444 - val_acc: 0.8556\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "151 [D loss supervised: 0.3201, acc.: 93.75%] [D loss unsupervised: 0.2045, acc.: 100.00%] [G loss: 0.567256, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2804 - acc: 1.0000 - val_loss: 0.6427 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "152 [D loss supervised: 0.2804, acc.: 100.00%] [D loss unsupervised: 0.1970, acc.: 100.00%] [G loss: 0.470065, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3649 - acc: 0.9375 - val_loss: 0.6434 - val_acc: 0.8541\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "153 [D loss supervised: 0.3649, acc.: 93.75%] [D loss unsupervised: 0.1944, acc.: 100.00%] [G loss: 1.304121, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2614 - acc: 1.0000 - val_loss: 0.6443 - val_acc: 0.8546\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "154 [D loss supervised: 0.2614, acc.: 100.00%] [D loss unsupervised: 0.1964, acc.: 100.00%] [G loss: 0.802815, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3192 - acc: 0.9688 - val_loss: 0.6419 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "155 [D loss supervised: 0.3192, acc.: 96.88%] [D loss unsupervised: 0.1960, acc.: 100.00%] [G loss: 0.777039, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 0.6384 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "156 [D loss supervised: 0.2705, acc.: 100.00%] [D loss unsupervised: 0.1953, acc.: 100.00%] [G loss: 0.677087, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4270 - acc: 0.9375 - val_loss: 0.6281 - val_acc: 0.8617\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "157 [D loss supervised: 0.4270, acc.: 93.75%] [D loss unsupervised: 0.1960, acc.: 100.00%] [G loss: 0.591675, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3459 - acc: 0.9688 - val_loss: 0.6209 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "158 [D loss supervised: 0.3459, acc.: 96.88%] [D loss unsupervised: 0.1981, acc.: 100.00%] [G loss: 0.474534, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3588 - acc: 0.9375 - val_loss: 0.6187 - val_acc: 0.8644\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "159 [D loss supervised: 0.3588, acc.: 93.75%] [D loss unsupervised: 0.1914, acc.: 100.00%] [G loss: 0.322312, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3359 - acc: 0.9688 - val_loss: 0.6191 - val_acc: 0.8637\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "160 [D loss supervised: 0.3359, acc.: 96.88%] [D loss unsupervised: 0.2046, acc.: 100.00%] [G loss: 0.302545, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3394 - acc: 1.0000 - val_loss: 0.6172 - val_acc: 0.8644\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "161 [D loss supervised: 0.3394, acc.: 100.00%] [D loss unsupervised: 0.1927, acc.: 100.00%] [G loss: 0.451356, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3114 - acc: 0.9688 - val_loss: 0.6161 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "162 [D loss supervised: 0.3114, acc.: 96.88%] [D loss unsupervised: 0.1953, acc.: 100.00%] [G loss: 0.305910, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3899 - acc: 0.9375 - val_loss: 0.6193 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "163 [D loss supervised: 0.3899, acc.: 93.75%] [D loss unsupervised: 0.1900, acc.: 100.00%] [G loss: 0.238989, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5221 - acc: 0.8750 - val_loss: 0.6237 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "164 [D loss supervised: 0.5221, acc.: 87.50%] [D loss unsupervised: 0.1895, acc.: 100.00%] [G loss: 0.245507, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2577 - acc: 1.0000 - val_loss: 0.6282 - val_acc: 0.8588\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "165 [D loss supervised: 0.2577, acc.: 100.00%] [D loss unsupervised: 0.1977, acc.: 100.00%] [G loss: 0.251860, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3587 - acc: 0.9375 - val_loss: 0.6336 - val_acc: 0.8569\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "166 [D loss supervised: 0.3587, acc.: 93.75%] [D loss unsupervised: 0.1876, acc.: 100.00%] [G loss: 0.382869, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3111 - acc: 0.9688 - val_loss: 0.6396 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "167 [D loss supervised: 0.3111, acc.: 96.88%] [D loss unsupervised: 0.1875, acc.: 100.00%] [G loss: 0.278977, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4912 - acc: 0.9062 - val_loss: 0.6441 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "168 [D loss supervised: 0.4912, acc.: 90.62%] [D loss unsupervised: 0.1900, acc.: 100.00%] [G loss: 0.306897, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3906 - acc: 0.9688 - val_loss: 0.6441 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "169 [D loss supervised: 0.3906, acc.: 96.88%] [D loss unsupervised: 0.1888, acc.: 100.00%] [G loss: 0.262318, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3572 - acc: 0.9688 - val_loss: 0.6440 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "170 [D loss supervised: 0.3572, acc.: 96.88%] [D loss unsupervised: 0.2052, acc.: 100.00%] [G loss: 0.212178, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3802 - acc: 0.9375 - val_loss: 0.6423 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "171 [D loss supervised: 0.3802, acc.: 93.75%] [D loss unsupervised: 0.1890, acc.: 100.00%] [G loss: 0.191064, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2732 - acc: 0.9688 - val_loss: 0.6420 - val_acc: 0.8590\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "172 [D loss supervised: 0.2732, acc.: 96.88%] [D loss unsupervised: 0.2011, acc.: 100.00%] [G loss: 0.195483, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3600 - acc: 0.9375 - val_loss: 0.6437 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "173 [D loss supervised: 0.3600, acc.: 93.75%] [D loss unsupervised: 0.1900, acc.: 100.00%] [G loss: 0.219801, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2297 - acc: 1.0000 - val_loss: 0.6453 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "174 [D loss supervised: 0.2297, acc.: 100.00%] [D loss unsupervised: 0.1866, acc.: 100.00%] [G loss: 0.215623, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4170 - acc: 0.9688 - val_loss: 0.6451 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "175 [D loss supervised: 0.4170, acc.: 96.88%] [D loss unsupervised: 0.1860, acc.: 100.00%] [G loss: 0.314583, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6127 - acc: 0.8750 - val_loss: 0.6407 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "176 [D loss supervised: 0.6127, acc.: 87.50%] [D loss unsupervised: 0.1856, acc.: 100.00%] [G loss: 0.212004, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2965 - acc: 1.0000 - val_loss: 0.6404 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "177 [D loss supervised: 0.2965, acc.: 100.00%] [D loss unsupervised: 0.1863, acc.: 100.00%] [G loss: 0.423897, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3234 - acc: 0.9688 - val_loss: 0.6428 - val_acc: 0.8613\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "178 [D loss supervised: 0.3234, acc.: 96.88%] [D loss unsupervised: 0.1870, acc.: 100.00%] [G loss: 0.274281, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4329 - acc: 0.9375 - val_loss: 0.6436 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "179 [D loss supervised: 0.4329, acc.: 93.75%] [D loss unsupervised: 0.1866, acc.: 100.00%] [G loss: 0.253876, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3052 - acc: 0.9688 - val_loss: 0.6450 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "180 [D loss supervised: 0.3052, acc.: 96.88%] [D loss unsupervised: 0.1953, acc.: 100.00%] [G loss: 0.255276, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4774 - acc: 0.8438 - val_loss: 0.6527 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "181 [D loss supervised: 0.4774, acc.: 84.38%] [D loss unsupervised: 0.1852, acc.: 100.00%] [G loss: 0.245138, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4514 - acc: 0.9062 - val_loss: 0.6596 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "182 [D loss supervised: 0.4514, acc.: 90.62%] [D loss unsupervised: 0.1856, acc.: 100.00%] [G loss: 0.212038, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3280 - acc: 1.0000 - val_loss: 0.6663 - val_acc: 0.8546\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "183 [D loss supervised: 0.3280, acc.: 100.00%] [D loss unsupervised: 0.1847, acc.: 100.00%] [G loss: 0.245240, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3456 - acc: 0.9688 - val_loss: 0.6704 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "184 [D loss supervised: 0.3456, acc.: 96.88%] [D loss unsupervised: 0.1846, acc.: 100.00%] [G loss: 0.202802, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2775 - acc: 0.9688 - val_loss: 0.6731 - val_acc: 0.8514\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "185 [D loss supervised: 0.2775, acc.: 96.88%] [D loss unsupervised: 0.1889, acc.: 100.00%] [G loss: 0.233355, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3526 - acc: 0.9375 - val_loss: 0.6738 - val_acc: 0.8517\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "186 [D loss supervised: 0.3526, acc.: 93.75%] [D loss unsupervised: 0.1827, acc.: 100.00%] [G loss: 0.228480, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2884 - acc: 0.9375 - val_loss: 0.6728 - val_acc: 0.8528\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "187 [D loss supervised: 0.2884, acc.: 93.75%] [D loss unsupervised: 0.1893, acc.: 100.00%] [G loss: 0.196910, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3746 - acc: 0.9375 - val_loss: 0.6699 - val_acc: 0.8535\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "188 [D loss supervised: 0.3746, acc.: 93.75%] [D loss unsupervised: 0.1942, acc.: 100.00%] [G loss: 0.199235, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3055 - acc: 0.9688 - val_loss: 0.6694 - val_acc: 0.8525\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "189 [D loss supervised: 0.3055, acc.: 96.88%] [D loss unsupervised: 0.1842, acc.: 100.00%] [G loss: 0.208844, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3911 - acc: 0.9375 - val_loss: 0.6681 - val_acc: 0.8517\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "190 [D loss supervised: 0.3911, acc.: 93.75%] [D loss unsupervised: 0.1828, acc.: 100.00%] [G loss: 0.210852, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3731 - acc: 0.9375 - val_loss: 0.6577 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "191 [D loss supervised: 0.3731, acc.: 93.75%] [D loss unsupervised: 0.1825, acc.: 100.00%] [G loss: 0.253171, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3317 - acc: 0.9375 - val_loss: 0.6473 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "192 [D loss supervised: 0.3317, acc.: 93.75%] [D loss unsupervised: 0.1831, acc.: 100.00%] [G loss: 0.294286, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3568 - acc: 0.9688 - val_loss: 0.6407 - val_acc: 0.8600\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "193 [D loss supervised: 0.3568, acc.: 96.88%] [D loss unsupervised: 0.1823, acc.: 100.00%] [G loss: 0.239953, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4631 - acc: 0.9062 - val_loss: 0.6380 - val_acc: 0.8616\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "194 [D loss supervised: 0.4631, acc.: 90.62%] [D loss unsupervised: 0.1816, acc.: 100.00%] [G loss: 0.243206, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3706 - acc: 0.9375 - val_loss: 0.6385 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "195 [D loss supervised: 0.3706, acc.: 93.75%] [D loss unsupervised: 0.1827, acc.: 100.00%] [G loss: 0.246963, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3047 - acc: 0.9375 - val_loss: 0.6383 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "196 [D loss supervised: 0.3047, acc.: 93.75%] [D loss unsupervised: 0.1808, acc.: 100.00%] [G loss: 0.249767, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2442 - acc: 1.0000 - val_loss: 0.6415 - val_acc: 0.8559\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "197 [D loss supervised: 0.2442, acc.: 100.00%] [D loss unsupervised: 0.1808, acc.: 100.00%] [G loss: 0.226455, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4332 - acc: 0.9375 - val_loss: 0.6450 - val_acc: 0.8543\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "198 [D loss supervised: 0.4332, acc.: 93.75%] [D loss unsupervised: 0.1814, acc.: 100.00%] [G loss: 0.303439, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4836 - acc: 0.9375 - val_loss: 0.6493 - val_acc: 0.8530\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "199 [D loss supervised: 0.4836, acc.: 93.75%] [D loss unsupervised: 0.1803, acc.: 100.00%] [G loss: 0.221888, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2867 - acc: 0.9688 - val_loss: 0.6537 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "200 [D loss supervised: 0.2867, acc.: 96.88%] [D loss unsupervised: 0.1813, acc.: 100.00%] [G loss: 0.210411, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2726 - acc: 1.0000 - val_loss: 0.6582 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "201 [D loss supervised: 0.2726, acc.: 100.00%] [D loss unsupervised: 0.1830, acc.: 100.00%] [G loss: 0.254184, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4526 - acc: 0.8750 - val_loss: 0.6611 - val_acc: 0.8494\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "202 [D loss supervised: 0.4526, acc.: 87.50%] [D loss unsupervised: 0.1848, acc.: 100.00%] [G loss: 0.201694, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3353 - acc: 1.0000 - val_loss: 0.6597 - val_acc: 0.8498\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "203 [D loss supervised: 0.3353, acc.: 100.00%] [D loss unsupervised: 0.1802, acc.: 100.00%] [G loss: 0.205894, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2964 - acc: 0.9688 - val_loss: 0.6554 - val_acc: 0.8504\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "204 [D loss supervised: 0.2964, acc.: 96.88%] [D loss unsupervised: 0.1821, acc.: 100.00%] [G loss: 0.208413, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3589 - acc: 0.9375 - val_loss: 0.6510 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "205 [D loss supervised: 0.3589, acc.: 93.75%] [D loss unsupervised: 0.1860, acc.: 100.00%] [G loss: 0.210445, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3753 - acc: 0.9062 - val_loss: 0.6455 - val_acc: 0.8535\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "206 [D loss supervised: 0.3753, acc.: 90.62%] [D loss unsupervised: 0.1799, acc.: 100.00%] [G loss: 0.194537, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2679 - acc: 1.0000 - val_loss: 0.6411 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "207 [D loss supervised: 0.2679, acc.: 100.00%] [D loss unsupervised: 0.1807, acc.: 100.00%] [G loss: 0.194039, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2348 - acc: 1.0000 - val_loss: 0.6373 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "208 [D loss supervised: 0.2348, acc.: 100.00%] [D loss unsupervised: 0.1833, acc.: 100.00%] [G loss: 0.214562, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3968 - acc: 0.9375 - val_loss: 0.6329 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "209 [D loss supervised: 0.3968, acc.: 93.75%] [D loss unsupervised: 0.1789, acc.: 100.00%] [G loss: 0.259103, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3537 - acc: 0.9688 - val_loss: 0.6310 - val_acc: 0.8583\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "210 [D loss supervised: 0.3537, acc.: 96.88%] [D loss unsupervised: 0.1797, acc.: 100.00%] [G loss: 0.279172, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3816 - acc: 0.9688 - val_loss: 0.6293 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "211 [D loss supervised: 0.3816, acc.: 96.88%] [D loss unsupervised: 0.1815, acc.: 100.00%] [G loss: 0.204386, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2637 - acc: 1.0000 - val_loss: 0.6297 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "212 [D loss supervised: 0.2637, acc.: 100.00%] [D loss unsupervised: 0.1807, acc.: 100.00%] [G loss: 0.245599, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3912 - acc: 0.9375 - val_loss: 0.6306 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "213 [D loss supervised: 0.3912, acc.: 93.75%] [D loss unsupervised: 0.1782, acc.: 100.00%] [G loss: 0.389351, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2547 - acc: 1.0000 - val_loss: 0.6331 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "214 [D loss supervised: 0.2547, acc.: 100.00%] [D loss unsupervised: 0.1779, acc.: 100.00%] [G loss: 0.230548, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3575 - acc: 0.9062 - val_loss: 0.6369 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "215 [D loss supervised: 0.3575, acc.: 90.62%] [D loss unsupervised: 0.1808, acc.: 100.00%] [G loss: 0.224265, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3692 - acc: 0.9688 - val_loss: 0.6424 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "216 [D loss supervised: 0.3692, acc.: 96.88%] [D loss unsupervised: 0.1779, acc.: 100.00%] [G loss: 0.376576, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2907 - acc: 0.9688 - val_loss: 0.6501 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "217 [D loss supervised: 0.2907, acc.: 96.88%] [D loss unsupervised: 0.1780, acc.: 100.00%] [G loss: 0.213649, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4467 - acc: 0.9062 - val_loss: 0.6580 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "218 [D loss supervised: 0.4467, acc.: 90.62%] [D loss unsupervised: 0.1809, acc.: 100.00%] [G loss: 0.212965, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2963 - acc: 0.9688 - val_loss: 0.6670 - val_acc: 0.8531\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "219 [D loss supervised: 0.2963, acc.: 96.88%] [D loss unsupervised: 0.1810, acc.: 100.00%] [G loss: 0.397975, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2310 - acc: 1.0000 - val_loss: 0.6755 - val_acc: 0.8493\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "220 [D loss supervised: 0.2310, acc.: 100.00%] [D loss unsupervised: 0.1773, acc.: 100.00%] [G loss: 0.224362, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2809 - acc: 0.9688 - val_loss: 0.6814 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "221 [D loss supervised: 0.2809, acc.: 96.88%] [D loss unsupervised: 0.1776, acc.: 100.00%] [G loss: 0.282925, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3885 - acc: 0.9375 - val_loss: 0.6834 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "222 [D loss supervised: 0.3885, acc.: 93.75%] [D loss unsupervised: 0.1770, acc.: 100.00%] [G loss: 0.260175, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4146 - acc: 0.9062 - val_loss: 0.6789 - val_acc: 0.8487\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "223 [D loss supervised: 0.4146, acc.: 90.62%] [D loss unsupervised: 0.1778, acc.: 100.00%] [G loss: 0.296069, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3304 - acc: 0.9688 - val_loss: 0.6741 - val_acc: 0.8513\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "224 [D loss supervised: 0.3304, acc.: 96.88%] [D loss unsupervised: 0.1791, acc.: 100.00%] [G loss: 0.406790, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3849 - acc: 0.9375 - val_loss: 0.6707 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "225 [D loss supervised: 0.3849, acc.: 93.75%] [D loss unsupervised: 0.1795, acc.: 100.00%] [G loss: 0.268861, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2390 - acc: 1.0000 - val_loss: 0.6685 - val_acc: 0.8517\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "226 [D loss supervised: 0.2390, acc.: 100.00%] [D loss unsupervised: 0.1828, acc.: 100.00%] [G loss: 0.433112, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3501 - acc: 0.9688 - val_loss: 0.6675 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "227 [D loss supervised: 0.3501, acc.: 96.88%] [D loss unsupervised: 0.1804, acc.: 100.00%] [G loss: 0.469741, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2605 - acc: 1.0000 - val_loss: 0.6685 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "228 [D loss supervised: 0.2605, acc.: 100.00%] [D loss unsupervised: 0.1784, acc.: 100.00%] [G loss: 1.918772, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4677 - acc: 0.8125 - val_loss: 0.6673 - val_acc: 0.8515\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "229 [D loss supervised: 0.4677, acc.: 81.25%] [D loss unsupervised: 0.1773, acc.: 100.00%] [G loss: 1.407270, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2935 - acc: 1.0000 - val_loss: 0.6668 - val_acc: 0.8517\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "230 [D loss supervised: 0.2935, acc.: 100.00%] [D loss unsupervised: 0.1835, acc.: 100.00%] [G loss: 1.111043, acc.: 50.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3796 - acc: 0.9375 - val_loss: 0.6677 - val_acc: 0.8531\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "231 [D loss supervised: 0.3796, acc.: 93.75%] [D loss unsupervised: 0.1782, acc.: 100.00%] [G loss: 0.504281, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3619 - acc: 0.9375 - val_loss: 0.6654 - val_acc: 0.8513\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "232 [D loss supervised: 0.3619, acc.: 93.75%] [D loss unsupervised: 0.1822, acc.: 100.00%] [G loss: 0.696184, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2880 - acc: 1.0000 - val_loss: 0.6627 - val_acc: 0.8527\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "233 [D loss supervised: 0.2880, acc.: 100.00%] [D loss unsupervised: 0.1799, acc.: 100.00%] [G loss: 0.876953, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4407 - acc: 0.9062 - val_loss: 0.6582 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "234 [D loss supervised: 0.4407, acc.: 90.62%] [D loss unsupervised: 0.2005, acc.: 98.44%] [G loss: 0.672852, acc.: 71.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3146 - acc: 1.0000 - val_loss: 0.6539 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "235 [D loss supervised: 0.3146, acc.: 100.00%] [D loss unsupervised: 0.1774, acc.: 100.00%] [G loss: 0.404501, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2451 - acc: 1.0000 - val_loss: 0.6501 - val_acc: 0.8561\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "236 [D loss supervised: 0.2451, acc.: 100.00%] [D loss unsupervised: 0.1759, acc.: 100.00%] [G loss: 0.520752, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2824 - acc: 1.0000 - val_loss: 0.6484 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "237 [D loss supervised: 0.2824, acc.: 100.00%] [D loss unsupervised: 0.1769, acc.: 100.00%] [G loss: 0.668116, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2763 - acc: 1.0000 - val_loss: 0.6496 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "238 [D loss supervised: 0.2763, acc.: 100.00%] [D loss unsupervised: 0.1801, acc.: 100.00%] [G loss: 0.805557, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3715 - acc: 0.9062 - val_loss: 0.6530 - val_acc: 0.8569\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "239 [D loss supervised: 0.3715, acc.: 90.62%] [D loss unsupervised: 0.1761, acc.: 100.00%] [G loss: 2.572074, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2527 - acc: 1.0000 - val_loss: 0.6589 - val_acc: 0.8554\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "240 [D loss supervised: 0.2527, acc.: 100.00%] [D loss unsupervised: 0.1830, acc.: 100.00%] [G loss: 2.996155, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3641 - acc: 0.9688 - val_loss: 0.6675 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "241 [D loss supervised: 0.3641, acc.: 96.88%] [D loss unsupervised: 0.1767, acc.: 100.00%] [G loss: 1.944279, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3168 - acc: 0.9688 - val_loss: 0.6786 - val_acc: 0.8501\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "242 [D loss supervised: 0.3168, acc.: 96.88%] [D loss unsupervised: 0.1751, acc.: 100.00%] [G loss: 2.000528, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2790 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "243 [D loss supervised: 0.2790, acc.: 100.00%] [D loss unsupervised: 0.1742, acc.: 100.00%] [G loss: 1.099450, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3209 - acc: 0.9688 - val_loss: 0.7017 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "244 [D loss supervised: 0.3209, acc.: 96.88%] [D loss unsupervised: 0.1741, acc.: 100.00%] [G loss: 1.344966, acc.: 43.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2402 - acc: 1.0000 - val_loss: 0.7147 - val_acc: 0.8405\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "245 [D loss supervised: 0.2402, acc.: 100.00%] [D loss unsupervised: 0.1741, acc.: 100.00%] [G loss: 1.168115, acc.: 59.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2250 - acc: 1.0000 - val_loss: 0.7261 - val_acc: 0.8386\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "246 [D loss supervised: 0.2250, acc.: 100.00%] [D loss unsupervised: 0.1906, acc.: 100.00%] [G loss: 0.800429, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3079 - acc: 0.9375 - val_loss: 0.7400 - val_acc: 0.8358\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "247 [D loss supervised: 0.3079, acc.: 93.75%] [D loss unsupervised: 0.1733, acc.: 100.00%] [G loss: 1.048049, acc.: 62.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4442 - acc: 0.9062 - val_loss: 0.7469 - val_acc: 0.8350\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "248 [D loss supervised: 0.4442, acc.: 90.62%] [D loss unsupervised: 0.1764, acc.: 100.00%] [G loss: 0.605763, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2853 - acc: 0.9688 - val_loss: 0.7581 - val_acc: 0.8329\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "249 [D loss supervised: 0.2853, acc.: 96.88%] [D loss unsupervised: 0.1755, acc.: 100.00%] [G loss: 0.566263, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3852 - acc: 0.9375 - val_loss: 0.7653 - val_acc: 0.8306\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "250 [D loss supervised: 0.3852, acc.: 93.75%] [D loss unsupervised: 0.1734, acc.: 100.00%] [G loss: 0.293063, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2978 - acc: 1.0000 - val_loss: 0.7737 - val_acc: 0.8300\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "251 [D loss supervised: 0.2978, acc.: 100.00%] [D loss unsupervised: 0.1732, acc.: 100.00%] [G loss: 0.632126, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3617 - acc: 0.9688 - val_loss: 0.7845 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "252 [D loss supervised: 0.3617, acc.: 96.88%] [D loss unsupervised: 0.1844, acc.: 100.00%] [G loss: 0.612837, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3852 - acc: 0.9062 - val_loss: 0.7860 - val_acc: 0.8260\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "253 [D loss supervised: 0.3852, acc.: 90.62%] [D loss unsupervised: 0.1734, acc.: 100.00%] [G loss: 0.706996, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4115 - acc: 0.9375 - val_loss: 0.7805 - val_acc: 0.8259\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "254 [D loss supervised: 0.4115, acc.: 93.75%] [D loss unsupervised: 0.1756, acc.: 100.00%] [G loss: 0.559516, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3644 - acc: 0.9688 - val_loss: 0.7792 - val_acc: 0.8263\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "255 [D loss supervised: 0.3644, acc.: 96.88%] [D loss unsupervised: 0.1733, acc.: 100.00%] [G loss: 0.376396, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3950 - acc: 0.9375 - val_loss: 0.7738 - val_acc: 0.8282\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "256 [D loss supervised: 0.3950, acc.: 93.75%] [D loss unsupervised: 0.1724, acc.: 100.00%] [G loss: 0.699161, acc.: 75.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4330 - acc: 0.8750 - val_loss: 0.7595 - val_acc: 0.8314\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "257 [D loss supervised: 0.4330, acc.: 87.50%] [D loss unsupervised: 0.1712, acc.: 100.00%] [G loss: 0.593054, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3780 - acc: 0.9375 - val_loss: 0.7296 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "258 [D loss supervised: 0.3780, acc.: 93.75%] [D loss unsupervised: 0.1713, acc.: 100.00%] [G loss: 0.331796, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4104 - acc: 0.9062 - val_loss: 0.7060 - val_acc: 0.8434\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "259 [D loss supervised: 0.4104, acc.: 90.62%] [D loss unsupervised: 0.1711, acc.: 100.00%] [G loss: 0.534459, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3210 - acc: 0.9375 - val_loss: 0.6853 - val_acc: 0.8483\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "260 [D loss supervised: 0.3210, acc.: 93.75%] [D loss unsupervised: 0.1707, acc.: 100.00%] [G loss: 0.612633, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2868 - acc: 0.9688 - val_loss: 0.6664 - val_acc: 0.8521\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "261 [D loss supervised: 0.2868, acc.: 96.88%] [D loss unsupervised: 0.1709, acc.: 100.00%] [G loss: 0.363803, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2913 - acc: 0.9688 - val_loss: 0.6558 - val_acc: 0.8538\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "262 [D loss supervised: 0.2913, acc.: 96.88%] [D loss unsupervised: 0.1712, acc.: 100.00%] [G loss: 0.458299, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2892 - acc: 0.9688 - val_loss: 0.6503 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "263 [D loss supervised: 0.2892, acc.: 96.88%] [D loss unsupervised: 0.1709, acc.: 100.00%] [G loss: 0.449144, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2428 - acc: 1.0000 - val_loss: 0.6485 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "264 [D loss supervised: 0.2428, acc.: 100.00%] [D loss unsupervised: 0.1705, acc.: 100.00%] [G loss: 0.302191, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4171 - acc: 0.9062 - val_loss: 0.6505 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "265 [D loss supervised: 0.4171, acc.: 90.62%] [D loss unsupervised: 0.1758, acc.: 100.00%] [G loss: 0.264099, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2615 - acc: 1.0000 - val_loss: 0.6544 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "266 [D loss supervised: 0.2615, acc.: 100.00%] [D loss unsupervised: 0.1694, acc.: 100.00%] [G loss: 0.257174, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2358 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "267 [D loss supervised: 0.2358, acc.: 100.00%] [D loss unsupervised: 0.1779, acc.: 100.00%] [G loss: 0.464080, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3693 - acc: 0.9375 - val_loss: 0.6652 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "268 [D loss supervised: 0.3693, acc.: 93.75%] [D loss unsupervised: 0.1704, acc.: 100.00%] [G loss: 0.383687, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4360 - acc: 0.9375 - val_loss: 0.6690 - val_acc: 0.8493\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "269 [D loss supervised: 0.4360, acc.: 93.75%] [D loss unsupervised: 0.1707, acc.: 100.00%] [G loss: 0.273488, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2974 - acc: 0.9688 - val_loss: 0.6717 - val_acc: 0.8497\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "270 [D loss supervised: 0.2974, acc.: 96.88%] [D loss unsupervised: 0.1693, acc.: 100.00%] [G loss: 0.278982, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2995 - acc: 0.9688 - val_loss: 0.6770 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "271 [D loss supervised: 0.2995, acc.: 96.88%] [D loss unsupervised: 0.1696, acc.: 100.00%] [G loss: 0.269656, acc.: 93.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2542 - acc: 1.0000 - val_loss: 0.6823 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "272 [D loss supervised: 0.2542, acc.: 100.00%] [D loss unsupervised: 0.1685, acc.: 100.00%] [G loss: 0.324545, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4098 - acc: 0.9375 - val_loss: 0.6852 - val_acc: 0.8469\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "273 [D loss supervised: 0.4098, acc.: 93.75%] [D loss unsupervised: 0.1696, acc.: 100.00%] [G loss: 0.256478, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2421 - acc: 1.0000 - val_loss: 0.6879 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "274 [D loss supervised: 0.2421, acc.: 100.00%] [D loss unsupervised: 0.1680, acc.: 100.00%] [G loss: 0.352953, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4109 - acc: 0.9062 - val_loss: 0.6902 - val_acc: 0.8454\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "275 [D loss supervised: 0.4109, acc.: 90.62%] [D loss unsupervised: 0.1681, acc.: 100.00%] [G loss: 0.266654, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2221 - acc: 1.0000 - val_loss: 0.6939 - val_acc: 0.8447\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "276 [D loss supervised: 0.2221, acc.: 100.00%] [D loss unsupervised: 0.1676, acc.: 100.00%] [G loss: 0.359775, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4414 - acc: 0.9062 - val_loss: 0.6985 - val_acc: 0.8417\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "277 [D loss supervised: 0.4414, acc.: 90.62%] [D loss unsupervised: 0.1691, acc.: 100.00%] [G loss: 0.287203, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2655 - acc: 1.0000 - val_loss: 0.7050 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "278 [D loss supervised: 0.2655, acc.: 100.00%] [D loss unsupervised: 0.1672, acc.: 100.00%] [G loss: 0.542694, acc.: 84.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3244 - acc: 0.9375 - val_loss: 0.7088 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "279 [D loss supervised: 0.3244, acc.: 93.75%] [D loss unsupervised: 0.1691, acc.: 100.00%] [G loss: 0.613879, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3546 - acc: 0.9375 - val_loss: 0.7108 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "280 [D loss supervised: 0.3546, acc.: 93.75%] [D loss unsupervised: 0.1669, acc.: 100.00%] [G loss: 0.254461, acc.: 100.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2518 - acc: 1.0000 - val_loss: 0.7140 - val_acc: 0.8364\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "281 [D loss supervised: 0.2518, acc.: 100.00%] [D loss unsupervised: 0.1680, acc.: 100.00%] [G loss: 0.350174, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3814 - acc: 0.9375 - val_loss: 0.7138 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "282 [D loss supervised: 0.3814, acc.: 93.75%] [D loss unsupervised: 0.1667, acc.: 100.00%] [G loss: 0.359523, acc.: 96.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3081 - acc: 0.9688 - val_loss: 0.7097 - val_acc: 0.8384\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "283 [D loss supervised: 0.3081, acc.: 96.88%] [D loss unsupervised: 0.1670, acc.: 100.00%] [G loss: 0.540897, acc.: 90.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3098 - acc: 0.9688 - val_loss: 0.7114 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "284 [D loss supervised: 0.3098, acc.: 96.88%] [D loss unsupervised: 0.1668, acc.: 100.00%] [G loss: 0.664151, acc.: 81.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2807 - acc: 0.9688 - val_loss: 0.7135 - val_acc: 0.8376\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "285 [D loss supervised: 0.2807, acc.: 96.88%] [D loss unsupervised: 0.1669, acc.: 100.00%] [G loss: 0.800738, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4104 - acc: 0.9688 - val_loss: 0.7122 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "286 [D loss supervised: 0.4104, acc.: 96.88%] [D loss unsupervised: 0.1661, acc.: 100.00%] [G loss: 0.537691, acc.: 87.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2670 - acc: 1.0000 - val_loss: 0.7083 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "287 [D loss supervised: 0.2670, acc.: 100.00%] [D loss unsupervised: 0.1668, acc.: 100.00%] [G loss: 0.747716, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2753 - acc: 1.0000 - val_loss: 0.7027 - val_acc: 0.8396\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "288 [D loss supervised: 0.2753, acc.: 100.00%] [D loss unsupervised: 0.1659, acc.: 100.00%] [G loss: 1.402482, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3673 - acc: 0.9375 - val_loss: 0.6962 - val_acc: 0.8414\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "289 [D loss supervised: 0.3673, acc.: 93.75%] [D loss unsupervised: 0.1668, acc.: 100.00%] [G loss: 0.643325, acc.: 78.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3570 - acc: 0.9688 - val_loss: 0.6942 - val_acc: 0.8407\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "290 [D loss supervised: 0.3570, acc.: 96.88%] [D loss unsupervised: 0.1671, acc.: 100.00%] [G loss: 1.278895, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2709 - acc: 1.0000 - val_loss: 0.6920 - val_acc: 0.8434\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "291 [D loss supervised: 0.2709, acc.: 100.00%] [D loss unsupervised: 0.1664, acc.: 100.00%] [G loss: 1.427744, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2531 - acc: 1.0000 - val_loss: 0.6936 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "292 [D loss supervised: 0.2531, acc.: 100.00%] [D loss unsupervised: 0.1670, acc.: 100.00%] [G loss: 0.821489, acc.: 68.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2633 - acc: 1.0000 - val_loss: 0.6960 - val_acc: 0.8442\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "293 [D loss supervised: 0.2633, acc.: 100.00%] [D loss unsupervised: 0.1658, acc.: 100.00%] [G loss: 1.776463, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2473 - acc: 1.0000 - val_loss: 0.6977 - val_acc: 0.8448\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "294 [D loss supervised: 0.2473, acc.: 100.00%] [D loss unsupervised: 0.1653, acc.: 100.00%] [G loss: 2.199261, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3225 - acc: 0.9688 - val_loss: 0.7006 - val_acc: 0.8428\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "295 [D loss supervised: 0.3225, acc.: 96.88%] [D loss unsupervised: 0.1668, acc.: 100.00%] [G loss: 1.099059, acc.: 50.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2757 - acc: 1.0000 - val_loss: 0.7024 - val_acc: 0.8419\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "296 [D loss supervised: 0.2757, acc.: 100.00%] [D loss unsupervised: 0.1657, acc.: 100.00%] [G loss: 1.987286, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2377 - acc: 1.0000 - val_loss: 0.7049 - val_acc: 0.8414\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "297 [D loss supervised: 0.2377, acc.: 100.00%] [D loss unsupervised: 0.1697, acc.: 100.00%] [G loss: 1.735085, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3714 - acc: 0.9688 - val_loss: 0.7038 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "298 [D loss supervised: 0.3714, acc.: 96.88%] [D loss unsupervised: 0.1660, acc.: 100.00%] [G loss: 1.375732, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5475 - acc: 0.7812 - val_loss: 0.6933 - val_acc: 0.8442\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "299 [D loss supervised: 0.5475, acc.: 78.12%] [D loss unsupervised: 0.1703, acc.: 100.00%] [G loss: 2.571230, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3273 - acc: 0.9688 - val_loss: 0.6886 - val_acc: 0.8447\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "300 [D loss supervised: 0.3273, acc.: 96.88%] [D loss unsupervised: 0.1656, acc.: 100.00%] [G loss: 3.750202, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2623 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.8456\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "301 [D loss supervised: 0.2623, acc.: 100.00%] [D loss unsupervised: 0.1661, acc.: 100.00%] [G loss: 4.190045, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2852 - acc: 1.0000 - val_loss: 0.6803 - val_acc: 0.8459\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "302 [D loss supervised: 0.2852, acc.: 100.00%] [D loss unsupervised: 0.1670, acc.: 100.00%] [G loss: 3.624425, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2527 - acc: 1.0000 - val_loss: 0.6781 - val_acc: 0.8452\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "303 [D loss supervised: 0.2527, acc.: 100.00%] [D loss unsupervised: 0.1692, acc.: 100.00%] [G loss: 2.806790, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3425 - acc: 0.9375 - val_loss: 0.6767 - val_acc: 0.8459\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "304 [D loss supervised: 0.3425, acc.: 93.75%] [D loss unsupervised: 0.1761, acc.: 100.00%] [G loss: 2.003493, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3110 - acc: 0.9688 - val_loss: 0.6725 - val_acc: 0.8475\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "305 [D loss supervised: 0.3110, acc.: 96.88%] [D loss unsupervised: 0.1713, acc.: 100.00%] [G loss: 2.283381, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2510 - acc: 1.0000 - val_loss: 0.6697 - val_acc: 0.8476\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "306 [D loss supervised: 0.2510, acc.: 100.00%] [D loss unsupervised: 0.1782, acc.: 100.00%] [G loss: 1.937716, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3729 - acc: 0.9688 - val_loss: 0.6673 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "307 [D loss supervised: 0.3729, acc.: 96.88%] [D loss unsupervised: 0.1662, acc.: 100.00%] [G loss: 2.071823, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2813 - acc: 1.0000 - val_loss: 0.6670 - val_acc: 0.8485\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "308 [D loss supervised: 0.2813, acc.: 100.00%] [D loss unsupervised: 0.1652, acc.: 100.00%] [G loss: 2.144446, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3043 - acc: 0.9688 - val_loss: 0.6717 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "309 [D loss supervised: 0.3043, acc.: 96.88%] [D loss unsupervised: 0.1663, acc.: 100.00%] [G loss: 1.929560, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3117 - acc: 0.9688 - val_loss: 0.6767 - val_acc: 0.8457\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "310 [D loss supervised: 0.3117, acc.: 96.88%] [D loss unsupervised: 0.1677, acc.: 100.00%] [G loss: 2.389674, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4195 - acc: 0.9688 - val_loss: 0.6835 - val_acc: 0.8444\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "311 [D loss supervised: 0.4195, acc.: 96.88%] [D loss unsupervised: 0.1637, acc.: 100.00%] [G loss: 1.888204, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2226 - acc: 1.0000 - val_loss: 0.6903 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "312 [D loss supervised: 0.2226, acc.: 100.00%] [D loss unsupervised: 0.1643, acc.: 100.00%] [G loss: 2.098484, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2653 - acc: 1.0000 - val_loss: 0.6953 - val_acc: 0.8432\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "313 [D loss supervised: 0.2653, acc.: 100.00%] [D loss unsupervised: 0.1637, acc.: 100.00%] [G loss: 2.415478, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2692 - acc: 0.9688 - val_loss: 0.6984 - val_acc: 0.8430\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "314 [D loss supervised: 0.2692, acc.: 96.88%] [D loss unsupervised: 0.1645, acc.: 100.00%] [G loss: 2.503240, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2510 - acc: 1.0000 - val_loss: 0.7019 - val_acc: 0.8436\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "315 [D loss supervised: 0.2510, acc.: 100.00%] [D loss unsupervised: 0.1733, acc.: 100.00%] [G loss: 2.712358, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2582 - acc: 1.0000 - val_loss: 0.7056 - val_acc: 0.8429\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "316 [D loss supervised: 0.2582, acc.: 100.00%] [D loss unsupervised: 0.1643, acc.: 100.00%] [G loss: 3.908401, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3157 - acc: 1.0000 - val_loss: 0.7083 - val_acc: 0.8421\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "317 [D loss supervised: 0.3157, acc.: 100.00%] [D loss unsupervised: 0.1624, acc.: 100.00%] [G loss: 4.493382, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3376 - acc: 0.9375 - val_loss: 0.7080 - val_acc: 0.8430\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "318 [D loss supervised: 0.3376, acc.: 93.75%] [D loss unsupervised: 0.1621, acc.: 100.00%] [G loss: 3.596434, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3272 - acc: 0.9375 - val_loss: 0.7052 - val_acc: 0.8446\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "319 [D loss supervised: 0.3272, acc.: 93.75%] [D loss unsupervised: 0.1620, acc.: 100.00%] [G loss: 3.337653, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2935 - acc: 1.0000 - val_loss: 0.7033 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "320 [D loss supervised: 0.2935, acc.: 100.00%] [D loss unsupervised: 0.1742, acc.: 98.44%] [G loss: 2.666442, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3687 - acc: 0.9375 - val_loss: 0.6994 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "321 [D loss supervised: 0.3687, acc.: 93.75%] [D loss unsupervised: 0.1680, acc.: 100.00%] [G loss: 2.819689, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3197 - acc: 0.9688 - val_loss: 0.6969 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "322 [D loss supervised: 0.3197, acc.: 96.88%] [D loss unsupervised: 0.1624, acc.: 100.00%] [G loss: 3.654212, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3249 - acc: 0.9688 - val_loss: 0.6916 - val_acc: 0.8487\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "323 [D loss supervised: 0.3249, acc.: 96.88%] [D loss unsupervised: 0.1623, acc.: 100.00%] [G loss: 2.816136, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3208 - acc: 0.9688 - val_loss: 0.6876 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "324 [D loss supervised: 0.3208, acc.: 96.88%] [D loss unsupervised: 0.1639, acc.: 100.00%] [G loss: 2.760268, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2679 - acc: 0.9688 - val_loss: 0.6852 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "325 [D loss supervised: 0.2679, acc.: 96.88%] [D loss unsupervised: 0.1630, acc.: 100.00%] [G loss: 3.443612, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2374 - acc: 1.0000 - val_loss: 0.6839 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "326 [D loss supervised: 0.2374, acc.: 100.00%] [D loss unsupervised: 0.1616, acc.: 100.00%] [G loss: 3.249836, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2559 - acc: 1.0000 - val_loss: 0.6830 - val_acc: 0.8533\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "327 [D loss supervised: 0.2559, acc.: 100.00%] [D loss unsupervised: 0.1622, acc.: 100.00%] [G loss: 3.019246, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2799 - acc: 0.9688 - val_loss: 0.6799 - val_acc: 0.8533\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "328 [D loss supervised: 0.2799, acc.: 96.88%] [D loss unsupervised: 0.1642, acc.: 100.00%] [G loss: 3.172407, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2690 - acc: 1.0000 - val_loss: 0.6771 - val_acc: 0.8545\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "329 [D loss supervised: 0.2690, acc.: 100.00%] [D loss unsupervised: 0.1620, acc.: 100.00%] [G loss: 3.282761, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2694 - acc: 0.9688 - val_loss: 0.6754 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "330 [D loss supervised: 0.2694, acc.: 96.88%] [D loss unsupervised: 0.1607, acc.: 100.00%] [G loss: 2.836027, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2751 - acc: 0.9688 - val_loss: 0.6755 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "331 [D loss supervised: 0.2751, acc.: 96.88%] [D loss unsupervised: 0.1631, acc.: 100.00%] [G loss: 2.609496, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2111 - acc: 1.0000 - val_loss: 0.6768 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "332 [D loss supervised: 0.2111, acc.: 100.00%] [D loss unsupervised: 0.1610, acc.: 100.00%] [G loss: 2.534936, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2426 - acc: 1.0000 - val_loss: 0.6786 - val_acc: 0.8499\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "333 [D loss supervised: 0.2426, acc.: 100.00%] [D loss unsupervised: 0.1618, acc.: 100.00%] [G loss: 2.855857, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2514 - acc: 1.0000 - val_loss: 0.6815 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "334 [D loss supervised: 0.2514, acc.: 100.00%] [D loss unsupervised: 0.1624, acc.: 100.00%] [G loss: 2.753378, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2579 - acc: 1.0000 - val_loss: 0.6857 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "335 [D loss supervised: 0.2579, acc.: 100.00%] [D loss unsupervised: 0.1614, acc.: 100.00%] [G loss: 3.662387, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2480 - acc: 1.0000 - val_loss: 0.6904 - val_acc: 0.8453\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "336 [D loss supervised: 0.2480, acc.: 100.00%] [D loss unsupervised: 0.1660, acc.: 100.00%] [G loss: 3.041087, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3409 - acc: 0.9688 - val_loss: 0.6908 - val_acc: 0.8453\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "337 [D loss supervised: 0.3409, acc.: 96.88%] [D loss unsupervised: 0.1629, acc.: 100.00%] [G loss: 2.672217, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2280 - acc: 1.0000 - val_loss: 0.6918 - val_acc: 0.8449\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "338 [D loss supervised: 0.2280, acc.: 100.00%] [D loss unsupervised: 0.1613, acc.: 100.00%] [G loss: 2.524186, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2965 - acc: 0.9688 - val_loss: 0.6906 - val_acc: 0.8458\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "339 [D loss supervised: 0.2965, acc.: 96.88%] [D loss unsupervised: 0.1623, acc.: 100.00%] [G loss: 2.624174, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2452 - acc: 1.0000 - val_loss: 0.6893 - val_acc: 0.8465\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "340 [D loss supervised: 0.2452, acc.: 100.00%] [D loss unsupervised: 0.1610, acc.: 100.00%] [G loss: 2.698624, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3019 - acc: 0.9375 - val_loss: 0.6830 - val_acc: 0.8496\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "341 [D loss supervised: 0.3019, acc.: 93.75%] [D loss unsupervised: 0.1612, acc.: 100.00%] [G loss: 1.669277, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2693 - acc: 1.0000 - val_loss: 0.6751 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "342 [D loss supervised: 0.2693, acc.: 100.00%] [D loss unsupervised: 0.1704, acc.: 100.00%] [G loss: 4.340105, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2313 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "343 [D loss supervised: 0.2313, acc.: 100.00%] [D loss unsupervised: 0.1663, acc.: 100.00%] [G loss: 5.235507, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2628 - acc: 0.9688 - val_loss: 0.6631 - val_acc: 0.8532\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "344 [D loss supervised: 0.2628, acc.: 96.88%] [D loss unsupervised: 0.1854, acc.: 98.44%] [G loss: 5.492744, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3989 - acc: 0.9688 - val_loss: 0.6598 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "345 [D loss supervised: 0.3989, acc.: 96.88%] [D loss unsupervised: 0.1618, acc.: 100.00%] [G loss: 5.442096, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2877 - acc: 0.9688 - val_loss: 0.6575 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "346 [D loss supervised: 0.2877, acc.: 96.88%] [D loss unsupervised: 0.1628, acc.: 100.00%] [G loss: 5.697756, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3193 - acc: 0.9375 - val_loss: 0.6568 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "347 [D loss supervised: 0.3193, acc.: 93.75%] [D loss unsupervised: 0.1581, acc.: 100.00%] [G loss: 5.394989, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2997 - acc: 0.9688 - val_loss: 0.6588 - val_acc: 0.8546\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "348 [D loss supervised: 0.2997, acc.: 96.88%] [D loss unsupervised: 0.1576, acc.: 100.00%] [G loss: 5.322198, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4745 - acc: 0.9062 - val_loss: 0.6619 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "349 [D loss supervised: 0.4745, acc.: 90.62%] [D loss unsupervised: 0.1678, acc.: 100.00%] [G loss: 5.165796, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3399 - acc: 0.9688 - val_loss: 0.6642 - val_acc: 0.8523\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "350 [D loss supervised: 0.3399, acc.: 96.88%] [D loss unsupervised: 0.1576, acc.: 100.00%] [G loss: 4.754653, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2516 - acc: 1.0000 - val_loss: 0.6690 - val_acc: 0.8524\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "351 [D loss supervised: 0.2516, acc.: 100.00%] [D loss unsupervised: 0.1610, acc.: 100.00%] [G loss: 4.092860, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2894 - acc: 0.9688 - val_loss: 0.6740 - val_acc: 0.8508\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "352 [D loss supervised: 0.2894, acc.: 96.88%] [D loss unsupervised: 0.1585, acc.: 100.00%] [G loss: 4.107539, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3900 - acc: 0.9375 - val_loss: 0.6781 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "353 [D loss supervised: 0.3900, acc.: 93.75%] [D loss unsupervised: 0.1587, acc.: 100.00%] [G loss: 3.533521, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2504 - acc: 1.0000 - val_loss: 0.6827 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "354 [D loss supervised: 0.2504, acc.: 100.00%] [D loss unsupervised: 0.1712, acc.: 100.00%] [G loss: 3.594002, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2948 - acc: 0.9688 - val_loss: 0.6846 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "355 [D loss supervised: 0.2948, acc.: 96.88%] [D loss unsupervised: 0.1569, acc.: 100.00%] [G loss: 3.686290, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3592 - acc: 0.9375 - val_loss: 0.6896 - val_acc: 0.8482\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "356 [D loss supervised: 0.3592, acc.: 93.75%] [D loss unsupervised: 0.1572, acc.: 100.00%] [G loss: 3.348302, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3714 - acc: 0.9375 - val_loss: 0.6947 - val_acc: 0.8488\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "357 [D loss supervised: 0.3714, acc.: 93.75%] [D loss unsupervised: 0.1570, acc.: 100.00%] [G loss: 3.466629, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2335 - acc: 1.0000 - val_loss: 0.6998 - val_acc: 0.8481\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "358 [D loss supervised: 0.2335, acc.: 100.00%] [D loss unsupervised: 0.1597, acc.: 100.00%] [G loss: 3.709291, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2647 - acc: 0.9688 - val_loss: 0.7051 - val_acc: 0.8484\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "359 [D loss supervised: 0.2647, acc.: 96.88%] [D loss unsupervised: 0.1556, acc.: 100.00%] [G loss: 3.690846, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2297 - acc: 1.0000 - val_loss: 0.7142 - val_acc: 0.8453\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "360 [D loss supervised: 0.2297, acc.: 100.00%] [D loss unsupervised: 0.1552, acc.: 100.00%] [G loss: 3.862269, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2388 - acc: 1.0000 - val_loss: 0.7254 - val_acc: 0.8419\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "361 [D loss supervised: 0.2388, acc.: 100.00%] [D loss unsupervised: 0.1552, acc.: 100.00%] [G loss: 3.894103, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3270 - acc: 0.9688 - val_loss: 0.7363 - val_acc: 0.8399\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "362 [D loss supervised: 0.3270, acc.: 96.88%] [D loss unsupervised: 0.1544, acc.: 100.00%] [G loss: 4.062887, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2900 - acc: 0.9688 - val_loss: 0.7478 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "363 [D loss supervised: 0.2900, acc.: 96.88%] [D loss unsupervised: 0.1555, acc.: 100.00%] [G loss: 3.587575, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3414 - acc: 0.9375 - val_loss: 0.7592 - val_acc: 0.8342\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "364 [D loss supervised: 0.3414, acc.: 93.75%] [D loss unsupervised: 0.1552, acc.: 100.00%] [G loss: 3.461124, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3266 - acc: 0.9375 - val_loss: 0.7711 - val_acc: 0.8313\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "365 [D loss supervised: 0.3266, acc.: 93.75%] [D loss unsupervised: 0.1541, acc.: 100.00%] [G loss: 3.225020, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2462 - acc: 1.0000 - val_loss: 0.7838 - val_acc: 0.8280\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "366 [D loss supervised: 0.2462, acc.: 100.00%] [D loss unsupervised: 0.1540, acc.: 100.00%] [G loss: 3.540123, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2575 - acc: 0.9688 - val_loss: 0.7971 - val_acc: 0.8261\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "367 [D loss supervised: 0.2575, acc.: 96.88%] [D loss unsupervised: 0.1539, acc.: 100.00%] [G loss: 3.476610, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2360 - acc: 1.0000 - val_loss: 0.8096 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "368 [D loss supervised: 0.2360, acc.: 100.00%] [D loss unsupervised: 0.1546, acc.: 100.00%] [G loss: 2.924460, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5089 - acc: 0.9062 - val_loss: 0.8248 - val_acc: 0.8182\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "369 [D loss supervised: 0.5089, acc.: 90.62%] [D loss unsupervised: 0.1538, acc.: 100.00%] [G loss: 2.785222, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2117 - acc: 1.0000 - val_loss: 0.8364 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "370 [D loss supervised: 0.2117, acc.: 100.00%] [D loss unsupervised: 0.1529, acc.: 100.00%] [G loss: 2.634441, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2440 - acc: 1.0000 - val_loss: 0.8455 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "371 [D loss supervised: 0.2440, acc.: 100.00%] [D loss unsupervised: 0.1630, acc.: 100.00%] [G loss: 3.149295, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3517 - acc: 0.9688 - val_loss: 0.8510 - val_acc: 0.8137\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "372 [D loss supervised: 0.3517, acc.: 96.88%] [D loss unsupervised: 0.1526, acc.: 100.00%] [G loss: 2.881303, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2790 - acc: 0.9688 - val_loss: 0.8533 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "373 [D loss supervised: 0.2790, acc.: 96.88%] [D loss unsupervised: 0.1521, acc.: 100.00%] [G loss: 3.009426, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2218 - acc: 1.0000 - val_loss: 0.8541 - val_acc: 0.8129\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "374 [D loss supervised: 0.2218, acc.: 100.00%] [D loss unsupervised: 0.1520, acc.: 100.00%] [G loss: 2.955455, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2817 - acc: 0.9688 - val_loss: 0.8612 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "375 [D loss supervised: 0.2817, acc.: 96.88%] [D loss unsupervised: 0.1525, acc.: 100.00%] [G loss: 2.211748, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2761 - acc: 1.0000 - val_loss: 0.8651 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "376 [D loss supervised: 0.2761, acc.: 100.00%] [D loss unsupervised: 0.1516, acc.: 100.00%] [G loss: 2.608203, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3885 - acc: 0.9062 - val_loss: 0.8589 - val_acc: 0.8131\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "377 [D loss supervised: 0.3885, acc.: 90.62%] [D loss unsupervised: 0.1515, acc.: 100.00%] [G loss: 2.669611, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3561 - acc: 0.9375 - val_loss: 0.8499 - val_acc: 0.8159\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "378 [D loss supervised: 0.3561, acc.: 93.75%] [D loss unsupervised: 0.1520, acc.: 100.00%] [G loss: 2.696965, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3964 - acc: 0.9375 - val_loss: 0.8337 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "379 [D loss supervised: 0.3964, acc.: 93.75%] [D loss unsupervised: 0.1513, acc.: 100.00%] [G loss: 2.621931, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2520 - acc: 1.0000 - val_loss: 0.8161 - val_acc: 0.8228\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "380 [D loss supervised: 0.2520, acc.: 100.00%] [D loss unsupervised: 0.1512, acc.: 100.00%] [G loss: 2.034041, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2262 - acc: 1.0000 - val_loss: 0.8011 - val_acc: 0.8275\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "381 [D loss supervised: 0.2262, acc.: 100.00%] [D loss unsupervised: 0.1512, acc.: 100.00%] [G loss: 1.971328, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2903 - acc: 0.9688 - val_loss: 0.7817 - val_acc: 0.8306\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "382 [D loss supervised: 0.2903, acc.: 96.88%] [D loss unsupervised: 0.1510, acc.: 100.00%] [G loss: 2.264974, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2553 - acc: 1.0000 - val_loss: 0.7655 - val_acc: 0.8332\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "383 [D loss supervised: 0.2553, acc.: 100.00%] [D loss unsupervised: 0.1507, acc.: 100.00%] [G loss: 1.910738, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4048 - acc: 0.8750 - val_loss: 0.7520 - val_acc: 0.8362\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "384 [D loss supervised: 0.4048, acc.: 87.50%] [D loss unsupervised: 0.1505, acc.: 100.00%] [G loss: 2.457308, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2358 - acc: 0.9688 - val_loss: 0.7394 - val_acc: 0.8406\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "385 [D loss supervised: 0.2358, acc.: 96.88%] [D loss unsupervised: 0.1508, acc.: 100.00%] [G loss: 2.360926, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3001 - acc: 0.9688 - val_loss: 0.7283 - val_acc: 0.8429\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "386 [D loss supervised: 0.3001, acc.: 96.88%] [D loss unsupervised: 0.1506, acc.: 100.00%] [G loss: 1.781089, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2780 - acc: 1.0000 - val_loss: 0.7191 - val_acc: 0.8450\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "387 [D loss supervised: 0.2780, acc.: 100.00%] [D loss unsupervised: 0.1498, acc.: 100.00%] [G loss: 2.109504, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4583 - acc: 0.9375 - val_loss: 0.7092 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "388 [D loss supervised: 0.4583, acc.: 93.75%] [D loss unsupervised: 0.1500, acc.: 100.00%] [G loss: 1.565225, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3631 - acc: 0.9062 - val_loss: 0.6987 - val_acc: 0.8494\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "389 [D loss supervised: 0.3631, acc.: 90.62%] [D loss unsupervised: 0.1582, acc.: 100.00%] [G loss: 3.389671, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3211 - acc: 0.9375 - val_loss: 0.6892 - val_acc: 0.8504\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "390 [D loss supervised: 0.3211, acc.: 93.75%] [D loss unsupervised: 0.1500, acc.: 100.00%] [G loss: 4.000185, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2868 - acc: 0.9688 - val_loss: 0.6852 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "391 [D loss supervised: 0.2868, acc.: 96.88%] [D loss unsupervised: 0.1496, acc.: 100.00%] [G loss: 4.762583, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2279 - acc: 1.0000 - val_loss: 0.6826 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "392 [D loss supervised: 0.2279, acc.: 100.00%] [D loss unsupervised: 0.1491, acc.: 100.00%] [G loss: 4.836833, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2172 - acc: 1.0000 - val_loss: 0.6806 - val_acc: 0.8514\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "393 [D loss supervised: 0.2172, acc.: 100.00%] [D loss unsupervised: 0.1500, acc.: 100.00%] [G loss: 4.562516, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2716 - acc: 0.9688 - val_loss: 0.6795 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "394 [D loss supervised: 0.2716, acc.: 96.88%] [D loss unsupervised: 0.1513, acc.: 100.00%] [G loss: 4.759452, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2521 - acc: 1.0000 - val_loss: 0.6786 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "395 [D loss supervised: 0.2521, acc.: 100.00%] [D loss unsupervised: 0.1497, acc.: 100.00%] [G loss: 4.162257, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2705 - acc: 0.9688 - val_loss: 0.6784 - val_acc: 0.8518\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "396 [D loss supervised: 0.2705, acc.: 96.88%] [D loss unsupervised: 0.1506, acc.: 100.00%] [G loss: 4.086682, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2272 - acc: 1.0000 - val_loss: 0.6780 - val_acc: 0.8515\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "397 [D loss supervised: 0.2272, acc.: 100.00%] [D loss unsupervised: 0.1504, acc.: 100.00%] [G loss: 4.116241, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2939 - acc: 0.9375 - val_loss: 0.6778 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "398 [D loss supervised: 0.2939, acc.: 93.75%] [D loss unsupervised: 0.1494, acc.: 100.00%] [G loss: 4.009306, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2475 - acc: 1.0000 - val_loss: 0.6777 - val_acc: 0.8533\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "399 [D loss supervised: 0.2475, acc.: 100.00%] [D loss unsupervised: 0.1492, acc.: 100.00%] [G loss: 4.119387, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3682 - acc: 0.9062 - val_loss: 0.6766 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "400 [D loss supervised: 0.3682, acc.: 90.62%] [D loss unsupervised: 0.1487, acc.: 100.00%] [G loss: 3.901102, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2599 - acc: 0.9688 - val_loss: 0.6770 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "401 [D loss supervised: 0.2599, acc.: 96.88%] [D loss unsupervised: 0.1500, acc.: 100.00%] [G loss: 4.127256, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2282 - acc: 1.0000 - val_loss: 0.6788 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "402 [D loss supervised: 0.2282, acc.: 100.00%] [D loss unsupervised: 0.1480, acc.: 100.00%] [G loss: 3.826200, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3044 - acc: 0.9688 - val_loss: 0.6799 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "403 [D loss supervised: 0.3044, acc.: 96.88%] [D loss unsupervised: 0.1475, acc.: 100.00%] [G loss: 3.884667, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3233 - acc: 0.9062 - val_loss: 0.6797 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "404 [D loss supervised: 0.3233, acc.: 90.62%] [D loss unsupervised: 0.1501, acc.: 100.00%] [G loss: 3.970012, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2668 - acc: 0.9688 - val_loss: 0.6753 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "405 [D loss supervised: 0.2668, acc.: 96.88%] [D loss unsupervised: 0.1478, acc.: 100.00%] [G loss: 3.942588, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3937 - acc: 0.9375 - val_loss: 0.6726 - val_acc: 0.8571\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "406 [D loss supervised: 0.3937, acc.: 93.75%] [D loss unsupervised: 0.1477, acc.: 100.00%] [G loss: 3.656971, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2757 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "407 [D loss supervised: 0.2757, acc.: 100.00%] [D loss unsupervised: 0.1475, acc.: 100.00%] [G loss: 4.533122, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3523 - acc: 0.9688 - val_loss: 0.6668 - val_acc: 0.8577\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "408 [D loss supervised: 0.3523, acc.: 96.88%] [D loss unsupervised: 0.1472, acc.: 100.00%] [G loss: 3.887986, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2160 - acc: 1.0000 - val_loss: 0.6653 - val_acc: 0.8588\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "409 [D loss supervised: 0.2160, acc.: 100.00%] [D loss unsupervised: 0.1469, acc.: 100.00%] [G loss: 3.734803, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2278 - acc: 1.0000 - val_loss: 0.6656 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "410 [D loss supervised: 0.2278, acc.: 100.00%] [D loss unsupervised: 0.1467, acc.: 100.00%] [G loss: 4.272110, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2921 - acc: 0.9688 - val_loss: 0.6658 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "411 [D loss supervised: 0.2921, acc.: 96.88%] [D loss unsupervised: 0.1464, acc.: 100.00%] [G loss: 4.293713, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2982 - acc: 0.9375 - val_loss: 0.6672 - val_acc: 0.8608\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "412 [D loss supervised: 0.2982, acc.: 93.75%] [D loss unsupervised: 0.1463, acc.: 100.00%] [G loss: 4.482901, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2454 - acc: 0.9688 - val_loss: 0.6678 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "413 [D loss supervised: 0.2454, acc.: 96.88%] [D loss unsupervised: 0.1459, acc.: 100.00%] [G loss: 4.349250, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3288 - acc: 0.9688 - val_loss: 0.6667 - val_acc: 0.8622\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "414 [D loss supervised: 0.3288, acc.: 96.88%] [D loss unsupervised: 0.1466, acc.: 100.00%] [G loss: 4.232227, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2871 - acc: 0.9688 - val_loss: 0.6673 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "415 [D loss supervised: 0.2871, acc.: 96.88%] [D loss unsupervised: 0.1473, acc.: 100.00%] [G loss: 4.339596, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2526 - acc: 1.0000 - val_loss: 0.6677 - val_acc: 0.8607\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "416 [D loss supervised: 0.2526, acc.: 100.00%] [D loss unsupervised: 0.1460, acc.: 100.00%] [G loss: 4.687909, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2214 - acc: 1.0000 - val_loss: 0.6674 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "417 [D loss supervised: 0.2214, acc.: 100.00%] [D loss unsupervised: 0.1454, acc.: 100.00%] [G loss: 4.723396, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3188 - acc: 0.9688 - val_loss: 0.6661 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "418 [D loss supervised: 0.3188, acc.: 96.88%] [D loss unsupervised: 0.1451, acc.: 100.00%] [G loss: 4.592272, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2721 - acc: 0.9688 - val_loss: 0.6643 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "419 [D loss supervised: 0.2721, acc.: 96.88%] [D loss unsupervised: 0.1450, acc.: 100.00%] [G loss: 4.607597, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2456 - acc: 1.0000 - val_loss: 0.6640 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "420 [D loss supervised: 0.2456, acc.: 100.00%] [D loss unsupervised: 0.1447, acc.: 100.00%] [G loss: 5.015951, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3323 - acc: 0.9688 - val_loss: 0.6645 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "421 [D loss supervised: 0.3323, acc.: 96.88%] [D loss unsupervised: 0.1452, acc.: 100.00%] [G loss: 4.512965, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2333 - acc: 1.0000 - val_loss: 0.6656 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "422 [D loss supervised: 0.2333, acc.: 100.00%] [D loss unsupervised: 0.1445, acc.: 100.00%] [G loss: 4.577627, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2474 - acc: 1.0000 - val_loss: 0.6664 - val_acc: 0.8586\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "423 [D loss supervised: 0.2474, acc.: 100.00%] [D loss unsupervised: 0.1444, acc.: 100.00%] [G loss: 4.749954, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2469 - acc: 1.0000 - val_loss: 0.6666 - val_acc: 0.8583\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "424 [D loss supervised: 0.2469, acc.: 100.00%] [D loss unsupervised: 0.1443, acc.: 100.00%] [G loss: 4.740180, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2741 - acc: 0.9688 - val_loss: 0.6679 - val_acc: 0.8581\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "425 [D loss supervised: 0.2741, acc.: 96.88%] [D loss unsupervised: 0.1444, acc.: 100.00%] [G loss: 4.534158, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2265 - acc: 1.0000 - val_loss: 0.6693 - val_acc: 0.8579\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "426 [D loss supervised: 0.2265, acc.: 100.00%] [D loss unsupervised: 0.1438, acc.: 100.00%] [G loss: 4.683218, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2266 - acc: 1.0000 - val_loss: 0.6704 - val_acc: 0.8581\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "427 [D loss supervised: 0.2266, acc.: 100.00%] [D loss unsupervised: 0.1438, acc.: 100.00%] [G loss: 4.597580, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2689 - acc: 0.9688 - val_loss: 0.6712 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "428 [D loss supervised: 0.2689, acc.: 96.88%] [D loss unsupervised: 0.1437, acc.: 100.00%] [G loss: 4.690327, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2805 - acc: 1.0000 - val_loss: 0.6714 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "429 [D loss supervised: 0.2805, acc.: 100.00%] [D loss unsupervised: 0.1433, acc.: 100.00%] [G loss: 4.519864, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3387 - acc: 0.9688 - val_loss: 0.6693 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "430 [D loss supervised: 0.3387, acc.: 96.88%] [D loss unsupervised: 0.1430, acc.: 100.00%] [G loss: 4.560594, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2282 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "431 [D loss supervised: 0.2282, acc.: 100.00%] [D loss unsupervised: 0.1430, acc.: 100.00%] [G loss: 4.746303, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2812 - acc: 0.9688 - val_loss: 0.6681 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "432 [D loss supervised: 0.2812, acc.: 96.88%] [D loss unsupervised: 0.1434, acc.: 100.00%] [G loss: 4.752958, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2283 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "433 [D loss supervised: 0.2283, acc.: 100.00%] [D loss unsupervised: 0.1426, acc.: 100.00%] [G loss: 5.075414, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3194 - acc: 0.9688 - val_loss: 0.6681 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "434 [D loss supervised: 0.3194, acc.: 96.88%] [D loss unsupervised: 0.1423, acc.: 100.00%] [G loss: 5.178544, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2634 - acc: 1.0000 - val_loss: 0.6681 - val_acc: 0.8617\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "435 [D loss supervised: 0.2634, acc.: 100.00%] [D loss unsupervised: 0.1429, acc.: 100.00%] [G loss: 4.946586, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2843 - acc: 1.0000 - val_loss: 0.6679 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "436 [D loss supervised: 0.2843, acc.: 100.00%] [D loss unsupervised: 0.1430, acc.: 100.00%] [G loss: 4.856692, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3601 - acc: 0.9688 - val_loss: 0.6684 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "437 [D loss supervised: 0.3601, acc.: 96.88%] [D loss unsupervised: 0.1420, acc.: 100.00%] [G loss: 4.918026, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2458 - acc: 1.0000 - val_loss: 0.6698 - val_acc: 0.8600\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "438 [D loss supervised: 0.2458, acc.: 100.00%] [D loss unsupervised: 0.1440, acc.: 100.00%] [G loss: 4.955874, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2460 - acc: 1.0000 - val_loss: 0.6733 - val_acc: 0.8601\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "439 [D loss supervised: 0.2460, acc.: 100.00%] [D loss unsupervised: 0.1419, acc.: 100.00%] [G loss: 4.845293, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2816 - acc: 0.9688 - val_loss: 0.6719 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "440 [D loss supervised: 0.2816, acc.: 96.88%] [D loss unsupervised: 0.1422, acc.: 100.00%] [G loss: 4.851287, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2269 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.8598\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "441 [D loss supervised: 0.2269, acc.: 100.00%] [D loss unsupervised: 0.1415, acc.: 100.00%] [G loss: 4.664833, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3090 - acc: 0.9375 - val_loss: 0.6749 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "442 [D loss supervised: 0.3090, acc.: 93.75%] [D loss unsupervised: 0.1414, acc.: 100.00%] [G loss: 4.568464, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2410 - acc: 1.0000 - val_loss: 0.6804 - val_acc: 0.8577\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "443 [D loss supervised: 0.2410, acc.: 100.00%] [D loss unsupervised: 0.1412, acc.: 100.00%] [G loss: 4.806532, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2992 - acc: 0.9688 - val_loss: 0.6862 - val_acc: 0.8556\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "444 [D loss supervised: 0.2992, acc.: 96.88%] [D loss unsupervised: 0.1408, acc.: 100.00%] [G loss: 4.464635, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2389 - acc: 1.0000 - val_loss: 0.6944 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "445 [D loss supervised: 0.2389, acc.: 100.00%] [D loss unsupervised: 0.1417, acc.: 100.00%] [G loss: 4.717868, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2453 - acc: 1.0000 - val_loss: 0.7032 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "446 [D loss supervised: 0.2453, acc.: 100.00%] [D loss unsupervised: 0.1408, acc.: 100.00%] [G loss: 4.764218, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2472 - acc: 1.0000 - val_loss: 0.7082 - val_acc: 0.8501\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "447 [D loss supervised: 0.2472, acc.: 100.00%] [D loss unsupervised: 0.1402, acc.: 100.00%] [G loss: 4.537995, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2629 - acc: 1.0000 - val_loss: 0.7075 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "448 [D loss supervised: 0.2629, acc.: 100.00%] [D loss unsupervised: 0.1404, acc.: 100.00%] [G loss: 4.905994, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2276 - acc: 1.0000 - val_loss: 0.7048 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "449 [D loss supervised: 0.2276, acc.: 100.00%] [D loss unsupervised: 0.1400, acc.: 100.00%] [G loss: 4.760175, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2231 - acc: 1.0000 - val_loss: 0.7020 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "450 [D loss supervised: 0.2231, acc.: 100.00%] [D loss unsupervised: 0.1400, acc.: 100.00%] [G loss: 4.980199, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4558 - acc: 0.9062 - val_loss: 0.6979 - val_acc: 0.8518\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "451 [D loss supervised: 0.4558, acc.: 90.62%] [D loss unsupervised: 0.1397, acc.: 100.00%] [G loss: 4.940465, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2989 - acc: 0.9688 - val_loss: 0.6918 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "452 [D loss supervised: 0.2989, acc.: 96.88%] [D loss unsupervised: 0.1395, acc.: 100.00%] [G loss: 4.937975, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2465 - acc: 1.0000 - val_loss: 0.6839 - val_acc: 0.8556\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "453 [D loss supervised: 0.2465, acc.: 100.00%] [D loss unsupervised: 0.1394, acc.: 100.00%] [G loss: 5.102133, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2733 - acc: 1.0000 - val_loss: 0.6758 - val_acc: 0.8570\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "454 [D loss supervised: 0.2733, acc.: 100.00%] [D loss unsupervised: 0.1391, acc.: 100.00%] [G loss: 5.016792, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2287 - acc: 1.0000 - val_loss: 0.6693 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "455 [D loss supervised: 0.2287, acc.: 100.00%] [D loss unsupervised: 0.1388, acc.: 100.00%] [G loss: 5.239878, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2935 - acc: 0.9688 - val_loss: 0.6635 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "456 [D loss supervised: 0.2935, acc.: 96.88%] [D loss unsupervised: 0.1397, acc.: 100.00%] [G loss: 5.558189, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2238 - acc: 1.0000 - val_loss: 0.6602 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "457 [D loss supervised: 0.2238, acc.: 100.00%] [D loss unsupervised: 0.1387, acc.: 100.00%] [G loss: 5.147485, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2268 - acc: 1.0000 - val_loss: 0.6585 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "458 [D loss supervised: 0.2268, acc.: 100.00%] [D loss unsupervised: 0.1388, acc.: 100.00%] [G loss: 4.853664, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2229 - acc: 1.0000 - val_loss: 0.6584 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "459 [D loss supervised: 0.2229, acc.: 100.00%] [D loss unsupervised: 0.1391, acc.: 100.00%] [G loss: 4.322186, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2737 - acc: 1.0000 - val_loss: 0.6594 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "460 [D loss supervised: 0.2737, acc.: 100.00%] [D loss unsupervised: 0.1388, acc.: 100.00%] [G loss: 4.049326, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3187 - acc: 0.9688 - val_loss: 0.6646 - val_acc: 0.8581\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "461 [D loss supervised: 0.3187, acc.: 96.88%] [D loss unsupervised: 0.1400, acc.: 100.00%] [G loss: 2.220240, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2505 - acc: 0.9688 - val_loss: 0.6718 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "462 [D loss supervised: 0.2505, acc.: 96.88%] [D loss unsupervised: 0.1393, acc.: 100.00%] [G loss: 2.249941, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4185 - acc: 0.9375 - val_loss: 0.6780 - val_acc: 0.8551\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "463 [D loss supervised: 0.4185, acc.: 93.75%] [D loss unsupervised: 0.1389, acc.: 100.00%] [G loss: 2.582822, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2693 - acc: 0.9688 - val_loss: 0.6839 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "464 [D loss supervised: 0.2693, acc.: 96.88%] [D loss unsupervised: 0.1390, acc.: 100.00%] [G loss: 2.103788, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2342 - acc: 1.0000 - val_loss: 0.6910 - val_acc: 0.8532\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "465 [D loss supervised: 0.2342, acc.: 100.00%] [D loss unsupervised: 0.1385, acc.: 100.00%] [G loss: 2.594411, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4510 - acc: 0.9688 - val_loss: 0.6953 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "466 [D loss supervised: 0.4510, acc.: 96.88%] [D loss unsupervised: 0.4548, acc.: 89.06%] [G loss: 7.661736, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2573 - acc: 1.0000 - val_loss: 0.6991 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "467 [D loss supervised: 0.2573, acc.: 100.00%] [D loss unsupervised: 3.5930, acc.: 50.00%] [G loss: 6.902751, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2884 - acc: 1.0000 - val_loss: 0.7025 - val_acc: 0.8514\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "468 [D loss supervised: 0.2884, acc.: 100.00%] [D loss unsupervised: 2.7185, acc.: 51.56%] [G loss: 4.337932, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2478 - acc: 0.9688 - val_loss: 0.7066 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "469 [D loss supervised: 0.2478, acc.: 96.88%] [D loss unsupervised: 1.3844, acc.: 56.25%] [G loss: 2.658898, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2339 - acc: 1.0000 - val_loss: 0.7106 - val_acc: 0.8494\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "470 [D loss supervised: 0.2339, acc.: 100.00%] [D loss unsupervised: 0.6416, acc.: 70.31%] [G loss: 1.650034, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2526 - acc: 1.0000 - val_loss: 0.7117 - val_acc: 0.8493\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "471 [D loss supervised: 0.2526, acc.: 100.00%] [D loss unsupervised: 0.4064, acc.: 90.62%] [G loss: 1.551754, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2684 - acc: 0.9688 - val_loss: 0.7068 - val_acc: 0.8504\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "472 [D loss supervised: 0.2684, acc.: 96.88%] [D loss unsupervised: 0.3367, acc.: 95.31%] [G loss: 1.214458, acc.: 18.75%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2897 - acc: 0.9688 - val_loss: 0.7001 - val_acc: 0.8515\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "473 [D loss supervised: 0.2897, acc.: 96.88%] [D loss unsupervised: 0.3922, acc.: 90.62%] [G loss: 1.595357, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2964 - acc: 0.9688 - val_loss: 0.6926 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "474 [D loss supervised: 0.2964, acc.: 96.88%] [D loss unsupervised: 0.2680, acc.: 100.00%] [G loss: 2.101987, acc.: 0.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2453 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.8552\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "475 [D loss supervised: 0.2453, acc.: 100.00%] [D loss unsupervised: 0.2171, acc.: 100.00%] [G loss: 2.277400, acc.: 6.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2537 - acc: 1.0000 - val_loss: 0.6764 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "476 [D loss supervised: 0.2537, acc.: 100.00%] [D loss unsupervised: 0.2274, acc.: 98.44%] [G loss: 2.517119, acc.: 3.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2535 - acc: 1.0000 - val_loss: 0.6680 - val_acc: 0.8582\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "477 [D loss supervised: 0.2535, acc.: 100.00%] [D loss unsupervised: 0.2394, acc.: 98.44%] [G loss: 2.303618, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2715 - acc: 0.9688 - val_loss: 0.6632 - val_acc: 0.8582\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "478 [D loss supervised: 0.2715, acc.: 96.88%] [D loss unsupervised: 0.2022, acc.: 100.00%] [G loss: 2.013140, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3229 - acc: 0.9688 - val_loss: 0.6580 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "479 [D loss supervised: 0.3229, acc.: 96.88%] [D loss unsupervised: 0.2472, acc.: 98.44%] [G loss: 2.174323, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3364 - acc: 0.9688 - val_loss: 0.6546 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "480 [D loss supervised: 0.3364, acc.: 96.88%] [D loss unsupervised: 0.2114, acc.: 98.44%] [G loss: 2.309388, acc.: 15.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2189 - acc: 1.0000 - val_loss: 0.6541 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "481 [D loss supervised: 0.2189, acc.: 100.00%] [D loss unsupervised: 0.2291, acc.: 96.88%] [G loss: 2.534171, acc.: 12.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3876 - acc: 0.9688 - val_loss: 0.6546 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "482 [D loss supervised: 0.3876, acc.: 96.88%] [D loss unsupervised: 0.2166, acc.: 98.44%] [G loss: 2.278991, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2825 - acc: 1.0000 - val_loss: 0.6555 - val_acc: 0.8600\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "483 [D loss supervised: 0.2825, acc.: 100.00%] [D loss unsupervised: 0.1847, acc.: 100.00%] [G loss: 1.896575, acc.: 37.50%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3679 - acc: 0.9062 - val_loss: 0.6558 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "484 [D loss supervised: 0.3679, acc.: 90.62%] [D loss unsupervised: 0.1920, acc.: 100.00%] [G loss: 2.221781, acc.: 9.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4336 - acc: 0.9688 - val_loss: 0.6550 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "485 [D loss supervised: 0.4336, acc.: 96.88%] [D loss unsupervised: 0.1979, acc.: 100.00%] [G loss: 2.156103, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2294 - acc: 1.0000 - val_loss: 0.6544 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "486 [D loss supervised: 0.2294, acc.: 100.00%] [D loss unsupervised: 0.2054, acc.: 100.00%] [G loss: 1.599647, acc.: 40.62%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2300 - acc: 1.0000 - val_loss: 0.6536 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "487 [D loss supervised: 0.2300, acc.: 100.00%] [D loss unsupervised: 0.1835, acc.: 100.00%] [G loss: 1.822977, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2535 - acc: 0.9688 - val_loss: 0.6542 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "488 [D loss supervised: 0.2535, acc.: 96.88%] [D loss unsupervised: 0.2088, acc.: 98.44%] [G loss: 1.552279, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2216 - acc: 1.0000 - val_loss: 0.6549 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "489 [D loss supervised: 0.2216, acc.: 100.00%] [D loss unsupervised: 0.1720, acc.: 100.00%] [G loss: 1.513780, acc.: 53.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3555 - acc: 0.9375 - val_loss: 0.6532 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "490 [D loss supervised: 0.3555, acc.: 93.75%] [D loss unsupervised: 0.1954, acc.: 98.44%] [G loss: 1.531944, acc.: 46.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.2147 - acc: 1.0000 - val_loss: 0.6524 - val_acc: 0.8602\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "491 [D loss supervised: 0.2147, acc.: 100.00%] [D loss unsupervised: 0.1861, acc.: 100.00%] [G loss: 1.661533, acc.: 25.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2522 - acc: 1.0000 - val_loss: 0.6522 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "492 [D loss supervised: 0.2522, acc.: 100.00%] [D loss unsupervised: 0.1844, acc.: 100.00%] [G loss: 1.850374, acc.: 28.12%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2721 - acc: 0.9688 - val_loss: 0.6514 - val_acc: 0.8600\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "493 [D loss supervised: 0.2721, acc.: 96.88%] [D loss unsupervised: 0.2019, acc.: 98.44%] [G loss: 1.870528, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2437 - acc: 1.0000 - val_loss: 0.6499 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "494 [D loss supervised: 0.2437, acc.: 100.00%] [D loss unsupervised: 0.1927, acc.: 98.44%] [G loss: 1.664862, acc.: 34.38%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2408 - acc: 1.0000 - val_loss: 0.6483 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "495 [D loss supervised: 0.2408, acc.: 100.00%] [D loss unsupervised: 0.1980, acc.: 100.00%] [G loss: 1.210494, acc.: 50.00%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2748 - acc: 0.9688 - val_loss: 0.6473 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "496 [D loss supervised: 0.2748, acc.: 96.88%] [D loss unsupervised: 0.1951, acc.: 98.44%] [G loss: 1.759200, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2233 - acc: 1.0000 - val_loss: 0.6463 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "497 [D loss supervised: 0.2233, acc.: 100.00%] [D loss unsupervised: 0.1917, acc.: 98.44%] [G loss: 1.907724, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2499 - acc: 0.9688 - val_loss: 0.6461 - val_acc: 0.8599\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "498 [D loss supervised: 0.2499, acc.: 96.88%] [D loss unsupervised: 0.1834, acc.: 98.44%] [G loss: 1.604475, acc.: 31.25%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2468 - acc: 1.0000 - val_loss: 0.6474 - val_acc: 0.8598\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "499 [D loss supervised: 0.2468, acc.: 100.00%] [D loss unsupervised: 0.1774, acc.: 98.44%] [G loss: 1.538978, acc.: 21.88%]\n",
            "Epoch 1/1\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4775 - acc: 0.9062 - val_loss: 0.6483 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "500 [D loss supervised: 0.4775, acc.: 90.62%] [D loss unsupervised: 0.1845, acc.: 98.44%] [G loss: 1.581404, acc.: 21.88%]\n",
            "Training time: 1852.0433s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfUFnOB4qLYj",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rt3ZhdCn6mk",
        "colab_type": "code",
        "outputId": "e0a9b039-a46b-4686-c3a5-a2b4fcfd723c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-300.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 22s 448us/step\n",
            "Training Accuracy: 88.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO7jfk1Pa-J8",
        "colab_type": "code",
        "outputId": "e8ae5ac1-25da-4a46-8b40-88cac26b3f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-300.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 385us/step\n",
            "Test Accuracy: 85.86%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN3Q_FxXLCN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_supervised_loss = np.array(losses_d_supervised)\n",
        "d_unsupervised_loss = np.array(losses_d_unsupervised)\n",
        "d_unsupervised_real_loss = np.array(losses_d_unsupervised_real)\n",
        "d_unsupervised_fake_loss = np.array(losses_d_unsupervised_fake)\n",
        "d_loss = np.array(losses_d)\n",
        "g_loss = np.array(losses_g)  # Generator unsupervised loss\n",
        "all_loss = np.add(d_loss, g_loss)\n",
        "\n",
        "# Plot Discriminator supervised loss\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, d_supervised_loss, label=\"Discriminator supervised loss\", color='blue', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, d_unsupervised_loss, label=\"Discriminator unsupervised loss\", color='green', linestyle='dashed')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_real_loss, label=\"Discriminator unsupervised real loss\", color='yellow')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_fake_loss, label=\"Discriminator unsupervised fake loss\", color='yellow')\n",
        "plt.plot(iteration_checkpoints, g_loss, label=\"Generator unsupervised loss\", color='tab:red', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, all_loss, label=\"All losses\", color='black')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"SCGAN-2D's Discriminator Loss + Generator Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk87Xx_Aa-Bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82c3a30e-e34a-4bb6-d980-fe13456033da"
      },
      "source": [
        "div = 1   # 1\n",
        "accs = []\n",
        "tx = [x for x in range(1*div, len(iteration_checkpoints)+1, div)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"SCGAN-2D's accs with epoch, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 431us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 408us/step\n",
            "10000/10000 [==============================] - 4s 402us/step\n",
            "10000/10000 [==============================] - 4s 404us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 399us/step\n",
            "10000/10000 [==============================] - 4s 392us/step\n",
            "10000/10000 [==============================] - 4s 393us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 395us/step\n",
            "10000/10000 [==============================] - 4s 395us/step\n",
            "10000/10000 [==============================] - 4s 399us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 405us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 402us/step\n",
            "10000/10000 [==============================] - 4s 397us/step\n",
            "10000/10000 [==============================] - 4s 394us/step\n",
            "10000/10000 [==============================] - 4s 395us/step\n",
            "10000/10000 [==============================] - 4s 393us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 395us/step\n",
            "10000/10000 [==============================] - 4s 394us/step\n",
            "10000/10000 [==============================] - 4s 397us/step\n",
            "10000/10000 [==============================] - 4s 397us/step\n",
            "10000/10000 [==============================] - 4s 397us/step\n",
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 403us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 402us/step\n",
            "10000/10000 [==============================] - 4s 400us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 394us/step\n",
            "10000/10000 [==============================] - 4s 394us/step\n",
            "10000/10000 [==============================] - 4s 394us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 395us/step\n",
            "10000/10000 [==============================] - 4s 401us/step\n",
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 394us/step\n",
            "10000/10000 [==============================] - 4s 399us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 399us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 397us/step\n",
            "0.874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29ce5bd710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFWCAYAAAA2SU9mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXfwxMiWbBLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "7ea93829-d5ca-4b9c-d4a4-f1ce6f32b5f5"
      },
      "source": [
        "# d_supervised_loss = np.array(losses_d_supervised)\n",
        "# d_unsupervised_loss = np.array(losses_d_unsupervised)\n",
        "# d_unsupervised_real_loss = np.array(losses_d_unsupervised_real)\n",
        "# d_unsupervised_fake_loss = np.array(losses_d_unsupervised_fake)\n",
        "# d_loss = np.array(losses_d)\n",
        "# g_loss = np.array(losses_g)  # Generator unsupervised loss\n",
        "# all_loss = np.add(d_loss, g_loss)\n",
        "\n",
        "# # Plot Discriminator supervised loss\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.plot(iteration_checkpoints, d_supervised_loss, label=\"Discriminator supervised loss\", color='blue', linestyle='dashed')\n",
        "# plt.plot(iteration_checkpoints, d_unsupervised_loss, label=\"Discriminator unsupervised loss\", color='green', linestyle='dashed')\n",
        "# # plt.plot(iteration_checkpoints, d_unsupervised_real_loss, label=\"Discriminator unsupervised real loss\", color='yellow')\n",
        "# # plt.plot(iteration_checkpoints, d_unsupervised_fake_loss, label=\"Discriminator unsupervised fake loss\", color='yellow')\n",
        "# plt.plot(iteration_checkpoints, g_loss, label=\"Generator unsupervised loss\", color='tab:red', linestyle='dashed')\n",
        "# plt.plot(iteration_checkpoints, all_loss, label=\"All losses\", color='black')\n",
        "\n",
        "# plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "# plt.title(\"SCGAN-2D's Discriminator Loss + Generator Loss, num_labeled=%d\" % num_labeled)\n",
        "# plt.xlabel(\"Iteration\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.legend()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29ce2026a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAFWCAYAAAAR586OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq-YzbJRqOMH",
        "colab_type": "text"
      },
      "source": [
        "## Generate Pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB2PSr3uEFw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMF87KVVEFuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "# fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "# fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "# gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "d_name = {0:\"airplane\", 1:\"automobile\", 2:\"bird\", 3:\"cat\", 4:\"deer\", 5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9:\"truck\"}\t\n",
        "\n",
        "def sample_images(image_grid_rows=2, image_grid_columns=5):\n",
        "\n",
        "    # Sample random noise\n",
        "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # Get image labels 0-9\n",
        "    fake_labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "    fake_labels_category = to_categorical(fake_labels, num_classes=num_classes)\n",
        "\n",
        "    # Generate images from random noise\n",
        "    gen_imgs = generator.predict([z, fake_labels_category])\n",
        "\n",
        "    # Rescale image pixel values to [0, 1]\n",
        "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    # gen_imgs = (gen_imgs+1) * 255/2\n",
        "\n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows,\n",
        "                            image_grid_columns,\n",
        "                            figsize=(10, 4),\n",
        "                            sharey=True,\n",
        "                            sharex=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output a grid of images\n",
        "            axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')\n",
        "            axs[i, j].set_title(\"Class: \" + str(d_name[fake_labels[cnt]]))\n",
        "            cnt += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx4Ocr7DLE0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "3e79b096-9a7a-4c86-b2b6-0c1909a0b395"
      },
      "source": [
        "sample_images()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAD1CAYAAABUdy/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAD1CAYAAABUdy/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYpjIMLIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQdwFLMvBd-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_images_specific(image_grid_rows=2, image_grid_columns=5, label=0):\n",
        "\n",
        "    # Sample random noise\n",
        "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # Get image labels 0-9\n",
        "    fake_labels = [label] * 10\n",
        "    fake_labels_category = to_categorical(fake_labels, num_classes=num_classes)\n",
        "\n",
        "    # Generate images from random noise\n",
        "    gen_imgs = generator.predict([z, fake_labels_category])\n",
        "\n",
        "    # Rescale image pixel values to [0, 1]\n",
        "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    # gen_imgs = (gen_imgs+1) * 255/2\n",
        "\n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows,\n",
        "                            image_grid_columns,\n",
        "                            figsize=(10, 4),\n",
        "                            sharey=True,\n",
        "                            sharex=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output a grid of images\n",
        "            axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')\n",
        "            axs[i, j].set_title(\"Class: \" + str(d_name[fake_labels[cnt]]))\n",
        "            cnt += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Qw77TpBvcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "21bd40f1-7ffb-49d7-90e9-07a203fb9663"
      },
      "source": [
        "sample_images_specific(2,5,0)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAD1CAYAAABUdy/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgIT1giiqQU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def plot_images_labels_prediction(images,labels,prediction,idx,num=10):\n",
        "#     fig=plt.gcf()\n",
        "#     fig.set_size_inches(12,14)\n",
        "#     if num>25: num=25\n",
        "#     for i in range(0,num):\n",
        "#         ax = plt.subplot(5,5,i+1)\n",
        "#         ax.imshow(images[idx],cmap='binary') \n",
        "#         title= str(i)+' '+label_dict[labels[i][0]]   #\n",
        "#         if len(prediction)>0:\n",
        "#             title+= '=>'+label_dict[prediction[i]]   #\n",
        "#         ax.set_title(title,fontsize=10)\n",
        "#         ax.set_xticks([])\n",
        "#         ax.set_yticks([])\n",
        "#         idx+=1\n",
        "#     plt.show()\n",
        "\n",
        "# plot_images_labels_prediction(x_train,y_train,[],0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZNbvo9zqSZB",
        "colab_type": "text"
      },
      "source": [
        "# Pseudo Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYa-jKsACxRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pseudo_model = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "pseudo_model.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjVlrccQCxOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pseudo_train(iterations, batch_size, save_interval, alpha_f, t1, t2, iter_epochs):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # Get unlabeled examples and pseudo labels\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "        pseudo_label = pseudo_model.predict(imgs_unlabeled)\n",
        "\n",
        "        # -------------------------\n",
        "        #  Supervised Training\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs_labeled, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Train on labeled examples\n",
        "        alpha = 1\n",
        "        # loss_labeled, acc_labeled = pseudo_model.train_on_batch(imgs_labeled, labels)\n",
        "        datagen.fit(imgs_labeled)\n",
        "        pseudo_model.fit_generator(datagen.flow(imgs_labeled, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        loss_labeled, acc_labeled = history.losses[-1], history.accs[-1]\n",
        "\n",
        "\n",
        "        loss_unlabeled = -1\n",
        "        acc_unlabeled = -1\n",
        "\n",
        "        # -------------------------\n",
        "        #  Supervised Training\n",
        "        # -------------------------\n",
        "\n",
        "        # Set alpha\n",
        "        if iteration < t1: alpha = 0\n",
        "        else:\n",
        "            if t1 <= iteration < t2: alpha = (iteration - t1)/(t2 - t1) * alpha_f\n",
        "            else: alpha = alpha_f\n",
        "\n",
        "            # Train on unlabeled examples\n",
        "            loss_unlabeled, acc_unlabeled = pseudo_model.train_on_batch(imgs_unlabeled, pseudo_label)\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "          # Save losses\n",
        "          losses_pseudo_labeled.append(loss_labeled)\n",
        "          losses_pseudo_unlabeled.append(loss_unlabeled)\n",
        "          losses_pseudo.append(loss_labeled + alpha * loss_unlabeled)\n",
        "          accs_pseudo_labeled.append(acc_labeled)\n",
        "          accs_pseudo_unlabeled.append(acc_unlabeled)\n",
        "          accs_pseudo.append((acc_labeled + alpha*acc_unlabeled)/(1 + alpha))\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [supervised loss: %.4f, acc: %.2f%%] [unsupervised loss: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, loss_labeled, 100 * acc_labeled, \n",
        "                  loss_unlabeled, 100 * acc_unlabeled))\n",
        "          \n",
        "          pseudo_model.save(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\" + str(iteration+1) + \".h5\")\n",
        "          file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_supervised_losses.json\"\n",
        "          file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_unsupervised_losses.json\"\n",
        "          file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/p_losses.json\"\n",
        "          with open(file1, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo_labeled), json_file)\n",
        "          with open(file2, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo_unlabeled), json_file)\n",
        "          with open(file3, 'w') as json_file:\n",
        "                json.dump(str(losses_pseudo), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GnU0n4JCxMB",
        "colab_type": "code",
        "outputId": "70b8912e-33ef-4402-f870-379a5994b8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 50 # 30\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "alpha_f = 3\n",
        "t1 = 2 # 500\n",
        "t2 = 4 # 1000\n",
        "iter_epochs = 10\n",
        "\n",
        "losses_pseudo_labeled = []\n",
        "losses_pseudo_unlabeled = []\n",
        "losses_pseudo = []\n",
        "accs_pseudo_labeled = []\n",
        "accs_pseudo_unlabeled = []\n",
        "accs_pseudo = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "pseudo_model = load_model(\"./models/cifar10_model.035.h5\")\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the SGGAN for the specified number of iterations\n",
        "pseudo_train(iterations, batch_size, save_interval, alpha_f, t1, t2, iter_epochs)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 15s 15s/step - loss: 0.5187 - acc: 0.8438 - val_loss: 0.6246 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4502 - acc: 0.8750 - val_loss: 0.6295 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4992 - acc: 0.8750 - val_loss: 0.6264 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3824 - acc: 0.9062 - val_loss: 0.6310 - val_acc: 0.8585\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3377 - acc: 0.9375 - val_loss: 0.6443 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2767 - acc: 1.0000 - val_loss: 0.6573 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2750 - acc: 1.0000 - val_loss: 0.6726 - val_acc: 0.8450\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2612 - acc: 1.0000 - val_loss: 0.6912 - val_acc: 0.8393\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2600 - acc: 1.0000 - val_loss: 0.7119 - val_acc: 0.8331\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2472 - acc: 1.0000 - val_loss: 0.7324 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "1 [supervised loss: 0.2472, acc: 100.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7185 - acc: 0.7812 - val_loss: 0.7439 - val_acc: 0.8237\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7609 - acc: 0.8438 - val_loss: 0.7525 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5938 - acc: 0.8438 - val_loss: 0.7622 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4963 - acc: 0.8438 - val_loss: 0.7738 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3935 - acc: 0.9688 - val_loss: 0.7816 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3495 - acc: 0.9688 - val_loss: 0.7909 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3013 - acc: 0.9688 - val_loss: 0.7963 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2611 - acc: 1.0000 - val_loss: 0.8001 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2802 - acc: 1.0000 - val_loss: 0.8044 - val_acc: 0.8035\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2478 - acc: 1.0000 - val_loss: 0.8090 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "2 [supervised loss: 0.2478, acc: 100.00%] [unsupervised loss: -1.0000, acc: -100.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7037 - acc: 0.8750 - val_loss: 0.7997 - val_acc: 0.8053\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5812 - acc: 0.9062 - val_loss: 0.7883 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5407 - acc: 0.9062 - val_loss: 0.7737 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4879 - acc: 0.9375 - val_loss: 0.7600 - val_acc: 0.8174\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4591 - acc: 0.9062 - val_loss: 0.7526 - val_acc: 0.8184\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3662 - acc: 0.9688 - val_loss: 0.7494 - val_acc: 0.8223\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3193 - acc: 0.9688 - val_loss: 0.7395 - val_acc: 0.8241\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2533 - acc: 1.0000 - val_loss: 0.7334 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2616 - acc: 1.0000 - val_loss: 0.7296 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2465 - acc: 1.0000 - val_loss: 0.7273 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "3 [supervised loss: 0.2465, acc: 100.00%] [unsupervised loss: 1.0317, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4629 - acc: 0.9062 - val_loss: 0.7357 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4436 - acc: 0.9062 - val_loss: 0.7432 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4260 - acc: 0.9062 - val_loss: 0.7543 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2789 - acc: 1.0000 - val_loss: 0.7671 - val_acc: 0.8140\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3172 - acc: 0.9688 - val_loss: 0.7793 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2691 - acc: 1.0000 - val_loss: 0.7918 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2652 - acc: 1.0000 - val_loss: 0.8042 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2488 - acc: 1.0000 - val_loss: 0.8168 - val_acc: 0.8020\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2639 - acc: 1.0000 - val_loss: 0.8292 - val_acc: 0.7989\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2353 - acc: 1.0000 - val_loss: 0.8408 - val_acc: 0.7947\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "4 [supervised loss: 0.2353, acc: 100.00%] [unsupervised loss: 0.6375, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4921 - acc: 0.9062 - val_loss: 0.8606 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4898 - acc: 0.8750 - val_loss: 0.8694 - val_acc: 0.7881\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3901 - acc: 0.9375 - val_loss: 0.8764 - val_acc: 0.7874\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3687 - acc: 0.9375 - val_loss: 0.8807 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3743 - acc: 0.9688 - val_loss: 0.8854 - val_acc: 0.7842\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3083 - acc: 0.9688 - val_loss: 0.8906 - val_acc: 0.7828\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3799 - acc: 0.9375 - val_loss: 0.8944 - val_acc: 0.7839\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3279 - acc: 0.9688 - val_loss: 0.8988 - val_acc: 0.7839\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2737 - acc: 1.0000 - val_loss: 0.9044 - val_acc: 0.7831\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2769 - acc: 0.9688 - val_loss: 0.9096 - val_acc: 0.7830\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "5 [supervised loss: 0.2769, acc: 96.88%] [unsupervised loss: 0.7566, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6808 - acc: 0.8750 - val_loss: 0.9036 - val_acc: 0.7853\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7193 - acc: 0.8125 - val_loss: 0.8800 - val_acc: 0.7887\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5221 - acc: 0.9062 - val_loss: 0.8555 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5153 - acc: 0.9062 - val_loss: 0.8301 - val_acc: 0.8036\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3917 - acc: 0.9375 - val_loss: 0.8131 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4908 - acc: 0.9688 - val_loss: 0.7988 - val_acc: 0.8120\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3810 - acc: 0.9375 - val_loss: 0.7891 - val_acc: 0.8120\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2739 - acc: 1.0000 - val_loss: 0.7859 - val_acc: 0.8116\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3639 - acc: 0.9688 - val_loss: 0.7851 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2716 - acc: 1.0000 - val_loss: 0.7876 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "6 [supervised loss: 0.2716, acc: 100.00%] [unsupervised loss: 0.9091, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7527 - acc: 0.8125 - val_loss: 0.8065 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7241 - acc: 0.9062 - val_loss: 0.8046 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4780 - acc: 0.8750 - val_loss: 0.8022 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7295 - acc: 0.8438 - val_loss: 0.7975 - val_acc: 0.8072\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3980 - acc: 0.9375 - val_loss: 0.7962 - val_acc: 0.8093\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4474 - acc: 0.9375 - val_loss: 0.7972 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3447 - acc: 0.9688 - val_loss: 0.8002 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2867 - acc: 1.0000 - val_loss: 0.8070 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2804 - acc: 1.0000 - val_loss: 0.8172 - val_acc: 0.8019\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2396 - acc: 1.0000 - val_loss: 0.8265 - val_acc: 0.8002\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "7 [supervised loss: 0.2396, acc: 100.00%] [unsupervised loss: 0.6325, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6480 - acc: 0.8438 - val_loss: 0.8336 - val_acc: 0.7977\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6685 - acc: 0.8125 - val_loss: 0.8241 - val_acc: 0.7984\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5017 - acc: 0.9062 - val_loss: 0.8126 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4281 - acc: 0.9375 - val_loss: 0.8007 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4488 - acc: 0.8750 - val_loss: 0.7935 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3246 - acc: 0.9375 - val_loss: 0.7937 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3096 - acc: 0.9688 - val_loss: 0.8002 - val_acc: 0.8059\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2957 - acc: 0.9688 - val_loss: 0.8132 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2878 - acc: 0.9688 - val_loss: 0.8316 - val_acc: 0.7989\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2322 - acc: 1.0000 - val_loss: 0.8542 - val_acc: 0.7929\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "8 [supervised loss: 0.2322, acc: 100.00%] [unsupervised loss: 0.7895, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7514 - acc: 0.8750 - val_loss: 0.8655 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7200 - acc: 0.8438 - val_loss: 0.8687 - val_acc: 0.7890\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6679 - acc: 0.8438 - val_loss: 0.8759 - val_acc: 0.7873\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4953 - acc: 0.8750 - val_loss: 0.8817 - val_acc: 0.7864\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5462 - acc: 0.9062 - val_loss: 0.8847 - val_acc: 0.7843\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3288 - acc: 0.9688 - val_loss: 0.8941 - val_acc: 0.7794\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2838 - acc: 1.0000 - val_loss: 0.9075 - val_acc: 0.7772\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2790 - acc: 1.0000 - val_loss: 0.9223 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 0.9396 - val_acc: 0.7686\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2442 - acc: 1.0000 - val_loss: 0.9593 - val_acc: 0.7628\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "9 [supervised loss: 0.2442, acc: 100.00%] [unsupervised loss: 0.8021, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6101 - acc: 0.8438 - val_loss: 0.9846 - val_acc: 0.7583\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4511 - acc: 0.9062 - val_loss: 0.9857 - val_acc: 0.7576\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6921 - acc: 0.8438 - val_loss: 0.9702 - val_acc: 0.7616\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4361 - acc: 0.8750 - val_loss: 0.9482 - val_acc: 0.7676\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3500 - acc: 0.9062 - val_loss: 0.9270 - val_acc: 0.7736\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3253 - acc: 0.9688 - val_loss: 0.9124 - val_acc: 0.7791\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2867 - acc: 0.9688 - val_loss: 0.9032 - val_acc: 0.7841\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2718 - acc: 0.9688 - val_loss: 0.8990 - val_acc: 0.7879\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2650 - acc: 0.9688 - val_loss: 0.8971 - val_acc: 0.7878\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2357 - acc: 1.0000 - val_loss: 0.8993 - val_acc: 0.7879\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "10 [supervised loss: 0.2357, acc: 100.00%] [unsupervised loss: 0.8588, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8206 - acc: 0.8438 - val_loss: 0.9156 - val_acc: 0.7860\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7687 - acc: 0.7812 - val_loss: 0.9406 - val_acc: 0.7824\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7149 - acc: 0.7812 - val_loss: 0.9890 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4073 - acc: 0.9375 - val_loss: 1.0640 - val_acc: 0.7616\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3399 - acc: 0.9688 - val_loss: 1.1736 - val_acc: 0.7421\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3403 - acc: 0.9375 - val_loss: 1.3096 - val_acc: 0.7163\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2960 - acc: 0.9688 - val_loss: 1.4745 - val_acc: 0.6921\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2509 - acc: 1.0000 - val_loss: 1.6522 - val_acc: 0.6692\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2911 - acc: 0.9688 - val_loss: 1.8452 - val_acc: 0.6474\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2384 - acc: 1.0000 - val_loss: 2.0318 - val_acc: 0.6299\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "11 [supervised loss: 0.2384, acc: 100.00%] [unsupervised loss: 1.1306, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8202 - acc: 0.8125 - val_loss: 2.3062 - val_acc: 0.6044\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6360 - acc: 0.8125 - val_loss: 2.3658 - val_acc: 0.5993\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5099 - acc: 0.8750 - val_loss: 2.3687 - val_acc: 0.5992\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4819 - acc: 0.9062 - val_loss: 2.3175 - val_acc: 0.6061\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3894 - acc: 0.9688 - val_loss: 2.2081 - val_acc: 0.6167\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3093 - acc: 0.9688 - val_loss: 2.1359 - val_acc: 0.6240\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2856 - acc: 1.0000 - val_loss: 2.0826 - val_acc: 0.6295\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2648 - acc: 1.0000 - val_loss: 2.0427 - val_acc: 0.6322\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2737 - acc: 1.0000 - val_loss: 2.0122 - val_acc: 0.6359\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2548 - acc: 1.0000 - val_loss: 1.9835 - val_acc: 0.6380\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "12 [supervised loss: 0.2548, acc: 100.00%] [unsupervised loss: 1.7774, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8787 - acc: 0.7188 - val_loss: 2.1469 - val_acc: 0.6197\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8187 - acc: 0.8125 - val_loss: 2.1467 - val_acc: 0.6177\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7901 - acc: 0.8750 - val_loss: 2.0738 - val_acc: 0.6222\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5794 - acc: 0.9062 - val_loss: 2.0202 - val_acc: 0.6254\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5625 - acc: 0.9062 - val_loss: 1.9614 - val_acc: 0.6294\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3780 - acc: 0.9688 - val_loss: 1.9381 - val_acc: 0.6273\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3641 - acc: 0.9688 - val_loss: 1.9262 - val_acc: 0.6264\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3713 - acc: 0.9375 - val_loss: 1.9082 - val_acc: 0.6244\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3040 - acc: 0.9688 - val_loss: 1.8969 - val_acc: 0.6211\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2684 - acc: 1.0000 - val_loss: 1.8902 - val_acc: 0.6169\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "13 [supervised loss: 0.2684, acc: 100.00%] [unsupervised loss: 1.5994, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6032 - acc: 0.8438 - val_loss: 1.8438 - val_acc: 0.6154\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5265 - acc: 0.8750 - val_loss: 1.7776 - val_acc: 0.6224\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4985 - acc: 0.8750 - val_loss: 1.6993 - val_acc: 0.6346\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3760 - acc: 0.9375 - val_loss: 1.6207 - val_acc: 0.6453\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5315 - acc: 0.9375 - val_loss: 1.5413 - val_acc: 0.6585\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3852 - acc: 0.9375 - val_loss: 1.4719 - val_acc: 0.6707\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3163 - acc: 1.0000 - val_loss: 1.4097 - val_acc: 0.6814\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3006 - acc: 1.0000 - val_loss: 1.3579 - val_acc: 0.6917\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2962 - acc: 1.0000 - val_loss: 1.3157 - val_acc: 0.6994\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2503 - acc: 1.0000 - val_loss: 1.2814 - val_acc: 0.7066\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "14 [supervised loss: 0.2503, acc: 100.00%] [unsupervised loss: 1.7767, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8204 - acc: 0.8125 - val_loss: 1.3025 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7945 - acc: 0.8125 - val_loss: 1.2915 - val_acc: 0.7050\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4566 - acc: 0.9375 - val_loss: 1.2757 - val_acc: 0.7082\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5190 - acc: 0.8750 - val_loss: 1.2571 - val_acc: 0.7098\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4165 - acc: 0.9062 - val_loss: 1.2388 - val_acc: 0.7124\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3912 - acc: 0.9688 - val_loss: 1.2224 - val_acc: 0.7171\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3031 - acc: 0.9688 - val_loss: 1.2158 - val_acc: 0.7185\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2948 - acc: 1.0000 - val_loss: 1.2088 - val_acc: 0.7197\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2398 - acc: 1.0000 - val_loss: 1.2094 - val_acc: 0.7173\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2443 - acc: 1.0000 - val_loss: 1.2110 - val_acc: 0.7145\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "15 [supervised loss: 0.2443, acc: 100.00%] [unsupervised loss: 0.9956, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4498 - acc: 0.9062 - val_loss: 1.2294 - val_acc: 0.7092\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3811 - acc: 0.9688 - val_loss: 1.2442 - val_acc: 0.7046\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3703 - acc: 0.9375 - val_loss: 1.2596 - val_acc: 0.7015\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3970 - acc: 0.9375 - val_loss: 1.2765 - val_acc: 0.6998\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3023 - acc: 1.0000 - val_loss: 1.2938 - val_acc: 0.6969\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2667 - acc: 1.0000 - val_loss: 1.3128 - val_acc: 0.6935\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2721 - acc: 1.0000 - val_loss: 1.3337 - val_acc: 0.6908\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2880 - acc: 1.0000 - val_loss: 1.3524 - val_acc: 0.6893\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2479 - acc: 1.0000 - val_loss: 1.3637 - val_acc: 0.6886\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2290 - acc: 1.0000 - val_loss: 1.3752 - val_acc: 0.6862\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "16 [supervised loss: 0.2290, acc: 100.00%] [unsupervised loss: 0.7974, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7117 - acc: 0.8125 - val_loss: 1.4603 - val_acc: 0.6733\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7596 - acc: 0.8125 - val_loss: 1.5282 - val_acc: 0.6621\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5000 - acc: 0.9375 - val_loss: 1.6375 - val_acc: 0.6500\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4860 - acc: 0.9062 - val_loss: 1.7594 - val_acc: 0.6396\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4359 - acc: 0.9375 - val_loss: 1.9158 - val_acc: 0.6246\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5074 - acc: 0.9375 - val_loss: 2.0745 - val_acc: 0.6082\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3902 - acc: 0.9375 - val_loss: 2.1822 - val_acc: 0.6007\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3062 - acc: 0.9688 - val_loss: 2.3050 - val_acc: 0.5887\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2823 - acc: 1.0000 - val_loss: 2.3818 - val_acc: 0.5842\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2521 - acc: 1.0000 - val_loss: 2.4356 - val_acc: 0.5803\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "17 [supervised loss: 0.2521, acc: 100.00%] [unsupervised loss: 1.1480, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9379 - acc: 0.8750 - val_loss: 2.5090 - val_acc: 0.5717\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8150 - acc: 0.8750 - val_loss: 2.3903 - val_acc: 0.5777\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7231 - acc: 0.9062 - val_loss: 2.2131 - val_acc: 0.5892\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5064 - acc: 0.9375 - val_loss: 2.0349 - val_acc: 0.6011\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5182 - acc: 0.9375 - val_loss: 1.8667 - val_acc: 0.6180\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3301 - acc: 0.9688 - val_loss: 1.7374 - val_acc: 0.6329\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2827 - acc: 1.0000 - val_loss: 1.6598 - val_acc: 0.6477\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2664 - acc: 1.0000 - val_loss: 1.6344 - val_acc: 0.6573\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2449 - acc: 1.0000 - val_loss: 1.6452 - val_acc: 0.6565\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2383 - acc: 1.0000 - val_loss: 1.6809 - val_acc: 0.6526\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "18 [supervised loss: 0.2383, acc: 100.00%] [unsupervised loss: 1.8860, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4914 - acc: 0.8750 - val_loss: 1.8243 - val_acc: 0.6305\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3919 - acc: 0.9375 - val_loss: 1.8222 - val_acc: 0.6309\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5040 - acc: 0.8750 - val_loss: 1.7791 - val_acc: 0.6370\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3347 - acc: 0.9688 - val_loss: 1.7439 - val_acc: 0.6408\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3104 - acc: 0.9688 - val_loss: 1.7310 - val_acc: 0.6434\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2780 - acc: 0.9688 - val_loss: 1.7386 - val_acc: 0.6420\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 1.7588 - val_acc: 0.6401\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2562 - acc: 1.0000 - val_loss: 1.7867 - val_acc: 0.6369\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2428 - acc: 1.0000 - val_loss: 1.8187 - val_acc: 0.6332\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2530 - acc: 1.0000 - val_loss: 1.8493 - val_acc: 0.6297\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "19 [supervised loss: 0.2530, acc: 100.00%] [unsupervised loss: 1.7833, acc: 56.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6002 - acc: 0.8438 - val_loss: 1.8954 - val_acc: 0.6231\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5479 - acc: 0.8438 - val_loss: 1.9430 - val_acc: 0.6152\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3524 - acc: 1.0000 - val_loss: 1.9874 - val_acc: 0.6093\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4593 - acc: 0.8750 - val_loss: 2.0459 - val_acc: 0.6023\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3364 - acc: 1.0000 - val_loss: 2.0974 - val_acc: 0.5968\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3131 - acc: 0.9688 - val_loss: 2.1415 - val_acc: 0.5901\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3017 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.5852\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3244 - acc: 0.9688 - val_loss: 2.1948 - val_acc: 0.5853\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2505 - acc: 1.0000 - val_loss: 2.2110 - val_acc: 0.5845\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2647 - acc: 1.0000 - val_loss: 2.2036 - val_acc: 0.5832\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "20 [supervised loss: 0.2647, acc: 100.00%] [unsupervised loss: 1.6970, acc: 53.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5756 - acc: 0.8438 - val_loss: 2.3149 - val_acc: 0.5699\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4993 - acc: 0.8750 - val_loss: 2.1551 - val_acc: 0.5902\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3491 - acc: 0.9688 - val_loss: 2.0081 - val_acc: 0.6070\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3148 - acc: 0.9688 - val_loss: 1.8867 - val_acc: 0.6187\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3661 - acc: 0.9688 - val_loss: 1.7561 - val_acc: 0.6360\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2550 - acc: 1.0000 - val_loss: 1.6516 - val_acc: 0.6478\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2698 - acc: 0.9688 - val_loss: 1.5634 - val_acc: 0.6578\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2319 - acc: 1.0000 - val_loss: 1.4944 - val_acc: 0.6660\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2494 - acc: 1.0000 - val_loss: 1.4407 - val_acc: 0.6733\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2580 - acc: 1.0000 - val_loss: 1.3992 - val_acc: 0.6768\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "21 [supervised loss: 0.2580, acc: 100.00%] [unsupervised loss: 2.0369, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5114 - acc: 0.9062 - val_loss: 1.4310 - val_acc: 0.6701\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4519 - acc: 0.9062 - val_loss: 1.4498 - val_acc: 0.6665\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4453 - acc: 0.9375 - val_loss: 1.4754 - val_acc: 0.6613\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3248 - acc: 0.9688 - val_loss: 1.5037 - val_acc: 0.6563\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3946 - acc: 0.9375 - val_loss: 1.5331 - val_acc: 0.6526\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4036 - acc: 0.9688 - val_loss: 1.5584 - val_acc: 0.6485\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3267 - acc: 0.9375 - val_loss: 1.5712 - val_acc: 0.6467\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2413 - acc: 1.0000 - val_loss: 1.5859 - val_acc: 0.6447\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2730 - acc: 0.9688 - val_loss: 1.5997 - val_acc: 0.6412\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2586 - acc: 1.0000 - val_loss: 1.6160 - val_acc: 0.6374\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "22 [supervised loss: 0.2586, acc: 100.00%] [unsupervised loss: 1.4378, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7828 - acc: 0.8438 - val_loss: 1.6970 - val_acc: 0.6272\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7055 - acc: 0.8750 - val_loss: 1.6896 - val_acc: 0.6289\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6836 - acc: 0.8438 - val_loss: 1.6404 - val_acc: 0.6356\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3543 - acc: 0.9688 - val_loss: 1.5867 - val_acc: 0.6444\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5349 - acc: 0.9062 - val_loss: 1.5374 - val_acc: 0.6525\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3765 - acc: 0.9688 - val_loss: 1.4723 - val_acc: 0.6657\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2973 - acc: 1.0000 - val_loss: 1.4084 - val_acc: 0.6787\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2678 - acc: 1.0000 - val_loss: 1.3531 - val_acc: 0.6903\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2595 - acc: 1.0000 - val_loss: 1.3053 - val_acc: 0.7010\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2508 - acc: 1.0000 - val_loss: 1.2646 - val_acc: 0.7086\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "23 [supervised loss: 0.2508, acc: 100.00%] [unsupervised loss: 1.4586, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5499 - acc: 0.8438 - val_loss: 1.3188 - val_acc: 0.6950\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4926 - acc: 0.9375 - val_loss: 1.3526 - val_acc: 0.6884\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4681 - acc: 0.9375 - val_loss: 1.3835 - val_acc: 0.6834\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3564 - acc: 0.9688 - val_loss: 1.4289 - val_acc: 0.6755\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3586 - acc: 0.9688 - val_loss: 1.4856 - val_acc: 0.6649\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2927 - acc: 1.0000 - val_loss: 1.5608 - val_acc: 0.6522\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2736 - acc: 1.0000 - val_loss: 1.6438 - val_acc: 0.6430\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2838 - acc: 1.0000 - val_loss: 1.7328 - val_acc: 0.6340\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2828 - acc: 1.0000 - val_loss: 1.8277 - val_acc: 0.6220\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2294 - acc: 1.0000 - val_loss: 1.9168 - val_acc: 0.6165\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "24 [supervised loss: 0.2294, acc: 100.00%] [unsupervised loss: 1.3844, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8616 - acc: 0.7188 - val_loss: 1.7878 - val_acc: 0.6295\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7370 - acc: 0.8438 - val_loss: 1.6521 - val_acc: 0.6453\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5880 - acc: 0.8750 - val_loss: 1.5065 - val_acc: 0.6660\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5643 - acc: 0.9062 - val_loss: 1.3791 - val_acc: 0.6847\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4999 - acc: 0.9375 - val_loss: 1.2632 - val_acc: 0.7041\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3766 - acc: 0.9375 - val_loss: 1.1895 - val_acc: 0.7187\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3309 - acc: 0.9688 - val_loss: 1.1423 - val_acc: 0.7283\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3476 - acc: 0.9062 - val_loss: 1.1153 - val_acc: 0.7326\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2638 - acc: 1.0000 - val_loss: 1.1070 - val_acc: 0.7352\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2646 - acc: 1.0000 - val_loss: 1.1127 - val_acc: 0.7349\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "25 [supervised loss: 0.2646, acc: 100.00%] [unsupervised loss: 2.0392, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6795 - acc: 0.8438 - val_loss: 1.1271 - val_acc: 0.7298\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5865 - acc: 0.7812 - val_loss: 1.1229 - val_acc: 0.7300\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5506 - acc: 0.8125 - val_loss: 1.1224 - val_acc: 0.7327\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4794 - acc: 0.9062 - val_loss: 1.1252 - val_acc: 0.7321\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3865 - acc: 0.9688 - val_loss: 1.1293 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2936 - acc: 0.9688 - val_loss: 1.1351 - val_acc: 0.7331\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2867 - acc: 1.0000 - val_loss: 1.1441 - val_acc: 0.7305\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2837 - acc: 0.9688 - val_loss: 1.1508 - val_acc: 0.7291\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2558 - acc: 1.0000 - val_loss: 1.1581 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2902 - acc: 0.9688 - val_loss: 1.1649 - val_acc: 0.7248\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "26 [supervised loss: 0.2902, acc: 96.88%] [unsupervised loss: 0.9359, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6745 - acc: 0.8125 - val_loss: 1.2032 - val_acc: 0.7146\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4149 - acc: 0.9688 - val_loss: 1.2169 - val_acc: 0.7107\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5489 - acc: 0.8750 - val_loss: 1.2455 - val_acc: 0.7032\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5152 - acc: 0.8750 - val_loss: 1.2815 - val_acc: 0.6950\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4293 - acc: 0.9375 - val_loss: 1.3264 - val_acc: 0.6853\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3169 - acc: 0.9688 - val_loss: 1.3758 - val_acc: 0.6752\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2963 - acc: 1.0000 - val_loss: 1.4305 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2848 - acc: 1.0000 - val_loss: 1.4871 - val_acc: 0.6566\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2493 - acc: 1.0000 - val_loss: 1.5427 - val_acc: 0.6472\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2340 - acc: 1.0000 - val_loss: 1.5956 - val_acc: 0.6375\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "27 [supervised loss: 0.2340, acc: 100.00%] [unsupervised loss: 1.2431, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6002 - acc: 0.8750 - val_loss: 1.7686 - val_acc: 0.6101\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5903 - acc: 0.8125 - val_loss: 1.8067 - val_acc: 0.6047\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5797 - acc: 0.9062 - val_loss: 1.8392 - val_acc: 0.6011\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5154 - acc: 0.9375 - val_loss: 1.8624 - val_acc: 0.5976\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3739 - acc: 0.9375 - val_loss: 1.8720 - val_acc: 0.5978\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3189 - acc: 0.9375 - val_loss: 1.8810 - val_acc: 0.5969\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2837 - acc: 0.9688 - val_loss: 1.8803 - val_acc: 0.5972\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2751 - acc: 1.0000 - val_loss: 1.8762 - val_acc: 0.5978\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2392 - acc: 1.0000 - val_loss: 1.8689 - val_acc: 0.5976\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2359 - acc: 1.0000 - val_loss: 1.8579 - val_acc: 0.6021\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "28 [supervised loss: 0.2359, acc: 100.00%] [unsupervised loss: 1.6014, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5386 - acc: 0.9062 - val_loss: 1.9710 - val_acc: 0.5863\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5041 - acc: 0.8750 - val_loss: 1.9932 - val_acc: 0.5865\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5706 - acc: 0.8750 - val_loss: 2.0076 - val_acc: 0.5882\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4411 - acc: 0.9688 - val_loss: 2.0155 - val_acc: 0.5923\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3296 - acc: 1.0000 - val_loss: 2.0155 - val_acc: 0.5937\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3014 - acc: 1.0000 - val_loss: 2.0345 - val_acc: 0.5927\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2927 - acc: 1.0000 - val_loss: 2.0546 - val_acc: 0.5923\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2561 - acc: 1.0000 - val_loss: 2.0689 - val_acc: 0.5926\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2478 - acc: 1.0000 - val_loss: 2.0799 - val_acc: 0.5910\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2468 - acc: 1.0000 - val_loss: 2.0842 - val_acc: 0.5901\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "29 [supervised loss: 0.2468, acc: 100.00%] [unsupervised loss: 1.8560, acc: 53.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5570 - acc: 0.8750 - val_loss: 2.1329 - val_acc: 0.5825\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6259 - acc: 0.8438 - val_loss: 2.1223 - val_acc: 0.5849\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6919 - acc: 0.8438 - val_loss: 2.0594 - val_acc: 0.5920\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3874 - acc: 0.9688 - val_loss: 2.0002 - val_acc: 0.6001\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3824 - acc: 0.9375 - val_loss: 1.9479 - val_acc: 0.6060\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3409 - acc: 0.9688 - val_loss: 1.8917 - val_acc: 0.6151\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3159 - acc: 0.9688 - val_loss: 1.8460 - val_acc: 0.6210\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2571 - acc: 1.0000 - val_loss: 1.8043 - val_acc: 0.6248\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2720 - acc: 1.0000 - val_loss: 1.7660 - val_acc: 0.6292\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2421 - acc: 1.0000 - val_loss: 1.7293 - val_acc: 0.6318\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "30 [supervised loss: 0.2421, acc: 100.00%] [unsupervised loss: 1.9166, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5609 - acc: 0.9062 - val_loss: 1.7081 - val_acc: 0.6333\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8048 - acc: 0.7188 - val_loss: 1.6165 - val_acc: 0.6438\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6857 - acc: 0.9062 - val_loss: 1.4878 - val_acc: 0.6658\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4353 - acc: 0.9688 - val_loss: 1.3688 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4320 - acc: 0.9688 - val_loss: 1.2718 - val_acc: 0.7060\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3306 - acc: 0.9688 - val_loss: 1.2036 - val_acc: 0.7175\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3322 - acc: 1.0000 - val_loss: 1.1570 - val_acc: 0.7235\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2890 - acc: 1.0000 - val_loss: 1.1318 - val_acc: 0.7285\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2492 - acc: 1.0000 - val_loss: 1.1242 - val_acc: 0.7284\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2446 - acc: 1.0000 - val_loss: 1.1327 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "31 [supervised loss: 0.2446, acc: 100.00%] [unsupervised loss: 1.7883, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9339 - acc: 0.8750 - val_loss: 1.1961 - val_acc: 0.7114\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9021 - acc: 0.8438 - val_loss: 1.2342 - val_acc: 0.7037\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6647 - acc: 0.8125 - val_loss: 1.2670 - val_acc: 0.6982\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4548 - acc: 0.9062 - val_loss: 1.2979 - val_acc: 0.6939\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3741 - acc: 0.9062 - val_loss: 1.3233 - val_acc: 0.6895\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4131 - acc: 0.9375 - val_loss: 1.3454 - val_acc: 0.6862\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3265 - acc: 1.0000 - val_loss: 1.3646 - val_acc: 0.6806\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2585 - acc: 1.0000 - val_loss: 1.3779 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3076 - acc: 0.9688 - val_loss: 1.3895 - val_acc: 0.6746\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3430 - acc: 0.9688 - val_loss: 1.4019 - val_acc: 0.6709\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "32 [supervised loss: 0.3430, acc: 96.88%] [unsupervised loss: 0.7618, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2553 - acc: 0.6875 - val_loss: 1.4283 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0712 - acc: 0.8438 - val_loss: 1.4585 - val_acc: 0.6592\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7294 - acc: 0.7812 - val_loss: 1.5036 - val_acc: 0.6523\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5712 - acc: 0.8750 - val_loss: 1.5595 - val_acc: 0.6457\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3496 - acc: 0.9688 - val_loss: 1.6179 - val_acc: 0.6394\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3001 - acc: 1.0000 - val_loss: 1.6799 - val_acc: 0.6297\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2763 - acc: 1.0000 - val_loss: 1.7378 - val_acc: 0.6227\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3239 - acc: 0.9688 - val_loss: 1.7922 - val_acc: 0.6144\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2995 - acc: 0.9688 - val_loss: 1.8385 - val_acc: 0.6081\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2967 - acc: 0.9688 - val_loss: 1.8740 - val_acc: 0.6025\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "33 [supervised loss: 0.2967, acc: 96.88%] [unsupervised loss: 1.4789, acc: 68.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6175 - acc: 0.9375 - val_loss: 1.9641 - val_acc: 0.5883\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7321 - acc: 0.8438 - val_loss: 1.9400 - val_acc: 0.5884\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4500 - acc: 0.9375 - val_loss: 1.9044 - val_acc: 0.5912\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4819 - acc: 0.9062 - val_loss: 1.8684 - val_acc: 0.5940\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3680 - acc: 0.9688 - val_loss: 1.8254 - val_acc: 0.5983\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3477 - acc: 0.9688 - val_loss: 1.7883 - val_acc: 0.6018\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3369 - acc: 0.9375 - val_loss: 1.7491 - val_acc: 0.6066\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2695 - acc: 1.0000 - val_loss: 1.7137 - val_acc: 0.6124\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2370 - acc: 1.0000 - val_loss: 1.6830 - val_acc: 0.6164\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2342 - acc: 1.0000 - val_loss: 1.6561 - val_acc: 0.6204\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "34 [supervised loss: 0.2342, acc: 100.00%] [unsupervised loss: 1.7524, acc: 56.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6178 - acc: 0.8438 - val_loss: 1.6788 - val_acc: 0.6146\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7887 - acc: 0.8438 - val_loss: 1.6469 - val_acc: 0.6167\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3952 - acc: 0.9688 - val_loss: 1.6275 - val_acc: 0.6176\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4645 - acc: 0.9375 - val_loss: 1.6323 - val_acc: 0.6132\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3434 - acc: 0.9688 - val_loss: 1.6502 - val_acc: 0.6109\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2860 - acc: 1.0000 - val_loss: 1.6768 - val_acc: 0.6086\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2537 - acc: 1.0000 - val_loss: 1.7089 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2563 - acc: 1.0000 - val_loss: 1.7432 - val_acc: 0.5957\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2485 - acc: 1.0000 - val_loss: 1.7799 - val_acc: 0.5901\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2371 - acc: 1.0000 - val_loss: 1.8204 - val_acc: 0.5845\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "35 [supervised loss: 0.2371, acc: 100.00%] [unsupervised loss: 1.3952, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7405 - acc: 0.7812 - val_loss: 1.9832 - val_acc: 0.5603\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7494 - acc: 0.8125 - val_loss: 2.0725 - val_acc: 0.5485\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6010 - acc: 0.8438 - val_loss: 2.1817 - val_acc: 0.5324\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7304 - acc: 0.8438 - val_loss: 2.3063 - val_acc: 0.5185\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4814 - acc: 0.8750 - val_loss: 2.4332 - val_acc: 0.5041\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3289 - acc: 1.0000 - val_loss: 2.5632 - val_acc: 0.4878\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3404 - acc: 0.9688 - val_loss: 2.6954 - val_acc: 0.4723\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2977 - acc: 1.0000 - val_loss: 2.8292 - val_acc: 0.4569\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2541 - acc: 1.0000 - val_loss: 2.9506 - val_acc: 0.4432\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2517 - acc: 1.0000 - val_loss: 3.0502 - val_acc: 0.4346\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "36 [supervised loss: 0.2517, acc: 100.00%] [unsupervised loss: 2.5246, acc: 56.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5836 - acc: 0.9062 - val_loss: 3.4606 - val_acc: 0.3968\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5245 - acc: 0.8438 - val_loss: 3.4745 - val_acc: 0.3985\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4471 - acc: 0.9375 - val_loss: 3.3990 - val_acc: 0.4079\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6031 - acc: 0.9375 - val_loss: 3.2642 - val_acc: 0.4260\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3724 - acc: 0.9375 - val_loss: 3.0794 - val_acc: 0.4492\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2931 - acc: 0.9688 - val_loss: 2.9557 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2947 - acc: 0.9688 - val_loss: 2.8413 - val_acc: 0.4806\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2739 - acc: 0.9688 - val_loss: 2.7294 - val_acc: 0.4960\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2575 - acc: 1.0000 - val_loss: 2.6341 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2254 - acc: 1.0000 - val_loss: 2.5625 - val_acc: 0.5231\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "37 [supervised loss: 0.2254, acc: 100.00%] [unsupervised loss: 2.9869, acc: 43.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0633 - acc: 0.6875 - val_loss: 2.6579 - val_acc: 0.5177\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8126 - acc: 0.7812 - val_loss: 2.5162 - val_acc: 0.5390\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8259 - acc: 0.7500 - val_loss: 2.3084 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7560 - acc: 0.8125 - val_loss: 2.1634 - val_acc: 0.5929\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5374 - acc: 0.9375 - val_loss: 2.0724 - val_acc: 0.6025\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5191 - acc: 0.9062 - val_loss: 2.0397 - val_acc: 0.6097\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3929 - acc: 0.9375 - val_loss: 2.0670 - val_acc: 0.6034\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3096 - acc: 1.0000 - val_loss: 2.1259 - val_acc: 0.5946\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2962 - acc: 0.9688 - val_loss: 2.2027 - val_acc: 0.5850\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2704 - acc: 1.0000 - val_loss: 2.2869 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "38 [supervised loss: 0.2704, acc: 100.00%] [unsupervised loss: 1.6722, acc: 53.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7534 - acc: 0.7188 - val_loss: 2.6170 - val_acc: 0.5289\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7550 - acc: 0.7812 - val_loss: 2.7130 - val_acc: 0.5165\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5069 - acc: 0.9062 - val_loss: 2.7603 - val_acc: 0.5107\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5684 - acc: 0.8750 - val_loss: 2.7774 - val_acc: 0.5089\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4021 - acc: 0.9062 - val_loss: 2.8130 - val_acc: 0.5068\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3160 - acc: 0.9688 - val_loss: 2.8714 - val_acc: 0.5015\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3457 - acc: 0.9062 - val_loss: 2.9548 - val_acc: 0.4960\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2716 - acc: 1.0000 - val_loss: 3.0599 - val_acc: 0.4875\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2447 - acc: 1.0000 - val_loss: 3.1724 - val_acc: 0.4808\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 3.2779 - val_acc: 0.4746\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "39 [supervised loss: 0.2705, acc: 100.00%] [unsupervised loss: 2.0836, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9547 - acc: 0.8750 - val_loss: 3.4970 - val_acc: 0.4547\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8526 - acc: 0.8438 - val_loss: 3.4045 - val_acc: 0.4598\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8452 - acc: 0.8750 - val_loss: 3.1649 - val_acc: 0.4789\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6830 - acc: 0.9062 - val_loss: 2.8412 - val_acc: 0.5049\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4323 - acc: 0.9688 - val_loss: 2.5132 - val_acc: 0.5319\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3923 - acc: 0.9375 - val_loss: 2.2482 - val_acc: 0.5562\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3324 - acc: 0.9375 - val_loss: 2.0786 - val_acc: 0.5734\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3201 - acc: 0.9688 - val_loss: 1.9777 - val_acc: 0.5834\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2484 - acc: 1.0000 - val_loss: 1.9369 - val_acc: 0.5834\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2449 - acc: 1.0000 - val_loss: 1.9250 - val_acc: 0.5808\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "40 [supervised loss: 0.2449, acc: 100.00%] [unsupervised loss: 3.7949, acc: 40.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6666 - acc: 0.7812 - val_loss: 2.1702 - val_acc: 0.5504\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6215 - acc: 0.8438 - val_loss: 2.2214 - val_acc: 0.5399\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5410 - acc: 0.8750 - val_loss: 2.2337 - val_acc: 0.5343\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4642 - acc: 0.9062 - val_loss: 2.2160 - val_acc: 0.5326\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3546 - acc: 0.9688 - val_loss: 2.1796 - val_acc: 0.5348\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3891 - acc: 0.9688 - val_loss: 2.1208 - val_acc: 0.5391\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2891 - acc: 0.9688 - val_loss: 2.0581 - val_acc: 0.5463\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2755 - acc: 1.0000 - val_loss: 2.0089 - val_acc: 0.5495\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2717 - acc: 1.0000 - val_loss: 1.9673 - val_acc: 0.5565\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2584 - acc: 1.0000 - val_loss: 1.9328 - val_acc: 0.5581\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "41 [supervised loss: 0.2584, acc: 100.00%] [unsupervised loss: 2.0592, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0488 - acc: 0.8750 - val_loss: 1.8996 - val_acc: 0.5614\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8118 - acc: 0.8438 - val_loss: 1.8004 - val_acc: 0.5754\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7723 - acc: 0.8438 - val_loss: 1.7003 - val_acc: 0.5908\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8421 - acc: 0.9062 - val_loss: 1.6042 - val_acc: 0.6064\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5774 - acc: 0.9062 - val_loss: 1.5099 - val_acc: 0.6216\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4571 - acc: 0.8438 - val_loss: 1.4234 - val_acc: 0.6385\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4388 - acc: 0.9375 - val_loss: 1.3563 - val_acc: 0.6509\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3077 - acc: 1.0000 - val_loss: 1.3067 - val_acc: 0.6622\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3379 - acc: 0.9688 - val_loss: 1.2692 - val_acc: 0.6706\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2640 - acc: 1.0000 - val_loss: 1.2404 - val_acc: 0.6777\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "42 [supervised loss: 0.2640, acc: 100.00%] [unsupervised loss: 1.6064, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8114 - acc: 0.7500 - val_loss: 1.2544 - val_acc: 0.6716\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7558 - acc: 0.8438 - val_loss: 1.2201 - val_acc: 0.6780\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6294 - acc: 0.8750 - val_loss: 1.1859 - val_acc: 0.6845\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5632 - acc: 0.9062 - val_loss: 1.1548 - val_acc: 0.6929\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6211 - acc: 0.8750 - val_loss: 1.1316 - val_acc: 0.6996\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6181 - acc: 0.8750 - val_loss: 1.1131 - val_acc: 0.7046\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3400 - acc: 0.9688 - val_loss: 1.1049 - val_acc: 0.7065\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3092 - acc: 1.0000 - val_loss: 1.1055 - val_acc: 0.7075\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3054 - acc: 1.0000 - val_loss: 1.1119 - val_acc: 0.7075\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2931 - acc: 0.9688 - val_loss: 1.1229 - val_acc: 0.7049\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "43 [supervised loss: 0.2931, acc: 96.88%] [unsupervised loss: 0.9050, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7595 - acc: 0.8125 - val_loss: 1.1345 - val_acc: 0.7018\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6994 - acc: 0.8125 - val_loss: 1.1100 - val_acc: 0.7093\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7157 - acc: 0.8438 - val_loss: 1.0836 - val_acc: 0.7176\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5599 - acc: 0.8750 - val_loss: 1.0646 - val_acc: 0.7236\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5777 - acc: 0.8125 - val_loss: 1.0533 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3704 - acc: 0.9375 - val_loss: 1.0516 - val_acc: 0.7267\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3482 - acc: 1.0000 - val_loss: 1.0533 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2894 - acc: 1.0000 - val_loss: 1.0567 - val_acc: 0.7254\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2809 - acc: 1.0000 - val_loss: 1.0598 - val_acc: 0.7256\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2684 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.7251\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "44 [supervised loss: 0.2684, acc: 100.00%] [unsupervised loss: 1.3506, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8768 - acc: 0.7812 - val_loss: 1.0454 - val_acc: 0.7312\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7326 - acc: 0.8750 - val_loss: 1.0354 - val_acc: 0.7329\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7710 - acc: 0.7500 - val_loss: 1.0292 - val_acc: 0.7362\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6317 - acc: 0.8438 - val_loss: 1.0284 - val_acc: 0.7355\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4582 - acc: 0.9688 - val_loss: 1.0402 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3456 - acc: 0.9688 - val_loss: 1.0645 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3697 - acc: 0.9375 - val_loss: 1.1078 - val_acc: 0.7153\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3325 - acc: 0.9688 - val_loss: 1.1700 - val_acc: 0.7003\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2744 - acc: 1.0000 - val_loss: 1.2476 - val_acc: 0.6812\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2615 - acc: 1.0000 - val_loss: 1.3360 - val_acc: 0.6656\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "45 [supervised loss: 0.2615, acc: 100.00%] [unsupervised loss: 0.9194, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7870 - acc: 0.7812 - val_loss: 1.5268 - val_acc: 0.6311\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6165 - acc: 0.9375 - val_loss: 1.5834 - val_acc: 0.6206\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6145 - acc: 0.8750 - val_loss: 1.6375 - val_acc: 0.6150\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3741 - acc: 0.9688 - val_loss: 1.6898 - val_acc: 0.6092\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3677 - acc: 1.0000 - val_loss: 1.7414 - val_acc: 0.6019\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3403 - acc: 0.9375 - val_loss: 1.7935 - val_acc: 0.5952\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3125 - acc: 1.0000 - val_loss: 1.8382 - val_acc: 0.5893\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2936 - acc: 0.9688 - val_loss: 1.8719 - val_acc: 0.5837\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2738 - acc: 1.0000 - val_loss: 1.9025 - val_acc: 0.5808\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2392 - acc: 1.0000 - val_loss: 1.9319 - val_acc: 0.5775\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "46 [supervised loss: 0.2392, acc: 100.00%] [unsupervised loss: 1.5033, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8248 - acc: 0.7812 - val_loss: 2.0975 - val_acc: 0.5542\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5692 - acc: 0.8438 - val_loss: 2.1428 - val_acc: 0.5494\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6661 - acc: 0.9062 - val_loss: 2.1018 - val_acc: 0.5571\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5473 - acc: 0.8750 - val_loss: 2.0368 - val_acc: 0.5686\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4252 - acc: 0.9375 - val_loss: 1.9823 - val_acc: 0.5786\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3410 - acc: 0.9688 - val_loss: 1.9412 - val_acc: 0.5863\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2819 - acc: 0.9688 - val_loss: 1.9049 - val_acc: 0.5945\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2768 - acc: 0.9688 - val_loss: 1.8775 - val_acc: 0.6006\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2503 - acc: 1.0000 - val_loss: 1.8590 - val_acc: 0.6029\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2680 - acc: 1.0000 - val_loss: 1.8482 - val_acc: 0.6039\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "47 [supervised loss: 0.2680, acc: 100.00%] [unsupervised loss: 1.7593, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0117 - acc: 0.7500 - val_loss: 1.9936 - val_acc: 0.5823\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9416 - acc: 0.7812 - val_loss: 2.0536 - val_acc: 0.5770\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9296 - acc: 0.8125 - val_loss: 2.1070 - val_acc: 0.5716\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6724 - acc: 0.7812 - val_loss: 2.1635 - val_acc: 0.5671\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6204 - acc: 0.8438 - val_loss: 2.2230 - val_acc: 0.5603\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4316 - acc: 0.9375 - val_loss: 2.2632 - val_acc: 0.5572\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3486 - acc: 1.0000 - val_loss: 2.2827 - val_acc: 0.5545\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3117 - acc: 0.9688 - val_loss: 2.2862 - val_acc: 0.5553\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3144 - acc: 1.0000 - val_loss: 2.2969 - val_acc: 0.5542\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2822 - acc: 0.9688 - val_loss: 2.3154 - val_acc: 0.5546\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "48 [supervised loss: 0.2822, acc: 96.88%] [unsupervised loss: 1.2567, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5521 - acc: 0.8750 - val_loss: 2.3764 - val_acc: 0.5519\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9336 - acc: 0.7500 - val_loss: 2.3967 - val_acc: 0.5511\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6166 - acc: 0.9062 - val_loss: 2.3935 - val_acc: 0.5538\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5469 - acc: 0.8750 - val_loss: 2.3629 - val_acc: 0.5585\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5463 - acc: 0.9375 - val_loss: 2.3058 - val_acc: 0.5627\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3304 - acc: 0.9688 - val_loss: 2.2504 - val_acc: 0.5668\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3239 - acc: 1.0000 - val_loss: 2.1973 - val_acc: 0.5705\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3414 - acc: 0.9688 - val_loss: 2.1525 - val_acc: 0.5761\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3266 - acc: 0.9688 - val_loss: 2.1083 - val_acc: 0.5788\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2511 - acc: 1.0000 - val_loss: 2.0669 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "49 [supervised loss: 0.2511, acc: 100.00%] [unsupervised loss: 1.5377, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5863 - acc: 0.8438 - val_loss: 2.1497 - val_acc: 0.5727\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6399 - acc: 0.8438 - val_loss: 2.1449 - val_acc: 0.5767\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5304 - acc: 0.8438 - val_loss: 2.1395 - val_acc: 0.5834\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4831 - acc: 0.9062 - val_loss: 2.1293 - val_acc: 0.5889\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3888 - acc: 0.9688 - val_loss: 2.1271 - val_acc: 0.5910\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3068 - acc: 1.0000 - val_loss: 2.1347 - val_acc: 0.5918\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3390 - acc: 1.0000 - val_loss: 2.1450 - val_acc: 0.5959\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2855 - acc: 0.9688 - val_loss: 2.1625 - val_acc: 0.5962\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2798 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.5950\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2456 - acc: 1.0000 - val_loss: 2.2033 - val_acc: 0.5952\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "50 [supervised loss: 0.2456, acc: 100.00%] [unsupervised loss: 1.5870, acc: 68.75%]\n",
            "Training time: 1525.8914s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbIxExOyCxJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "7d8ec5d7-197f-4668-c5ea-939688bfa2f4"
      },
      "source": [
        "plot_pseudo_supervised_losses = np.array(losses_pseudo_labeled)\n",
        "plot_pseudo_unsupervised_losses = np.array(losses_pseudo_unlabeled)\n",
        "plot_pseudo_all_losses = np.array(losses_pseudo)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_all_losses, label=\"All loss\", color='black')\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_supervised_losses, label=\"Supervised loss\", color='tab:blue', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, plot_pseudo_unsupervised_losses, label=\"Unsupervised loss\", color='tab:green', linestyle='dashed')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Pseudo Label's Supervised and Unsupervised Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29c9c985c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3UAAAFPCAYAAADqcOfCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-4ZGValCxGj",
        "colab_type": "code",
        "outputId": "475a1cbb-8a96-4869-9d7e-fb4a3a5dd660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "# tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-8000.h5\")\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-30.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 25s 505us/step\n",
            "Training Accuracy: 75.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jD1xtbCw7H",
        "colab_type": "code",
        "outputId": "a5fcf47f-880b-47d5-b261-abad697b5333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "# tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-6000.h5\")\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-30.h5\", by_name=False)\n",
        "\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 459us/step\n",
            "Test Accuracy: 74.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeX3mD5qC5TQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d8ec0a4-8673-4dbb-da2b-4704024ad707"
      },
      "source": [
        "accs = []\n",
        "# tx = [x for x in range(1,31,1)]\n",
        "tx = [x for x in range(1, len(iteration_checkpoints)+1, 1)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  # tmodel = load_model(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\"+ str(e) +\".h5\")\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/pseudo-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"Pseudo Label's accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 393us/step\n",
            "10000/10000 [==============================] - 4s 383us/step\n",
            "10000/10000 [==============================] - 4s 383us/step\n",
            "10000/10000 [==============================] - 4s 392us/step\n",
            "10000/10000 [==============================] - 4s 389us/step\n",
            "10000/10000 [==============================] - 4s 388us/step\n",
            "10000/10000 [==============================] - 4s 384us/step\n",
            "10000/10000 [==============================] - 4s 381us/step\n",
            "10000/10000 [==============================] - 4s 379us/step\n",
            "10000/10000 [==============================] - 4s 377us/step\n",
            "10000/10000 [==============================] - 4s 377us/step\n",
            "10000/10000 [==============================] - 4s 380us/step\n",
            "10000/10000 [==============================] - 4s 382us/step\n",
            "10000/10000 [==============================] - 4s 375us/step\n",
            "10000/10000 [==============================] - 4s 376us/step\n",
            "10000/10000 [==============================] - 4s 373us/step\n",
            "10000/10000 [==============================] - 4s 375us/step\n",
            "10000/10000 [==============================] - 4s 374us/step\n",
            "10000/10000 [==============================] - 4s 374us/step\n",
            "10000/10000 [==============================] - 4s 370us/step\n",
            "10000/10000 [==============================] - 4s 375us/step\n",
            "10000/10000 [==============================] - 4s 375us/step\n",
            "10000/10000 [==============================] - 4s 373us/step\n",
            "10000/10000 [==============================] - 4s 376us/step\n",
            "10000/10000 [==============================] - 4s 375us/step\n",
            "10000/10000 [==============================] - 4s 374us/step\n",
            "10000/10000 [==============================] - 4s 374us/step\n",
            "10000/10000 [==============================] - 4s 374us/step\n",
            "10000/10000 [==============================] - 4s 383us/step\n",
            "10000/10000 [==============================] - 4s 388us/step\n",
            "10000/10000 [==============================] - 4s 391us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 395us/step\n",
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 401us/step\n",
            "10000/10000 [==============================] - 4s 406us/step\n",
            "10000/10000 [==============================] - 4s 396us/step\n",
            "10000/10000 [==============================] - 4s 383us/step\n",
            "10000/10000 [==============================] - 4s 379us/step\n",
            "10000/10000 [==============================] - 4s 378us/step\n",
            "10000/10000 [==============================] - 4s 374us/step\n",
            "10000/10000 [==============================] - 4s 373us/step\n",
            "10000/10000 [==============================] - 4s 368us/step\n",
            "10000/10000 [==============================] - 4s 369us/step\n",
            "10000/10000 [==============================] - 4s 373us/step\n",
            "10000/10000 [==============================] - 4s 373us/step\n",
            "10000/10000 [==============================] - 4s 378us/step\n",
            "10000/10000 [==============================] - 4s 379us/step\n",
            "0.8273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29cb550668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFPCAYAAAAfjmxyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n44QJniRJkiSNEyZ4kiRJkjRO/H9GdOpywORJPgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4PM4STC5Q3",
        "colab_type": "code",
        "outputId": "1c17555a-8b9c-4221-8c66-7d639af288f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(max(accs[8:]))\n",
        "# for acc in accs:\n",
        "#   print(acc)\n",
        "pseudo_accs = accs\n",
        "print(pseudo_accs)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7316\n",
            "[0.8273, 0.8029, 0.8252, 0.7924, 0.7832, 0.8101, 0.8002, 0.7918, 0.7578, 0.7872, 0.6125, 0.6252, 0.6143, 0.7034, 0.7118, 0.6801, 0.5732, 0.6383, 0.629, 0.565, 0.6735, 0.6313, 0.703, 0.6141, 0.7316, 0.7199, 0.6201, 0.5918, 0.5861, 0.629, 0.7205, 0.6671, 0.593, 0.6148, 0.5729, 0.4076, 0.5092, 0.5492, 0.4577, 0.5631, 0.5576, 0.67, 0.701, 0.7258, 0.6478, 0.5608, 0.5918, 0.5512, 0.5747, 0.5967]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9jhBOIcqT-h",
        "colab_type": "text"
      },
      "source": [
        "# Mean Teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxPsJI6lMwsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "teacher = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "student.compile(loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'], \n",
        "                optimizer=Adam())\n",
        "teacher.compile(loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'], \n",
        "                optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKFkigJMwpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_teacher_train(iterations, batch_size, save_interval, alpha, iter_epochs):\n",
        "\n",
        "    x_test, y_test = dataset.test_set()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # -------------------------\n",
        "        #  Train the model\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs_labeled, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # Get unlabeled examples\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "        # Train on labeled examples\n",
        "        # loss_labeled_classification, acc_labeled_classification = student.train_on_batch(imgs_labeled, labels)\n",
        "        datagen.fit(imgs_labeled)\n",
        "        student.fit_generator(datagen.flow(imgs_labeled, labels, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=iter_epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "        loss_labeled_classification, acc_labeled_classification = history.losses[-1], history.accs[-1]\n",
        "        pred_teacher_labeled = teacher.predict(imgs_labeled)\n",
        "        loss_labeled_consistency, acc_labeled_consistency = student.train_on_batch(imgs_labeled, pred_teacher_labeled)\n",
        "\n",
        "        # Train on unlabeled examples\n",
        "        pred_teacher_unlabeled = teacher.predict(imgs_unlabeled)\n",
        "        loss_unlabeled_consistency, acc_unlabeled_consistency = student.train_on_batch(imgs_unlabeled, pred_teacher_unlabeled)\n",
        "\n",
        "        # Update teacher model\n",
        "        teacher_weights_this = teacher.get_weights()\n",
        "        student_weights_this = student.get_weights()\n",
        "        for i in range(len(teacher_weights_this)):\n",
        "          teacher_weights_this[i] = alpha * teacher_weights_this[i] + (1-alpha) * student_weights_this[i]\n",
        "        # teacher_weights_this = alpha * teacher_weights_this + (1-alpha) * student_weights_this\n",
        "        teacher_weights_last = teacher_weights_this\n",
        "        teacher.set_weights(teacher_weights_this)\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "          # Save losses\n",
        "          supervised_losses.append(loss_labeled_classification)\n",
        "          unsupervised_losses.append(loss_labeled_consistency + loss_unlabeled_consistency)\n",
        "          labeled_consistency_costs.append(loss_labeled_consistency)\n",
        "          unlabeled_consistency_costs.append(loss_unlabeled_consistency)\n",
        "          accs_supervised.append(acc_labeled_classification)\n",
        "          accs_unsupervised.append((acc_labeled_consistency + acc_unlabeled_consistency)/2.0)\n",
        "          accs_labeled_consistency.append(acc_labeled_consistency)\n",
        "          accs_unlabeled_consistency.append(acc_unlabeled_consistency)\n",
        "\n",
        "          iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "          # Output training progress\n",
        "          print(\n",
        "              \"%d [supervised loss: %.4f, acc: %.2f%%] [unsupervised loss: %.4f, acc: %.2f%%] [labeled consistency loss: %.4f, acc:acc: %.2f%%] [unlabeled consistency loss: %.4f, acc: %.2f%%]\"\n",
        "              % (iteration + 1, loss_labeled_classification, 100 * acc_labeled_classification, \n",
        "                 loss_labeled_consistency + loss_unlabeled_consistency, 100 * ((acc_labeled_consistency + acc_unlabeled_consistency)/2.0), \n",
        "                 loss_labeled_consistency, 100 * acc_labeled_consistency, \n",
        "                  loss_unlabeled_consistency, 100 * acc_unlabeled_consistency))\n",
        "          \n",
        "          student.save(\"./models/models-label-\" + str(num_labeled) + \"/student-\" + str(iteration+1) + \".h5\")\n",
        "          teacher.save(\"./models/models-label-\" + str(num_labeled) + \"/teacher-\" + str(iteration+1) + \".h5\")\n",
        "          file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_supervised_losses.json\"\n",
        "          file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_unsupervised_losses.json\"\n",
        "          file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_labeled_consistency_costs.json\"\n",
        "          file4 = \"./losses/losses-label-\" + str(num_labeled) + \"/mt_unlabeled_consistency_costs.json\"\n",
        "          with open(file1, 'w') as json_file:\n",
        "                json.dump(str(supervised_losses), json_file)\n",
        "          with open(file2, 'w') as json_file:\n",
        "                json.dump(str(unsupervised_losses), json_file)\n",
        "          with open(file3, 'w') as json_file:\n",
        "                json.dump(str(labeled_consistency_costs), json_file)\n",
        "          with open(file4, 'w') as json_file:\n",
        "                json.dump(str(unlabeled_consistency_costs), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS6N6-dRMwnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22775e2e-8dbd-4310-914a-7d71846d7b21"
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 50 # 30\n",
        "batch_size = 32\n",
        "save_interval = 1\n",
        "alpha = 0.5\n",
        "iter_epochs = 10\n",
        "\n",
        "supervised_losses = [] # classification cost\n",
        "unsupervised_losses = [] # consistency cost\n",
        "labeled_consistency_costs = []\n",
        "unlabeled_consistency_costs = []\n",
        "accs_supervised = []\n",
        "accs_unsupervised = []\n",
        "accs_labeled_consistency = []\n",
        "accs_unlabeled_consistency = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "discriminator_supervised.trainable = True\n",
        "student = load_model(\"./models/cifar10_model.035.h5\")\n",
        "\n",
        "starttime = time.clock()\n",
        "\n",
        "# Train the mean teacher for the specified number of iterations\n",
        "mean_teacher_train(iterations, batch_size, save_interval, alpha, iter_epochs)\n",
        "\n",
        "endtime = time.clock()\n",
        "print(\"Training time: %.4fs\" % (endtime - starttime))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 17s 17s/step - loss: 0.5169 - acc: 0.9062 - val_loss: 0.6074 - val_acc: 0.8653\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5301 - acc: 0.8750 - val_loss: 0.5981 - val_acc: 0.8690\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4095 - acc: 0.9062 - val_loss: 0.5970 - val_acc: 0.8704\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5726 - acc: 0.9062 - val_loss: 0.5970 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3376 - acc: 0.9688 - val_loss: 0.5985 - val_acc: 0.8704\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2961 - acc: 0.9688 - val_loss: 0.6010 - val_acc: 0.8700\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3007 - acc: 1.0000 - val_loss: 0.6096 - val_acc: 0.8677\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2754 - acc: 1.0000 - val_loss: 0.6176 - val_acc: 0.8643\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2653 - acc: 1.0000 - val_loss: 0.6248 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2585 - acc: 1.0000 - val_loss: 0.6349 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "1 [supervised loss: 0.2585, acc: 100.00%] [unsupervised loss: 19.5027, acc: 9.38%] [labeled consistency loss: 10.0486, acc:acc: 9.38%] [unlabeled consistency loss: 9.4541, acc: 9.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.5474 - acc: 0.8750 - val_loss: 0.6741 - val_acc: 0.8456\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6548 - acc: 0.8125 - val_loss: 0.7011 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.4668 - acc: 0.9375 - val_loss: 0.7339 - val_acc: 0.8271\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5352 - acc: 0.8750 - val_loss: 0.7619 - val_acc: 0.8172\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5260 - acc: 0.8750 - val_loss: 0.7848 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4471 - acc: 0.9062 - val_loss: 0.7989 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4491 - acc: 0.9062 - val_loss: 0.8133 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4070 - acc: 0.9688 - val_loss: 0.8228 - val_acc: 0.7971\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4117 - acc: 0.9688 - val_loss: 0.8242 - val_acc: 0.7967\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2842 - acc: 1.0000 - val_loss: 0.8228 - val_acc: 0.7988\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "2 [supervised loss: 0.2842, acc: 100.00%] [unsupervised loss: 15.2951, acc: 17.19%] [labeled consistency loss: 7.9226, acc:acc: 12.50%] [unlabeled consistency loss: 7.3725, acc: 21.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7744 - acc: 0.8125 - val_loss: 0.8996 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6092 - acc: 0.8750 - val_loss: 0.9228 - val_acc: 0.7664\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8420 - acc: 0.8438 - val_loss: 0.9464 - val_acc: 0.7614\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6729 - acc: 0.8750 - val_loss: 0.9716 - val_acc: 0.7564\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5056 - acc: 0.9062 - val_loss: 1.0011 - val_acc: 0.7485\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4114 - acc: 0.9375 - val_loss: 1.0355 - val_acc: 0.7417\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3423 - acc: 1.0000 - val_loss: 1.0784 - val_acc: 0.7299\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3501 - acc: 1.0000 - val_loss: 1.1247 - val_acc: 0.7157\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3067 - acc: 1.0000 - val_loss: 1.1733 - val_acc: 0.7027\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2927 - acc: 1.0000 - val_loss: 1.2216 - val_acc: 0.6889\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "3 [supervised loss: 0.2927, acc: 100.00%] [unsupervised loss: 12.2142, acc: 18.75%] [labeled consistency loss: 6.9228, acc:acc: 12.50%] [unlabeled consistency loss: 5.2913, acc: 25.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9033 - acc: 0.7188 - val_loss: 1.4345 - val_acc: 0.6362\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0086 - acc: 0.7188 - val_loss: 1.4752 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8966 - acc: 0.6875 - val_loss: 1.4974 - val_acc: 0.6249\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9634 - acc: 0.7500 - val_loss: 1.5111 - val_acc: 0.6265\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6735 - acc: 0.9062 - val_loss: 1.5249 - val_acc: 0.6267\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5966 - acc: 0.8750 - val_loss: 1.5478 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5369 - acc: 0.9375 - val_loss: 1.5735 - val_acc: 0.6244\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4290 - acc: 0.9688 - val_loss: 1.6067 - val_acc: 0.6183\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4853 - acc: 0.9062 - val_loss: 1.6341 - val_acc: 0.6154\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3710 - acc: 0.9688 - val_loss: 1.6562 - val_acc: 0.6096\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "4 [supervised loss: 0.3710, acc: 96.88%] [unsupervised loss: 5.2323, acc: 64.06%] [labeled consistency loss: 3.1120, acc:acc: 56.25%] [unlabeled consistency loss: 2.1203, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.2062 - acc: 0.6562 - val_loss: 1.7469 - val_acc: 0.5937\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5413 - acc: 0.6875 - val_loss: 1.5899 - val_acc: 0.6191\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0768 - acc: 0.6562 - val_loss: 1.4089 - val_acc: 0.6523\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8192 - acc: 0.8125 - val_loss: 1.2791 - val_acc: 0.6758\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9037 - acc: 0.8125 - val_loss: 1.2107 - val_acc: 0.6886\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6598 - acc: 0.9062 - val_loss: 1.2057 - val_acc: 0.6871\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5443 - acc: 0.9062 - val_loss: 1.2539 - val_acc: 0.6794\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5303 - acc: 0.9688 - val_loss: 1.3431 - val_acc: 0.6615\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4729 - acc: 0.9062 - val_loss: 1.4547 - val_acc: 0.6412\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4683 - acc: 0.9375 - val_loss: 1.5772 - val_acc: 0.6202\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "5 [supervised loss: 0.4683, acc: 93.75%] [unsupervised loss: 4.8727, acc: 67.19%] [labeled consistency loss: 3.0058, acc:acc: 68.75%] [unlabeled consistency loss: 1.8670, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9616 - acc: 0.7500 - val_loss: 1.5763 - val_acc: 0.6229\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8540 - acc: 0.8125 - val_loss: 1.5198 - val_acc: 0.6314\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7424 - acc: 0.8750 - val_loss: 1.4459 - val_acc: 0.6452\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7151 - acc: 0.8750 - val_loss: 1.3663 - val_acc: 0.6599\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6401 - acc: 0.8750 - val_loss: 1.2894 - val_acc: 0.6763\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5488 - acc: 0.9688 - val_loss: 1.2156 - val_acc: 0.6943\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5534 - acc: 0.9375 - val_loss: 1.1476 - val_acc: 0.7132\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4277 - acc: 0.9688 - val_loss: 1.0936 - val_acc: 0.7268\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3976 - acc: 0.9688 - val_loss: 1.0512 - val_acc: 0.7355\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4334 - acc: 0.9688 - val_loss: 1.0186 - val_acc: 0.7446\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "6 [supervised loss: 0.4334, acc: 96.88%] [unsupervised loss: 2.7133, acc: 84.38%] [labeled consistency loss: 1.5238, acc:acc: 84.38%] [unlabeled consistency loss: 1.1894, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8391 - acc: 0.8125 - val_loss: 0.9898 - val_acc: 0.7522\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8025 - acc: 0.7812 - val_loss: 0.9857 - val_acc: 0.7529\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6332 - acc: 0.9375 - val_loss: 0.9808 - val_acc: 0.7526\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6160 - acc: 0.9062 - val_loss: 0.9833 - val_acc: 0.7512\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5406 - acc: 0.9375 - val_loss: 0.9903 - val_acc: 0.7485\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5282 - acc: 0.9062 - val_loss: 1.0035 - val_acc: 0.7447\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4679 - acc: 0.9062 - val_loss: 1.0256 - val_acc: 0.7399\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3792 - acc: 1.0000 - val_loss: 1.0508 - val_acc: 0.7353\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3831 - acc: 1.0000 - val_loss: 1.0781 - val_acc: 0.7275\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3335 - acc: 1.0000 - val_loss: 1.1062 - val_acc: 0.7209\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "7 [supervised loss: 0.3335, acc: 100.00%] [unsupervised loss: 2.5447, acc: 81.25%] [labeled consistency loss: 1.1592, acc:acc: 84.38%] [unlabeled consistency loss: 1.3855, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7441 - acc: 0.7812 - val_loss: 1.1592 - val_acc: 0.7066\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6194 - acc: 0.9062 - val_loss: 1.1686 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6719 - acc: 0.8750 - val_loss: 1.1644 - val_acc: 0.7030\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5637 - acc: 0.8438 - val_loss: 1.1458 - val_acc: 0.7067\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5333 - acc: 0.9062 - val_loss: 1.1283 - val_acc: 0.7110\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5113 - acc: 0.9375 - val_loss: 1.0974 - val_acc: 0.7186\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4011 - acc: 0.9688 - val_loss: 1.0684 - val_acc: 0.7247\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3290 - acc: 0.9688 - val_loss: 1.0404 - val_acc: 0.7335\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3290 - acc: 1.0000 - val_loss: 1.0107 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2772 - acc: 1.0000 - val_loss: 0.9881 - val_acc: 0.7447\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "8 [supervised loss: 0.2772, acc: 100.00%] [unsupervised loss: 2.5590, acc: 79.69%] [labeled consistency loss: 1.3174, acc:acc: 84.38%] [unlabeled consistency loss: 1.2415, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6123 - acc: 0.8438 - val_loss: 0.9739 - val_acc: 0.7494\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5993 - acc: 0.8438 - val_loss: 0.9666 - val_acc: 0.7518\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4362 - acc: 0.9688 - val_loss: 0.9586 - val_acc: 0.7545\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3902 - acc: 0.9688 - val_loss: 0.9515 - val_acc: 0.7569\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3817 - acc: 0.9688 - val_loss: 0.9484 - val_acc: 0.7570\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3962 - acc: 1.0000 - val_loss: 0.9499 - val_acc: 0.7581\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3209 - acc: 0.9688 - val_loss: 0.9559 - val_acc: 0.7573\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3040 - acc: 1.0000 - val_loss: 0.9670 - val_acc: 0.7520\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2953 - acc: 1.0000 - val_loss: 0.9824 - val_acc: 0.7477\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3588 - acc: 0.9688 - val_loss: 0.9988 - val_acc: 0.7423\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "9 [supervised loss: 0.3588, acc: 96.88%] [unsupervised loss: 2.5591, acc: 85.94%] [labeled consistency loss: 1.4233, acc:acc: 90.62%] [unlabeled consistency loss: 1.1358, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7544 - acc: 0.7812 - val_loss: 0.9984 - val_acc: 0.7426\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7582 - acc: 0.8438 - val_loss: 0.9975 - val_acc: 0.7424\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5702 - acc: 0.9375 - val_loss: 0.9904 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6061 - acc: 0.8438 - val_loss: 0.9827 - val_acc: 0.7478\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5823 - acc: 0.8438 - val_loss: 0.9734 - val_acc: 0.7513\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5616 - acc: 0.8750 - val_loss: 0.9682 - val_acc: 0.7522\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3998 - acc: 0.9688 - val_loss: 0.9658 - val_acc: 0.7536\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3857 - acc: 0.9375 - val_loss: 0.9642 - val_acc: 0.7532\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3095 - acc: 1.0000 - val_loss: 0.9658 - val_acc: 0.7532\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3093 - acc: 1.0000 - val_loss: 0.9673 - val_acc: 0.7530\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "10 [supervised loss: 0.3093, acc: 100.00%] [unsupervised loss: 2.7651, acc: 70.31%] [labeled consistency loss: 1.3037, acc:acc: 81.25%] [unlabeled consistency loss: 1.4614, acc: 59.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6708 - acc: 0.7188 - val_loss: 0.9698 - val_acc: 0.7495\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7861 - acc: 0.8125 - val_loss: 0.9775 - val_acc: 0.7471\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7688 - acc: 0.8125 - val_loss: 0.9921 - val_acc: 0.7416\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6706 - acc: 0.8438 - val_loss: 1.0279 - val_acc: 0.7345\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4743 - acc: 0.9375 - val_loss: 1.0738 - val_acc: 0.7233\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3923 - acc: 0.9688 - val_loss: 1.1268 - val_acc: 0.7122\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3042 - acc: 1.0000 - val_loss: 1.1815 - val_acc: 0.7007\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3413 - acc: 1.0000 - val_loss: 1.2316 - val_acc: 0.6915\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2875 - acc: 1.0000 - val_loss: 1.2792 - val_acc: 0.6792\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2655 - acc: 1.0000 - val_loss: 1.3234 - val_acc: 0.6679\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "11 [supervised loss: 0.2655, acc: 100.00%] [unsupervised loss: 2.5888, acc: 84.38%] [labeled consistency loss: 1.6588, acc:acc: 78.12%] [unlabeled consistency loss: 0.9300, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5586 - acc: 0.8438 - val_loss: 1.4492 - val_acc: 0.6420\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6901 - acc: 0.7812 - val_loss: 1.4355 - val_acc: 0.6445\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6635 - acc: 0.8125 - val_loss: 1.3694 - val_acc: 0.6616\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4638 - acc: 0.9375 - val_loss: 1.3111 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4426 - acc: 0.9375 - val_loss: 1.2665 - val_acc: 0.6890\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4421 - acc: 0.9375 - val_loss: 1.2339 - val_acc: 0.6952\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3638 - acc: 0.9375 - val_loss: 1.2064 - val_acc: 0.7024\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3025 - acc: 1.0000 - val_loss: 1.1844 - val_acc: 0.7081\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2550 - acc: 1.0000 - val_loss: 1.1689 - val_acc: 0.7120\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2716 - acc: 1.0000 - val_loss: 1.1562 - val_acc: 0.7140\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "12 [supervised loss: 0.2716, acc: 100.00%] [unsupervised loss: 2.5547, acc: 84.38%] [labeled consistency loss: 1.3785, acc:acc: 84.38%] [unlabeled consistency loss: 1.1762, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6873 - acc: 0.8125 - val_loss: 1.1753 - val_acc: 0.7115\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6448 - acc: 0.8438 - val_loss: 1.1491 - val_acc: 0.7161\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6354 - acc: 0.8750 - val_loss: 1.1124 - val_acc: 0.7256\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5115 - acc: 0.9062 - val_loss: 1.0735 - val_acc: 0.7343\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4871 - acc: 0.9375 - val_loss: 1.0288 - val_acc: 0.7432\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3644 - acc: 0.9688 - val_loss: 0.9910 - val_acc: 0.7524\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4272 - acc: 0.9688 - val_loss: 0.9617 - val_acc: 0.7608\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2809 - acc: 1.0000 - val_loss: 0.9402 - val_acc: 0.7641\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2830 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 0.7684\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2892 - acc: 1.0000 - val_loss: 0.9149 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "13 [supervised loss: 0.2892, acc: 100.00%] [unsupervised loss: 3.0697, acc: 76.56%] [labeled consistency loss: 1.8176, acc:acc: 75.00%] [unlabeled consistency loss: 1.2520, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6089 - acc: 0.8125 - val_loss: 0.9632 - val_acc: 0.7591\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5428 - acc: 0.8750 - val_loss: 0.9778 - val_acc: 0.7563\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4989 - acc: 0.8438 - val_loss: 0.9895 - val_acc: 0.7535\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4388 - acc: 0.9062 - val_loss: 0.9944 - val_acc: 0.7511\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3977 - acc: 0.9688 - val_loss: 0.9970 - val_acc: 0.7495\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3485 - acc: 1.0000 - val_loss: 0.9997 - val_acc: 0.7501\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3918 - acc: 0.9375 - val_loss: 0.9971 - val_acc: 0.7491\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3637 - acc: 0.9688 - val_loss: 0.9926 - val_acc: 0.7486\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2924 - acc: 1.0000 - val_loss: 0.9888 - val_acc: 0.7494\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2814 - acc: 0.9688 - val_loss: 0.9874 - val_acc: 0.7484\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "14 [supervised loss: 0.2814, acc: 96.88%] [unsupervised loss: 2.4242, acc: 79.69%] [labeled consistency loss: 1.5690, acc:acc: 75.00%] [unlabeled consistency loss: 0.8552, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6258 - acc: 0.8750 - val_loss: 1.0587 - val_acc: 0.7294\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5012 - acc: 0.9062 - val_loss: 1.0476 - val_acc: 0.7317\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4551 - acc: 0.9062 - val_loss: 1.0286 - val_acc: 0.7357\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3600 - acc: 0.9688 - val_loss: 1.0146 - val_acc: 0.7402\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3176 - acc: 1.0000 - val_loss: 1.0124 - val_acc: 0.7398\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3267 - acc: 0.9688 - val_loss: 1.0196 - val_acc: 0.7389\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3062 - acc: 1.0000 - val_loss: 1.0352 - val_acc: 0.7342\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3092 - acc: 1.0000 - val_loss: 1.0587 - val_acc: 0.7279\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2916 - acc: 1.0000 - val_loss: 1.0911 - val_acc: 0.7193\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2767 - acc: 1.0000 - val_loss: 1.1279 - val_acc: 0.7112\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "15 [supervised loss: 0.2767, acc: 100.00%] [unsupervised loss: 2.9454, acc: 73.44%] [labeled consistency loss: 1.6855, acc:acc: 75.00%] [unlabeled consistency loss: 1.2599, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4980 - acc: 0.8750 - val_loss: 1.1111 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5473 - acc: 0.8750 - val_loss: 1.1005 - val_acc: 0.7150\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4697 - acc: 0.9375 - val_loss: 1.0932 - val_acc: 0.7181\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4459 - acc: 0.9688 - val_loss: 1.0844 - val_acc: 0.7179\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3598 - acc: 0.9688 - val_loss: 1.0845 - val_acc: 0.7187\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3429 - acc: 0.9688 - val_loss: 1.0859 - val_acc: 0.7159\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3288 - acc: 1.0000 - val_loss: 1.0882 - val_acc: 0.7144\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2994 - acc: 1.0000 - val_loss: 1.0898 - val_acc: 0.7130\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2810 - acc: 1.0000 - val_loss: 1.0912 - val_acc: 0.7130\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2648 - acc: 1.0000 - val_loss: 1.0908 - val_acc: 0.7118\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "16 [supervised loss: 0.2648, acc: 100.00%] [unsupervised loss: 1.8463, acc: 81.25%] [labeled consistency loss: 0.9901, acc:acc: 78.12%] [unlabeled consistency loss: 0.8561, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7164 - acc: 0.8750 - val_loss: 1.0961 - val_acc: 0.7107\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7502 - acc: 0.8125 - val_loss: 1.0780 - val_acc: 0.7125\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6190 - acc: 0.7812 - val_loss: 1.0481 - val_acc: 0.7210\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5955 - acc: 0.8438 - val_loss: 1.0158 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4743 - acc: 0.8750 - val_loss: 0.9866 - val_acc: 0.7360\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4703 - acc: 0.9062 - val_loss: 0.9609 - val_acc: 0.7422\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4365 - acc: 0.9375 - val_loss: 0.9414 - val_acc: 0.7487\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3609 - acc: 0.9688 - val_loss: 0.9297 - val_acc: 0.7503\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3520 - acc: 1.0000 - val_loss: 0.9288 - val_acc: 0.7485\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2852 - acc: 1.0000 - val_loss: 0.9348 - val_acc: 0.7454\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "17 [supervised loss: 0.2852, acc: 100.00%] [unsupervised loss: 3.3347, acc: 62.50%] [labeled consistency loss: 1.7859, acc:acc: 62.50%] [unlabeled consistency loss: 1.5488, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6863 - acc: 0.8438 - val_loss: 1.0080 - val_acc: 0.7231\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7719 - acc: 0.8125 - val_loss: 1.0223 - val_acc: 0.7183\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6456 - acc: 0.7812 - val_loss: 1.0339 - val_acc: 0.7159\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5093 - acc: 0.9062 - val_loss: 1.0482 - val_acc: 0.7123\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5357 - acc: 0.9062 - val_loss: 1.0624 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4650 - acc: 0.9062 - val_loss: 1.0793 - val_acc: 0.7070\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3361 - acc: 0.9688 - val_loss: 1.1005 - val_acc: 0.7004\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3209 - acc: 0.9688 - val_loss: 1.1245 - val_acc: 0.6967\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3333 - acc: 0.9688 - val_loss: 1.1508 - val_acc: 0.6931\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2836 - acc: 1.0000 - val_loss: 1.1777 - val_acc: 0.6871\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "18 [supervised loss: 0.2836, acc: 100.00%] [unsupervised loss: 2.8785, acc: 76.56%] [labeled consistency loss: 1.7261, acc:acc: 75.00%] [unlabeled consistency loss: 1.1524, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8118 - acc: 0.7500 - val_loss: 1.2333 - val_acc: 0.6748\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9020 - acc: 0.7188 - val_loss: 1.2298 - val_acc: 0.6748\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7233 - acc: 0.7812 - val_loss: 1.2249 - val_acc: 0.6784\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5458 - acc: 0.8750 - val_loss: 1.2233 - val_acc: 0.6796\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4752 - acc: 0.9062 - val_loss: 1.2248 - val_acc: 0.6795\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3480 - acc: 0.9375 - val_loss: 1.2296 - val_acc: 0.6776\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3388 - acc: 0.9688 - val_loss: 1.2382 - val_acc: 0.6748\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2972 - acc: 1.0000 - val_loss: 1.2485 - val_acc: 0.6720\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2576 - acc: 1.0000 - val_loss: 1.2581 - val_acc: 0.6708\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2973 - acc: 1.0000 - val_loss: 1.2659 - val_acc: 0.6682\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "19 [supervised loss: 0.2973, acc: 100.00%] [unsupervised loss: 2.5840, acc: 78.12%] [labeled consistency loss: 1.3893, acc:acc: 78.12%] [unlabeled consistency loss: 1.1947, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5559 - acc: 0.8438 - val_loss: 1.3513 - val_acc: 0.6492\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5934 - acc: 0.8750 - val_loss: 1.3629 - val_acc: 0.6474\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5171 - acc: 0.9062 - val_loss: 1.3610 - val_acc: 0.6477\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7367 - acc: 0.8438 - val_loss: 1.3547 - val_acc: 0.6487\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4727 - acc: 0.9688 - val_loss: 1.3504 - val_acc: 0.6493\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3814 - acc: 0.9688 - val_loss: 1.3454 - val_acc: 0.6524\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3616 - acc: 1.0000 - val_loss: 1.3464 - val_acc: 0.6549\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3283 - acc: 1.0000 - val_loss: 1.3473 - val_acc: 0.6551\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3198 - acc: 1.0000 - val_loss: 1.3517 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3185 - acc: 1.0000 - val_loss: 1.3604 - val_acc: 0.6559\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "20 [supervised loss: 0.3185, acc: 100.00%] [unsupervised loss: 3.0864, acc: 65.62%] [labeled consistency loss: 1.9039, acc:acc: 65.62%] [unlabeled consistency loss: 1.1825, acc: 65.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.9488 - acc: 0.7812 - val_loss: 1.4553 - val_acc: 0.6389\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7786 - acc: 0.7812 - val_loss: 1.4760 - val_acc: 0.6362\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6476 - acc: 0.8750 - val_loss: 1.4817 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4539 - acc: 0.9375 - val_loss: 1.4816 - val_acc: 0.6351\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3942 - acc: 0.9688 - val_loss: 1.4763 - val_acc: 0.6363\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3577 - acc: 0.9688 - val_loss: 1.4685 - val_acc: 0.6376\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3648 - acc: 1.0000 - val_loss: 1.4589 - val_acc: 0.6405\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3395 - acc: 1.0000 - val_loss: 1.4497 - val_acc: 0.6398\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3236 - acc: 1.0000 - val_loss: 1.4421 - val_acc: 0.6416\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3845 - acc: 0.9688 - val_loss: 1.4262 - val_acc: 0.6428\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "21 [supervised loss: 0.3845, acc: 96.88%] [unsupervised loss: 2.6589, acc: 68.75%] [labeled consistency loss: 1.3636, acc:acc: 65.62%] [unlabeled consistency loss: 1.2953, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7796 - acc: 0.8438 - val_loss: 1.5181 - val_acc: 0.6262\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5813 - acc: 0.9062 - val_loss: 1.5081 - val_acc: 0.6267\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8902 - acc: 0.8750 - val_loss: 1.4735 - val_acc: 0.6297\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5511 - acc: 0.9062 - val_loss: 1.4218 - val_acc: 0.6373\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4863 - acc: 0.9375 - val_loss: 1.3650 - val_acc: 0.6475\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4248 - acc: 0.9375 - val_loss: 1.3039 - val_acc: 0.6596\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4411 - acc: 0.9062 - val_loss: 1.2448 - val_acc: 0.6735\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4095 - acc: 0.8750 - val_loss: 1.1916 - val_acc: 0.6863\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3092 - acc: 1.0000 - val_loss: 1.1459 - val_acc: 0.6968\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3009 - acc: 1.0000 - val_loss: 1.1045 - val_acc: 0.7086\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "22 [supervised loss: 0.3009, acc: 100.00%] [unsupervised loss: 2.6675, acc: 71.88%] [labeled consistency loss: 1.4575, acc:acc: 81.25%] [unlabeled consistency loss: 1.2100, acc: 62.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6962 - acc: 0.7812 - val_loss: 1.0889 - val_acc: 0.7138\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6582 - acc: 0.7500 - val_loss: 1.0715 - val_acc: 0.7180\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7671 - acc: 0.7500 - val_loss: 1.0407 - val_acc: 0.7256\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5203 - acc: 0.8750 - val_loss: 1.0095 - val_acc: 0.7361\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3428 - acc: 1.0000 - val_loss: 0.9806 - val_acc: 0.7477\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4421 - acc: 0.9062 - val_loss: 0.9597 - val_acc: 0.7568\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3602 - acc: 0.9688 - val_loss: 0.9453 - val_acc: 0.7598\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2815 - acc: 1.0000 - val_loss: 0.9367 - val_acc: 0.7637\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2701 - acc: 1.0000 - val_loss: 0.9331 - val_acc: 0.7642\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2770 - acc: 1.0000 - val_loss: 0.9311 - val_acc: 0.7650\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "23 [supervised loss: 0.2770, acc: 100.00%] [unsupervised loss: 3.3042, acc: 71.88%] [labeled consistency loss: 2.0046, acc:acc: 71.88%] [unlabeled consistency loss: 1.2996, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6778 - acc: 0.8750 - val_loss: 0.9332 - val_acc: 0.7632\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4539 - acc: 0.9688 - val_loss: 0.9416 - val_acc: 0.7600\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5269 - acc: 0.9062 - val_loss: 0.9520 - val_acc: 0.7560\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4456 - acc: 0.9375 - val_loss: 0.9641 - val_acc: 0.7513\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4177 - acc: 0.9062 - val_loss: 0.9807 - val_acc: 0.7474\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3372 - acc: 0.9688 - val_loss: 0.9986 - val_acc: 0.7435\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3615 - acc: 0.9688 - val_loss: 1.0162 - val_acc: 0.7394\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3498 - acc: 0.9688 - val_loss: 1.0288 - val_acc: 0.7360\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2968 - acc: 1.0000 - val_loss: 1.0404 - val_acc: 0.7340\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2614 - acc: 1.0000 - val_loss: 1.0513 - val_acc: 0.7319\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "24 [supervised loss: 0.2614, acc: 100.00%] [unsupervised loss: 2.0781, acc: 82.81%] [labeled consistency loss: 1.2666, acc:acc: 75.00%] [unlabeled consistency loss: 0.8115, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5979 - acc: 0.8438 - val_loss: 1.0949 - val_acc: 0.7259\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6219 - acc: 0.8125 - val_loss: 1.0814 - val_acc: 0.7281\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5404 - acc: 0.8750 - val_loss: 1.0639 - val_acc: 0.7312\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4982 - acc: 0.9062 - val_loss: 1.0447 - val_acc: 0.7377\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4294 - acc: 0.9375 - val_loss: 1.0224 - val_acc: 0.7425\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3176 - acc: 1.0000 - val_loss: 1.0072 - val_acc: 0.7488\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3784 - acc: 0.9688 - val_loss: 0.9990 - val_acc: 0.7514\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3417 - acc: 0.9688 - val_loss: 0.9970 - val_acc: 0.7544\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3053 - acc: 1.0000 - val_loss: 1.0005 - val_acc: 0.7558\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2923 - acc: 0.9688 - val_loss: 1.0089 - val_acc: 0.7554\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "25 [supervised loss: 0.2923, acc: 96.88%] [unsupervised loss: 2.2153, acc: 79.69%] [labeled consistency loss: 1.0080, acc:acc: 84.38%] [unlabeled consistency loss: 1.2074, acc: 75.00%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0482 - acc: 0.7188 - val_loss: 0.9896 - val_acc: 0.7578\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8597 - acc: 0.7812 - val_loss: 0.9582 - val_acc: 0.7652\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7251 - acc: 0.8125 - val_loss: 0.9325 - val_acc: 0.7721\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6731 - acc: 0.8125 - val_loss: 0.9250 - val_acc: 0.7766\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5418 - acc: 0.9688 - val_loss: 0.9483 - val_acc: 0.7696\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4166 - acc: 0.9688 - val_loss: 0.9967 - val_acc: 0.7600\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3713 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.7407\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3376 - acc: 1.0000 - val_loss: 1.1571 - val_acc: 0.7232\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3061 - acc: 1.0000 - val_loss: 1.2563 - val_acc: 0.7048\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2996 - acc: 1.0000 - val_loss: 1.3604 - val_acc: 0.6863\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "26 [supervised loss: 0.2996, acc: 100.00%] [unsupervised loss: 2.9988, acc: 75.00%] [labeled consistency loss: 1.9287, acc:acc: 78.12%] [unlabeled consistency loss: 1.0701, acc: 71.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5372 - acc: 0.9688 - val_loss: 1.3917 - val_acc: 0.6805\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5103 - acc: 0.9062 - val_loss: 1.3716 - val_acc: 0.6850\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4622 - acc: 0.9062 - val_loss: 1.3485 - val_acc: 0.6902\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4702 - acc: 0.9062 - val_loss: 1.3269 - val_acc: 0.6951\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4429 - acc: 0.9688 - val_loss: 1.3110 - val_acc: 0.7006\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2953 - acc: 1.0000 - val_loss: 1.2995 - val_acc: 0.7023\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3606 - acc: 0.9375 - val_loss: 1.2965 - val_acc: 0.7023\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2589 - acc: 1.0000 - val_loss: 1.2957 - val_acc: 0.7017\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2555 - acc: 1.0000 - val_loss: 1.3003 - val_acc: 0.7009\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2631 - acc: 1.0000 - val_loss: 1.3068 - val_acc: 0.6994\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "27 [supervised loss: 0.2631, acc: 100.00%] [unsupervised loss: 1.7683, acc: 85.94%] [labeled consistency loss: 0.8821, acc:acc: 90.62%] [unlabeled consistency loss: 0.8861, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5810 - acc: 0.9062 - val_loss: 1.3073 - val_acc: 0.7012\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5327 - acc: 0.9062 - val_loss: 1.2884 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4960 - acc: 0.9375 - val_loss: 1.2585 - val_acc: 0.7091\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4827 - acc: 0.9062 - val_loss: 1.2249 - val_acc: 0.7138\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3077 - acc: 1.0000 - val_loss: 1.2037 - val_acc: 0.7151\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2944 - acc: 0.9688 - val_loss: 1.1937 - val_acc: 0.7148\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2836 - acc: 1.0000 - val_loss: 1.1949 - val_acc: 0.7124\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3108 - acc: 0.9688 - val_loss: 1.2048 - val_acc: 0.7119\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2810 - acc: 1.0000 - val_loss: 1.2207 - val_acc: 0.7087\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2880 - acc: 1.0000 - val_loss: 1.2419 - val_acc: 0.7016\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "28 [supervised loss: 0.2880, acc: 100.00%] [unsupervised loss: 2.1555, acc: 79.69%] [labeled consistency loss: 1.1491, acc:acc: 81.25%] [unlabeled consistency loss: 1.0065, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6937 - acc: 0.7812 - val_loss: 1.2768 - val_acc: 0.6952\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6344 - acc: 0.8438 - val_loss: 1.2493 - val_acc: 0.7010\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4380 - acc: 0.9375 - val_loss: 1.2151 - val_acc: 0.7102\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4456 - acc: 0.9375 - val_loss: 1.1779 - val_acc: 0.7194\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3780 - acc: 0.9375 - val_loss: 1.1434 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3442 - acc: 0.9688 - val_loss: 1.1144 - val_acc: 0.7342\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3116 - acc: 0.9688 - val_loss: 1.0928 - val_acc: 0.7399\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2797 - acc: 1.0000 - val_loss: 1.0763 - val_acc: 0.7427\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2642 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.7423\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2584 - acc: 1.0000 - val_loss: 1.0552 - val_acc: 0.7450\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "29 [supervised loss: 0.2584, acc: 100.00%] [unsupervised loss: 2.4609, acc: 78.12%] [labeled consistency loss: 1.5251, acc:acc: 75.00%] [unlabeled consistency loss: 0.9358, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6359 - acc: 0.8750 - val_loss: 1.0399 - val_acc: 0.7463\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6086 - acc: 0.9062 - val_loss: 1.0275 - val_acc: 0.7506\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5498 - acc: 0.8750 - val_loss: 1.0153 - val_acc: 0.7529\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4122 - acc: 0.9688 - val_loss: 1.0036 - val_acc: 0.7550\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4600 - acc: 0.9062 - val_loss: 0.9954 - val_acc: 0.7560\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3630 - acc: 1.0000 - val_loss: 0.9896 - val_acc: 0.7571\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3128 - acc: 1.0000 - val_loss: 0.9862 - val_acc: 0.7589\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2942 - acc: 1.0000 - val_loss: 0.9863 - val_acc: 0.7576\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2665 - acc: 1.0000 - val_loss: 0.9873 - val_acc: 0.7564\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3039 - acc: 1.0000 - val_loss: 0.9899 - val_acc: 0.7547\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "30 [supervised loss: 0.3039, acc: 100.00%] [unsupervised loss: 1.7073, acc: 90.62%] [labeled consistency loss: 0.9951, acc:acc: 90.62%] [unlabeled consistency loss: 0.7122, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7255 - acc: 0.8438 - val_loss: 0.9946 - val_acc: 0.7513\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5471 - acc: 0.8438 - val_loss: 0.9796 - val_acc: 0.7545\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5418 - acc: 0.8438 - val_loss: 0.9627 - val_acc: 0.7569\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4211 - acc: 0.9062 - val_loss: 0.9465 - val_acc: 0.7616\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4063 - acc: 0.9688 - val_loss: 0.9309 - val_acc: 0.7674\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3178 - acc: 0.9688 - val_loss: 0.9170 - val_acc: 0.7710\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3042 - acc: 1.0000 - val_loss: 0.9057 - val_acc: 0.7707\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2990 - acc: 0.9688 - val_loss: 0.8963 - val_acc: 0.7721\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2800 - acc: 1.0000 - val_loss: 0.8891 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2535 - acc: 1.0000 - val_loss: 0.8844 - val_acc: 0.7761\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "31 [supervised loss: 0.2535, acc: 100.00%] [unsupervised loss: 2.2750, acc: 81.25%] [labeled consistency loss: 1.3223, acc:acc: 78.12%] [unlabeled consistency loss: 0.9527, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7517 - acc: 0.7812 - val_loss: 0.8903 - val_acc: 0.7742\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6612 - acc: 0.8438 - val_loss: 0.8859 - val_acc: 0.7751\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4905 - acc: 0.9375 - val_loss: 0.8781 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4932 - acc: 0.8750 - val_loss: 0.8701 - val_acc: 0.7805\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5052 - acc: 0.8750 - val_loss: 0.8616 - val_acc: 0.7816\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4578 - acc: 0.9688 - val_loss: 0.8574 - val_acc: 0.7833\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3478 - acc: 0.9375 - val_loss: 0.8550 - val_acc: 0.7823\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2842 - acc: 1.0000 - val_loss: 0.8548 - val_acc: 0.7835\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3172 - acc: 1.0000 - val_loss: 0.8554 - val_acc: 0.7832\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2586 - acc: 1.0000 - val_loss: 0.8572 - val_acc: 0.7843\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "32 [supervised loss: 0.2586, acc: 100.00%] [unsupervised loss: 2.1000, acc: 81.25%] [labeled consistency loss: 1.3483, acc:acc: 75.00%] [unlabeled consistency loss: 0.7518, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4831 - acc: 0.9375 - val_loss: 0.8628 - val_acc: 0.7829\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4576 - acc: 0.9062 - val_loss: 0.8621 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4140 - acc: 0.9375 - val_loss: 0.8624 - val_acc: 0.7874\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3467 - acc: 0.9688 - val_loss: 0.8640 - val_acc: 0.7867\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4140 - acc: 0.9375 - val_loss: 0.8616 - val_acc: 0.7874\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2969 - acc: 1.0000 - val_loss: 0.8614 - val_acc: 0.7883\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2822 - acc: 1.0000 - val_loss: 0.8613 - val_acc: 0.7883\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2555 - acc: 1.0000 - val_loss: 0.8644 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2737 - acc: 1.0000 - val_loss: 0.8702 - val_acc: 0.7858\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2467 - acc: 1.0000 - val_loss: 0.8781 - val_acc: 0.7846\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "33 [supervised loss: 0.2467, acc: 100.00%] [unsupervised loss: 1.9261, acc: 82.81%] [labeled consistency loss: 1.0740, acc:acc: 78.12%] [unlabeled consistency loss: 0.8521, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7512 - acc: 0.8125 - val_loss: 0.8997 - val_acc: 0.7778\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6327 - acc: 0.7812 - val_loss: 0.9017 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6216 - acc: 0.8125 - val_loss: 0.9025 - val_acc: 0.7754\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6456 - acc: 0.7812 - val_loss: 0.8993 - val_acc: 0.7758\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4158 - acc: 0.9062 - val_loss: 0.8992 - val_acc: 0.7752\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3961 - acc: 0.9062 - val_loss: 0.9018 - val_acc: 0.7730\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4414 - acc: 0.8750 - val_loss: 0.9050 - val_acc: 0.7718\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3000 - acc: 0.9688 - val_loss: 0.9108 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3274 - acc: 0.9688 - val_loss: 0.9191 - val_acc: 0.7669\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3366 - acc: 0.9375 - val_loss: 0.9273 - val_acc: 0.7631\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "34 [supervised loss: 0.3366, acc: 93.75%] [unsupervised loss: 1.5884, acc: 89.06%] [labeled consistency loss: 0.8958, acc:acc: 93.75%] [unlabeled consistency loss: 0.6927, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5303 - acc: 0.9375 - val_loss: 0.9211 - val_acc: 0.7646\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5249 - acc: 0.9375 - val_loss: 0.9091 - val_acc: 0.7683\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3540 - acc: 0.9375 - val_loss: 0.8958 - val_acc: 0.7714\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3677 - acc: 0.9375 - val_loss: 0.8809 - val_acc: 0.7757\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3199 - acc: 0.9688 - val_loss: 0.8716 - val_acc: 0.7777\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2715 - acc: 1.0000 - val_loss: 0.8676 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2779 - acc: 1.0000 - val_loss: 0.8679 - val_acc: 0.7766\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2762 - acc: 1.0000 - val_loss: 0.8713 - val_acc: 0.7766\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2678 - acc: 1.0000 - val_loss: 0.8768 - val_acc: 0.7761\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2399 - acc: 1.0000 - val_loss: 0.8844 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "35 [supervised loss: 0.2399, acc: 100.00%] [unsupervised loss: 2.0052, acc: 90.62%] [labeled consistency loss: 1.1034, acc:acc: 90.62%] [unlabeled consistency loss: 0.9018, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4509 - acc: 0.9062 - val_loss: 0.9171 - val_acc: 0.7653\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4776 - acc: 0.9062 - val_loss: 0.9246 - val_acc: 0.7632\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4924 - acc: 0.8750 - val_loss: 0.9266 - val_acc: 0.7637\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4299 - acc: 0.9375 - val_loss: 0.9303 - val_acc: 0.7634\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3866 - acc: 0.9062 - val_loss: 0.9330 - val_acc: 0.7618\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3677 - acc: 0.9688 - val_loss: 0.9332 - val_acc: 0.7624\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2861 - acc: 0.9688 - val_loss: 0.9360 - val_acc: 0.7606\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3142 - acc: 0.9688 - val_loss: 0.9351 - val_acc: 0.7627\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2953 - acc: 0.9688 - val_loss: 0.9315 - val_acc: 0.7661\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 0.9283 - val_acc: 0.7662\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "36 [supervised loss: 0.2455, acc: 100.00%] [unsupervised loss: 1.7205, acc: 89.06%] [labeled consistency loss: 0.8484, acc:acc: 90.62%] [unlabeled consistency loss: 0.8721, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4941 - acc: 0.9062 - val_loss: 0.9402 - val_acc: 0.7635\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5331 - acc: 0.9375 - val_loss: 0.9284 - val_acc: 0.7676\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6046 - acc: 0.8750 - val_loss: 0.9146 - val_acc: 0.7721\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4956 - acc: 0.8750 - val_loss: 0.8971 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3439 - acc: 1.0000 - val_loss: 0.8841 - val_acc: 0.7793\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3703 - acc: 0.9062 - val_loss: 0.8806 - val_acc: 0.7801\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3004 - acc: 1.0000 - val_loss: 0.8845 - val_acc: 0.7774\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2705 - acc: 1.0000 - val_loss: 0.8929 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2494 - acc: 1.0000 - val_loss: 0.9055 - val_acc: 0.7668\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2438 - acc: 1.0000 - val_loss: 0.9212 - val_acc: 0.7610\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "37 [supervised loss: 0.2438, acc: 100.00%] [unsupervised loss: 1.8235, acc: 92.19%] [labeled consistency loss: 1.0532, acc:acc: 87.50%] [unlabeled consistency loss: 0.7703, acc: 96.88%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3909 - acc: 0.9062 - val_loss: 0.9505 - val_acc: 0.7516\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3381 - acc: 0.9688 - val_loss: 0.9469 - val_acc: 0.7532\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3537 - acc: 0.9375 - val_loss: 0.9434 - val_acc: 0.7571\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2786 - acc: 1.0000 - val_loss: 0.9409 - val_acc: 0.7595\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2730 - acc: 1.0000 - val_loss: 0.9422 - val_acc: 0.7616\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2449 - acc: 1.0000 - val_loss: 0.9464 - val_acc: 0.7621\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2593 - acc: 1.0000 - val_loss: 0.9521 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2361 - acc: 1.0000 - val_loss: 0.9584 - val_acc: 0.7612\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2682 - acc: 0.9688 - val_loss: 0.9618 - val_acc: 0.7603\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2360 - acc: 1.0000 - val_loss: 0.9659 - val_acc: 0.7606\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "38 [supervised loss: 0.2360, acc: 100.00%] [unsupervised loss: 1.8698, acc: 82.81%] [labeled consistency loss: 1.0049, acc:acc: 84.38%] [unlabeled consistency loss: 0.8650, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7497 - acc: 0.8438 - val_loss: 0.9538 - val_acc: 0.7632\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5405 - acc: 0.9062 - val_loss: 0.9247 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7402 - acc: 0.8125 - val_loss: 0.8862 - val_acc: 0.7817\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6209 - acc: 0.8438 - val_loss: 0.8467 - val_acc: 0.7913\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5197 - acc: 0.9375 - val_loss: 0.8124 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3089 - acc: 1.0000 - val_loss: 0.7859 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3232 - acc: 0.9688 - val_loss: 0.7673 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2761 - acc: 1.0000 - val_loss: 0.7567 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2560 - acc: 1.0000 - val_loss: 0.7532 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2286 - acc: 1.0000 - val_loss: 0.7564 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "39 [supervised loss: 0.2286, acc: 100.00%] [unsupervised loss: 2.6334, acc: 82.81%] [labeled consistency loss: 1.6267, acc:acc: 81.25%] [unlabeled consistency loss: 1.0067, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5741 - acc: 0.8438 - val_loss: 0.7727 - val_acc: 0.8129\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4315 - acc: 0.9062 - val_loss: 0.7769 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3265 - acc: 1.0000 - val_loss: 0.7812 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4617 - acc: 0.9688 - val_loss: 0.7891 - val_acc: 0.8120\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4178 - acc: 0.9688 - val_loss: 0.8013 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2882 - acc: 0.9688 - val_loss: 0.8157 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3162 - acc: 1.0000 - val_loss: 0.8323 - val_acc: 0.8037\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2490 - acc: 1.0000 - val_loss: 0.8510 - val_acc: 0.8015\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2901 - acc: 1.0000 - val_loss: 0.8719 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2787 - acc: 1.0000 - val_loss: 0.8948 - val_acc: 0.7904\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "40 [supervised loss: 0.2787, acc: 100.00%] [unsupervised loss: 1.7447, acc: 89.06%] [labeled consistency loss: 0.9835, acc:acc: 87.50%] [unlabeled consistency loss: 0.7612, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6396 - acc: 0.8438 - val_loss: 0.9116 - val_acc: 0.7868\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5143 - acc: 0.9062 - val_loss: 0.9053 - val_acc: 0.7892\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4108 - acc: 0.9375 - val_loss: 0.8957 - val_acc: 0.7919\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5221 - acc: 0.8438 - val_loss: 0.8873 - val_acc: 0.7942\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4140 - acc: 0.9375 - val_loss: 0.8767 - val_acc: 0.7956\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2827 - acc: 1.0000 - val_loss: 0.8683 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3122 - acc: 1.0000 - val_loss: 0.8615 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3698 - acc: 0.9688 - val_loss: 0.8560 - val_acc: 0.8001\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2621 - acc: 1.0000 - val_loss: 0.8550 - val_acc: 0.7988\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2564 - acc: 1.0000 - val_loss: 0.8572 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "41 [supervised loss: 0.2564, acc: 100.00%] [unsupervised loss: 2.0411, acc: 87.50%] [labeled consistency loss: 1.1260, acc:acc: 87.50%] [unlabeled consistency loss: 0.9151, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6564 - acc: 0.8438 - val_loss: 0.8687 - val_acc: 0.7938\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5156 - acc: 0.9062 - val_loss: 0.8672 - val_acc: 0.7961\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7007 - acc: 0.7812 - val_loss: 0.8610 - val_acc: 0.7970\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5406 - acc: 0.8750 - val_loss: 0.8554 - val_acc: 0.7983\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4217 - acc: 0.9375 - val_loss: 0.8552 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4050 - acc: 0.9375 - val_loss: 0.8579 - val_acc: 0.7959\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3488 - acc: 0.9688 - val_loss: 0.8678 - val_acc: 0.7916\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3384 - acc: 0.9688 - val_loss: 0.8814 - val_acc: 0.7877\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2898 - acc: 1.0000 - val_loss: 0.8992 - val_acc: 0.7810\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2853 - acc: 1.0000 - val_loss: 0.9188 - val_acc: 0.7748\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "42 [supervised loss: 0.2853, acc: 100.00%] [unsupervised loss: 1.6273, acc: 89.06%] [labeled consistency loss: 1.0229, acc:acc: 84.38%] [unlabeled consistency loss: 0.6045, acc: 93.75%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7714 - acc: 0.8125 - val_loss: 0.9417 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5959 - acc: 0.8438 - val_loss: 0.9554 - val_acc: 0.7662\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4840 - acc: 0.8750 - val_loss: 0.9714 - val_acc: 0.7608\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4927 - acc: 0.9062 - val_loss: 0.9896 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3686 - acc: 0.9375 - val_loss: 1.0053 - val_acc: 0.7511\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3592 - acc: 0.9375 - val_loss: 1.0204 - val_acc: 0.7483\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3330 - acc: 0.9688 - val_loss: 1.0337 - val_acc: 0.7440\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2924 - acc: 0.9688 - val_loss: 1.0424 - val_acc: 0.7429\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2706 - acc: 1.0000 - val_loss: 1.0472 - val_acc: 0.7407\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2578 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.7391\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "43 [supervised loss: 0.2578, acc: 100.00%] [unsupervised loss: 2.0411, acc: 85.94%] [labeled consistency loss: 1.2019, acc:acc: 90.62%] [unlabeled consistency loss: 0.8392, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4173 - acc: 0.9062 - val_loss: 1.0789 - val_acc: 0.7303\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3815 - acc: 0.9375 - val_loss: 1.0747 - val_acc: 0.7310\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3934 - acc: 0.9062 - val_loss: 1.0586 - val_acc: 0.7336\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3370 - acc: 0.9688 - val_loss: 1.0389 - val_acc: 0.7373\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3535 - acc: 0.9688 - val_loss: 1.0126 - val_acc: 0.7406\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2672 - acc: 1.0000 - val_loss: 0.9892 - val_acc: 0.7453\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2813 - acc: 0.9688 - val_loss: 0.9676 - val_acc: 0.7514\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3171 - acc: 0.9688 - val_loss: 0.9410 - val_acc: 0.7580\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3248 - acc: 0.9688 - val_loss: 0.9170 - val_acc: 0.7647\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2655 - acc: 1.0000 - val_loss: 0.9019 - val_acc: 0.7678\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "44 [supervised loss: 0.2655, acc: 100.00%] [unsupervised loss: 1.6660, acc: 89.06%] [labeled consistency loss: 0.7798, acc:acc: 93.75%] [unlabeled consistency loss: 0.8862, acc: 84.38%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4408 - acc: 0.9688 - val_loss: 0.8980 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4557 - acc: 0.9375 - val_loss: 0.8907 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3276 - acc: 0.9688 - val_loss: 0.8855 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4088 - acc: 0.9375 - val_loss: 0.8734 - val_acc: 0.7753\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3879 - acc: 0.9688 - val_loss: 0.8617 - val_acc: 0.7779\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3962 - acc: 0.9375 - val_loss: 0.8519 - val_acc: 0.7797\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2779 - acc: 1.0000 - val_loss: 0.8465 - val_acc: 0.7797\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2647 - acc: 1.0000 - val_loss: 0.8472 - val_acc: 0.7771\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2426 - acc: 1.0000 - val_loss: 0.8516 - val_acc: 0.7762\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2532 - acc: 1.0000 - val_loss: 0.8598 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "45 [supervised loss: 0.2532, acc: 100.00%] [unsupervised loss: 2.3369, acc: 89.06%] [labeled consistency loss: 1.3614, acc:acc: 90.62%] [unlabeled consistency loss: 0.9755, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5519 - acc: 0.9062 - val_loss: 0.8797 - val_acc: 0.7681\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4620 - acc: 0.9375 - val_loss: 0.8853 - val_acc: 0.7663\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5311 - acc: 0.8438 - val_loss: 0.8854 - val_acc: 0.7681\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4120 - acc: 0.9375 - val_loss: 0.8823 - val_acc: 0.7681\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3178 - acc: 1.0000 - val_loss: 0.8804 - val_acc: 0.7701\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3228 - acc: 0.9688 - val_loss: 0.8787 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3244 - acc: 0.9688 - val_loss: 0.8775 - val_acc: 0.7725\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2873 - acc: 0.9688 - val_loss: 0.8761 - val_acc: 0.7748\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2399 - acc: 1.0000 - val_loss: 0.8761 - val_acc: 0.7763\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2527 - acc: 1.0000 - val_loss: 0.8771 - val_acc: 0.7754\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "46 [supervised loss: 0.2527, acc: 100.00%] [unsupervised loss: 1.9841, acc: 84.38%] [labeled consistency loss: 1.2375, acc:acc: 78.12%] [unlabeled consistency loss: 0.7466, acc: 90.62%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5693 - acc: 0.8438 - val_loss: 0.8947 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6664 - acc: 0.8438 - val_loss: 0.8975 - val_acc: 0.7714\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4140 - acc: 0.9688 - val_loss: 0.9018 - val_acc: 0.7699\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4223 - acc: 0.9062 - val_loss: 0.9044 - val_acc: 0.7689\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4096 - acc: 0.9688 - val_loss: 0.9111 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2896 - acc: 0.9688 - val_loss: 0.9188 - val_acc: 0.7659\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2703 - acc: 1.0000 - val_loss: 0.9299 - val_acc: 0.7642\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2536 - acc: 1.0000 - val_loss: 0.9408 - val_acc: 0.7640\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2366 - acc: 1.0000 - val_loss: 0.9554 - val_acc: 0.7602\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2306 - acc: 1.0000 - val_loss: 0.9703 - val_acc: 0.7578\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "47 [supervised loss: 0.2306, acc: 100.00%] [unsupervised loss: 1.9803, acc: 82.81%] [labeled consistency loss: 1.0081, acc:acc: 87.50%] [unlabeled consistency loss: 0.9722, acc: 78.12%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5311 - acc: 0.8750 - val_loss: 0.9951 - val_acc: 0.7514\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5802 - acc: 0.7812 - val_loss: 0.9811 - val_acc: 0.7560\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4612 - acc: 0.8750 - val_loss: 0.9659 - val_acc: 0.7612\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3799 - acc: 0.9688 - val_loss: 0.9553 - val_acc: 0.7651\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3414 - acc: 1.0000 - val_loss: 0.9467 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3399 - acc: 0.9688 - val_loss: 0.9415 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2709 - acc: 1.0000 - val_loss: 0.9415 - val_acc: 0.7669\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2522 - acc: 1.0000 - val_loss: 0.9466 - val_acc: 0.7671\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3181 - acc: 0.9688 - val_loss: 0.9547 - val_acc: 0.7642\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2437 - acc: 1.0000 - val_loss: 0.9639 - val_acc: 0.7599\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "48 [supervised loss: 0.2437, acc: 100.00%] [unsupervised loss: 1.7818, acc: 84.38%] [labeled consistency loss: 0.9762, acc:acc: 87.50%] [unlabeled consistency loss: 0.8056, acc: 81.25%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5446 - acc: 0.8125 - val_loss: 1.0281 - val_acc: 0.7428\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4983 - acc: 0.8438 - val_loss: 1.0507 - val_acc: 0.7392\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5327 - acc: 0.8438 - val_loss: 1.0646 - val_acc: 0.7350\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5049 - acc: 0.9375 - val_loss: 1.0705 - val_acc: 0.7349\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3268 - acc: 0.9688 - val_loss: 1.0739 - val_acc: 0.7347\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3922 - acc: 0.9688 - val_loss: 1.0709 - val_acc: 0.7364\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2837 - acc: 0.9688 - val_loss: 1.0698 - val_acc: 0.7379\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2764 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.7389\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2557 - acc: 1.0000 - val_loss: 1.0721 - val_acc: 0.7398\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2290 - acc: 1.0000 - val_loss: 1.0735 - val_acc: 0.7395\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "49 [supervised loss: 0.2290, acc: 100.00%] [unsupervised loss: 2.0503, acc: 87.50%] [labeled consistency loss: 1.1933, acc:acc: 87.50%] [unlabeled consistency loss: 0.8569, acc: 87.50%]\n",
            "Epoch 1/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5380 - acc: 0.8750 - val_loss: 1.0844 - val_acc: 0.7384\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87680\n",
            "Epoch 2/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5454 - acc: 0.8750 - val_loss: 1.0720 - val_acc: 0.7403\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87680\n",
            "Epoch 3/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3979 - acc: 0.9375 - val_loss: 1.0514 - val_acc: 0.7465\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87680\n",
            "Epoch 4/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.4155 - acc: 0.9688 - val_loss: 1.0281 - val_acc: 0.7537\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87680\n",
            "Epoch 5/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3362 - acc: 1.0000 - val_loss: 1.0021 - val_acc: 0.7602\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87680\n",
            "Epoch 6/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2745 - acc: 1.0000 - val_loss: 0.9776 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87680\n",
            "Epoch 7/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2951 - acc: 0.9688 - val_loss: 0.9571 - val_acc: 0.7702\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87680\n",
            "Epoch 8/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2642 - acc: 1.0000 - val_loss: 0.9398 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87680\n",
            "Epoch 9/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2619 - acc: 1.0000 - val_loss: 0.9232 - val_acc: 0.7777\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87680\n",
            "Epoch 10/10\n",
            "Learning rate:  0.001\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2700 - acc: 1.0000 - val_loss: 0.9095 - val_acc: 0.7813\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87680\n",
            "50 [supervised loss: 0.2700, acc: 100.00%] [unsupervised loss: 1.9187, acc: 89.06%] [labeled consistency loss: 1.1377, acc:acc: 87.50%] [unlabeled consistency loss: 0.7810, acc: 90.62%]\n",
            "Training time: 1638.3324s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ndAup4NMwkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "3e3857f1-4697-45b6-b22a-180fc42ed11f"
      },
      "source": [
        "plot_supervised_losses = np.array(supervised_losses)\n",
        "plot_unsupervised_losses = np.array(unsupervised_losses)\n",
        "plot_labeled_consistency_costs = np.array(labeled_consistency_costs)\n",
        "plot_unlabeled_consistency_costs = np.array(unlabeled_consistency_costs)\n",
        "plot_all_losses = np.array(supervised_losses)+np.array(unsupervised_losses)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, plot_all_losses, label=\"All loss\", color='black')\n",
        "plt.plot(iteration_checkpoints, plot_supervised_losses, label=\"Supervised loss\", color='tab:blue')\n",
        "plt.plot(iteration_checkpoints, plot_unsupervised_losses, label=\"Unsupervised loss\", color='tab:green')\n",
        "plt.plot(iteration_checkpoints, plot_labeled_consistency_costs, label=\"Labeled consistency loss\", color='tab:red', linestyle='dashed')\n",
        "plt.plot(iteration_checkpoints, plot_unlabeled_consistency_costs, label=\"Unlabeled consistency loss\", color='tab:orange', linestyle='dashed')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Mean Teacher's Supervised and Unsupervised Loss, num_labeled=%d\" % num_labeled)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29c65bbcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFPCAYAAAASkBw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCGEEEIIIQY46vgJIYQQQgghxADn/5Fa9QwBecwEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omcpkiBMwiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "\n",
        "tmodel = build_discriminator_supervised(build_discriminator_net(img_shape, depth))\n",
        "tmodel.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-50.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftgecc1jMwfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "\n",
        "tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-50.h5\", by_name=False)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = tmodel.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDJmNbdMwce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "252c9f3f-bc20-4857-b0b9-25bb70b7342d"
      },
      "source": [
        "accs = []\n",
        "# tx = [x for x in range(1,51,1)]\n",
        "tx = [x for x in range(1, len(iteration_checkpoints)+1, 1)]\n",
        "acc_max = [0,0]\n",
        "\n",
        "for e in tx:\n",
        "  tmodel.load_weights(\"./models/models-label-\" + str(num_labeled) + \"/teacher-\"+ str(e) +\".h5\", by_name=False)\n",
        "  _, acc = tmodel.evaluate(x, y)\n",
        "  accs.append(acc)\n",
        "print(max(accs))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tx, accs, label=\"accs\", color='blue')\n",
        "plt.xticks(tx, rotation=90)\n",
        "plt.title(\"Mean Teacher's accs with epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accs\")\n",
        "plt.legend()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 398us/step\n",
            "10000/10000 [==============================] - 4s 403us/step\n",
            "10000/10000 [==============================] - 4s 400us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 402us/step\n",
            "10000/10000 [==============================] - 4s 405us/step\n",
            "10000/10000 [==============================] - 4s 408us/step\n",
            "10000/10000 [==============================] - 4s 405us/step\n",
            "10000/10000 [==============================] - 4s 404us/step\n",
            "10000/10000 [==============================] - 4s 420us/step\n",
            "10000/10000 [==============================] - 4s 423us/step\n",
            "10000/10000 [==============================] - 4s 429us/step\n",
            "10000/10000 [==============================] - 4s 429us/step\n",
            "10000/10000 [==============================] - 5s 455us/step\n",
            "10000/10000 [==============================] - 5s 459us/step\n",
            "10000/10000 [==============================] - 5s 456us/step\n",
            "10000/10000 [==============================] - 5s 455us/step\n",
            "10000/10000 [==============================] - 5s 451us/step\n",
            "10000/10000 [==============================] - 5s 451us/step\n",
            "10000/10000 [==============================] - 4s 442us/step\n",
            "10000/10000 [==============================] - 4s 419us/step\n",
            "10000/10000 [==============================] - 4s 422us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 408us/step\n",
            "10000/10000 [==============================] - 4s 415us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 418us/step\n",
            "10000/10000 [==============================] - 4s 416us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 4s 412us/step\n",
            "10000/10000 [==============================] - 4s 409us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 410us/step\n",
            "10000/10000 [==============================] - 4s 413us/step\n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "10000/10000 [==============================] - 4s 414us/step\n",
            "10000/10000 [==============================] - 5s 473us/step\n",
            "10000/10000 [==============================] - 4s 421us/step\n",
            "10000/10000 [==============================] - 4s 423us/step\n",
            "10000/10000 [==============================] - 4s 426us/step\n",
            "10000/10000 [==============================] - 4s 424us/step\n",
            "10000/10000 [==============================] - 4s 426us/step\n",
            "10000/10000 [==============================] - 4s 431us/step\n",
            "10000/10000 [==============================] - 4s 432us/step\n",
            "10000/10000 [==============================] - 4s 434us/step\n",
            "10000/10000 [==============================] - 4s 432us/step\n",
            "10000/10000 [==============================] - 4s 428us/step\n",
            "10000/10000 [==============================] - 4s 430us/step\n",
            "10000/10000 [==============================] - 4s 446us/step\n",
            "0.8222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29cde91668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFPCAYAAAAfjmxyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2feQLXtMwY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6301c9c9-63fe-405e-8e68-f1745db06d2a"
      },
      "source": [
        "print(max(accs[2:]))\n",
        "# for acc in accs:\n",
        "#   print(acc)\n",
        "mt_accs = accs\n",
        "print(mt_accs)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8222\n",
            "[0.1273, 0.1347, 0.509, 0.6353, 0.7401, 0.7658, 0.76, 0.769, 0.7773, 0.7715, 0.7454, 0.7417, 0.7735, 0.7696, 0.7684, 0.7499, 0.7576, 0.7374, 0.7161, 0.6974, 0.6765, 0.7108, 0.7644, 0.7509, 0.7682, 0.7745, 0.7488, 0.7462, 0.759, 0.7706, 0.7845, 0.7934, 0.7996, 0.8011, 0.7951, 0.7908, 0.7906, 0.7904, 0.8147, 0.8149, 0.8222, 0.8116, 0.7943, 0.7918, 0.795, 0.7924, 0.781, 0.7868, 0.7744, 0.7899]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}