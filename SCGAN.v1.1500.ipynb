{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "SCGAN.v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrJW9Ib5ICb",
        "colab_type": "text"
      },
      "source": [
        "# Semi-Supervised Conditional GAN (SCGAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWoI_bz5ICe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import (Activation, BatchNormalization, Concatenate, Dense,\n",
        "                          Dropout, Flatten, Input, Lambda, Reshape, Embedding,\n",
        "                          Multiply)\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn9DfamS5ICl",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqo19imG5ICn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, num_labeled):\n",
        "\n",
        "        # Number labeled examples to use for training\n",
        "        self.num_labeled = num_labeled\n",
        "\n",
        "        # Load the MNIST dataset\n",
        "        (self.x_train, self.y_train), (self.x_test,\n",
        "                                       self.y_test) = mnist.load_data()\n",
        "\n",
        "        def preprocess_imgs(x):\n",
        "            # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "            # Expand image dimensions to width x height x channels\n",
        "            x = np.expand_dims(x, axis=3)\n",
        "            return x\n",
        "\n",
        "        def preprocess_labels(y):\n",
        "            return y.reshape(-1, 1)\n",
        "\n",
        "        # Training data\n",
        "        self.x_train = preprocess_imgs(self.x_train)\n",
        "        self.y_train = preprocess_labels(self.y_train)\n",
        "\n",
        "        # Testing data\n",
        "        self.x_test = preprocess_imgs(self.x_test)\n",
        "        self.y_test = preprocess_labels(self.y_test)\n",
        "\n",
        "    def batch_labeled(self, batch_size):\n",
        "        # Get a random batch of labeled images and their labels\n",
        "        idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        labels = self.y_train[idx]\n",
        "        return imgs, labels\n",
        "\n",
        "    def batch_unlabeled(self, batch_size):\n",
        "        # Get a random batch of unlabeled images\n",
        "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0],\n",
        "                                batch_size)\n",
        "        imgs = self.x_train[idx]\n",
        "        return imgs\n",
        "\n",
        "    def training_set(self):\n",
        "        x_train = self.x_train[range(self.num_labeled)]\n",
        "        y_train = self.y_train[range(self.num_labeled)]\n",
        "        return x_train, y_train\n",
        "\n",
        "    def test_set(self):\n",
        "        return self.x_test, self.y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6p2xUzF5ICt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of labeled examples to use (rest will be used as unlabeled)\n",
        "num_labeled = 1500\n",
        "\n",
        "dataset = Dataset(num_labeled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW2-FIrD2z8n",
        "colab_type": "code",
        "outputId": "91a43745-055d-4c2a-eba3-9aa606480d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtT5bpqm5ICy",
        "colab_type": "text"
      },
      "source": [
        "# Semi-Supervied GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeaE_XJq5IC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "# Input image dimensions\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "# Size of the noise vector, used as input to the Generator\n",
        "z_dim = 100\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEGfARiO5IC5",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpyzLGKJ5IC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(z_dim):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Reshape input into 7x7x256 tensor via a fully connected layer\n",
        "    model.add(Dense(256 * 7 * 7, input_dim=z_dim))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "\n",
        "    # Transposed convolution layer, from 7x7x256 into 14x14x128 tensor\n",
        "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "\n",
        "    # Batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Transposed convolution layer, from 14x14x128 to 14x14x64 tensor\n",
        "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
        "\n",
        "    # Batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Transposed convolution layer, from 14x14x64 to 28x28x1 tensor\n",
        "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n",
        "\n",
        "    # Output layer with tanh activation\n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ), dtype='float32')\n",
        "    label_embedding = Dense(z_dim, input_dim=num_classes)(label)\n",
        "    print(label_embedding)\n",
        "    joined_representation = Multiply()([z, label_embedding])\n",
        "    print(joined_representation)\n",
        "    conditioned_img = model(joined_representation)\n",
        "\n",
        "    model = Model([z, label], conditioned_img)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRiDvGxWOKQZ",
        "colab_type": "code",
        "outputId": "d469bf40-3773-466a-d520-6107d1f57d24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "build_generator(z_dim).summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dense_13/BiasAdd:0\", shape=(?, 100), dtype=float32)\n",
            "Tensor(\"multiply_7/mul:0\", shape=(?, 100), dtype=float32)\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 100)          1100        input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_7 (Multiply)           (None, 100)          0           input_17[0][0]                   \n",
            "                                                                 dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_11 (Sequential)      (None, 28, 28, 1)    1637121     multiply_7[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,638,221\n",
            "Trainable params: 1,637,837\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXctmDp35IC-",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VahG39Gb5IC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_net(img_shape):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolutional layer, from 28x28x1 into 14x14x32 tensor\n",
        "    model.add(\n",
        "        Conv2D(32,\n",
        "               kernel_size=3,\n",
        "               strides=2,\n",
        "               input_shape=img_shape,\n",
        "               padding='same'))\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Convolutional layer, from 14x14x32 into 7x7x64 tensor\n",
        "    model.add(\n",
        "        Conv2D(64,\n",
        "               kernel_size=3,\n",
        "               strides=2,\n",
        "               input_shape=img_shape,\n",
        "               padding='same'))\n",
        "\n",
        "    # Batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Convolutional layer, from 7x7x64 tensor into 3x3x128 tensor\n",
        "    model.add(\n",
        "        Conv2D(128,\n",
        "               kernel_size=3,\n",
        "               strides=2,\n",
        "               input_shape=img_shape,\n",
        "               padding='same'))\n",
        "\n",
        "    # Batch normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Leaky ReLU activation\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Droupout\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Flatten the tensor\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layer with num_classes neurons\n",
        "    model.add(Dense(num_classes))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hSeX8Z75IDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_supervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    # Softmax activation, giving predicted probability distribution over the real classes\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc_Skbce5IDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_unsupervised(discriminator_net):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(discriminator_net)\n",
        "\n",
        "    def predict(x):\n",
        "        # Transform distribution over real classes into a binary real-vs-fake probability\n",
        "        prediction = 1.0 - (1.0 /\n",
        "                            (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
        "        return prediction\n",
        "\n",
        "    # 'Real-vs-fake' output neuron defined above\n",
        "    model.add(Lambda(predict))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xz4dCm75IDJ",
        "colab_type": "text"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcTPOPCS5IDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "\n",
        "#     model = Sequential()\n",
        "\n",
        "    # Combined Generator -> Discriminator model\n",
        "#     model.add(generator)\n",
        "#     model.add(discriminator)\n",
        "    \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    label = Input(shape=(num_classes, ))\n",
        "    img = generator([z, label])\n",
        "    output = discriminator(img)\n",
        "    model = Model([z, label], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7641c283-58d4-4732-bb27-9b438457f0cb",
        "id": "GZhORKSp6P03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "generator = build_generator(z_dim)\n",
        "discriminator_unsupervised.trainable = False\n",
        "gan = build_gan(generator, discriminator_unsupervised)\n",
        "gan.summary()\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dense_28/BiasAdd:0\", shape=(?, 100), dtype=float32)\n",
            "Tensor(\"multiply_13/mul:0\", shape=(?, 100), dtype=float32)\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_41 (InputLayer)           (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_42 (InputLayer)           (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_16 (Model)                (None, 28, 28, 1)    1638221     input_41[0][0]                   \n",
            "                                                                 input_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_21 (Sequential)      (None, 1)            113930      model_16[1][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,752,151\n",
            "Trainable params: 1,637,837\n",
            "Non-trainable params: 114,314\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-fxWZ8Y5IDP",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPQ_Yvmz5IDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Core Discriminator network:\n",
        "# These layers are shared during supervised and unsupervised training\n",
        "discriminator_net = build_discriminator_net(img_shape)\n",
        "\n",
        "# Build & compile the Discriminator for supervised training\n",
        "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
        "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "\n",
        "# Build & compile the Discriminator for unsupervised training\n",
        "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
        "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX5cwJ_i5IDS",
        "colab_type": "text"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMRweuap5IDT",
        "colab_type": "code",
        "outputId": "a3adf8fa-2661-41fb-f00a-303d3b0810fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Build the Generator\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "# Keep Discriminator’s parameters constant for Generator training\n",
        "discriminator_unsupervised.trainable = False\n",
        "\n",
        "# Build and compile GAN model with fixed Discriminator to train the Generator\n",
        "# Note that we are using the Discriminator version with unsupervised output\n",
        "gan = build_gan(generator, discriminator_unsupervised)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dense_31/BiasAdd:0\", shape=(?, 100), dtype=float32)\n",
            "Tensor(\"multiply_14/mul:0\", shape=(?, 100), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfkxFfH5IDW",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQx1f61tT1sF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir models/models-label-1500\n",
        "%mkdir losses/losses-label-1500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKb4Xds65IDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "supervised_losses = []\n",
        "unsupervised_losses = []\n",
        "g_losses = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "\n",
        "def train(iterations, batch_size, save_interval):\n",
        "\n",
        "    # Labels for real images: all ones\n",
        "    real = np.ones((batch_size, 1))\n",
        "\n",
        "    # Labels for fake images: all zeros\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # -------------------------\n",
        "        #  Train the Discriminator\n",
        "        # -------------------------\n",
        "\n",
        "        # Get labeled examples\n",
        "        imgs, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # One-hot encode labels\n",
        "        labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "        # Get unlabeled examples\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        fake_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "        fake_labels = to_categorical(fake_labels, num_classes=num_classes)\n",
        "        gen_imgs = generator.predict([z, fake_labels])\n",
        "\n",
        "        # Train on real labeled examples\n",
        "        d_loss_supervised, accuracy = discriminator_supervised.train_on_batch(imgs, labels)\n",
        "\n",
        "        # Train on real unlabeled examples\n",
        "        d_loss_real = discriminator_unsupervised.train_on_batch(imgs_unlabeled, real)\n",
        "\n",
        "        # Train on fake examples\n",
        "        d_loss_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "        d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train the Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        gen_imgs = generator.predict([z, labels])\n",
        "\n",
        "        # Train Generator\n",
        "        g_loss = gan.train_on_batch([z,labels], np.ones((batch_size, 1)))\n",
        "\n",
        "        if (iteration + 1) % save_interval == 0:\n",
        "\n",
        "            # Save Discriminator supervised classification loss to be plotted after training\n",
        "            supervised_losses.append(d_loss_supervised)\n",
        "            unsupervised_losses.append(d_loss_unsupervised)\n",
        "            g_losses.append(g_loss)\n",
        "            \n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # Output training progress\n",
        "            print(\n",
        "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f] [G loss: %f]\"\n",
        "                % (iteration + 1, d_loss_supervised, 100 * accuracy,\n",
        "                   d_loss_unsupervised, g_loss))\n",
        "            \n",
        "            discriminator_supervised.save(\"./models/models-label-\" + str(num_labeled) + \"/discriminator_supervised-\" + str(iteration+1) + \".h5\")\n",
        "            file1 = \"./losses/losses-label-\" + str(num_labeled) + \"/supervised_losses.json\"\n",
        "            file2 = \"./losses/losses-label-\" + str(num_labeled) + \"/unsupervised_losses.json\"\n",
        "            file3 = \"./losses/losses-label-\" + str(num_labeled) + \"/g_losses.json\"\n",
        "            with open(file1, 'w') as json_file:\n",
        "                  json.dump(str(supervised_losses), json_file)\n",
        "            with open(file2, 'w') as json_file:\n",
        "                  json.dump(str(unsupervised_losses), json_file)\n",
        "            with open(file3, 'w') as json_file:\n",
        "                  json.dump(str(g_losses), json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MoxwriW5IDb",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model and Inspect Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHHVLd0n5IDc",
        "colab_type": "text"
      },
      "source": [
        "Note that the `'Discrepancy between trainable weights and collected trainable'` warning from Keras is expected. It is by design: The Generator's trainable parameters are intentionally held constant during Discriminator training, and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0ZWH2w_r5IDe",
        "colab_type": "code",
        "outputId": "e0047867-c935-461c-9c2d-42f7f229f849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "# Set hyperparameters\n",
        "iterations = 8000\n",
        "batch_size = 32\n",
        "save_interval = 100\n",
        "\n",
        "# Train the SGAN for the specified number of iterations\n",
        "train(iterations, batch_size, save_interval)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100 [D loss supervised: 0.0058, acc.: 100.00%] [D loss unsupervised: 0.4383] [G loss: 3.068665]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-aeb532fa6399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the SGAN for the specified number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-191-69a978d9a09a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Train on fake examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_unsupervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0md_loss_unsupervised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T98POkWe5IDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "supervised_losses = np.array(supervised_losses)\n",
        "unsupervised_losses = np.array(unsupervised_losses)\n",
        "g_losses = np.array(g_losses)\n",
        "\n",
        "# Plot Discriminator supervised loss\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, supervised_losses, label=\"Discriminator supervised loss\", color='tab:blue')\n",
        "plt.plot(iteration_checkpoints, unsupervised_losses, label=\"Discriminator unsupervised loss\", color='tab:green')\n",
        "plt.plot(iteration_checkpoints, g_losses, label=\"Generator supervised loss\", color='tab:red')\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Discriminator(supervised + unsupervised) + Generator Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F61dqQWQ3ri6",
        "colab_type": "code",
        "outputId": "23456d88-9b0c-4183-f77c-e5e95a360453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(supervised_losses[-10:])"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00278353 0.01227447 0.00042866 0.00099266 0.00167445 0.00319302\n",
            " 0.00021582 0.00159243 0.0005405  0.00141361]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PScPJocN5IDk",
        "colab_type": "text"
      },
      "source": [
        "## SGAN Classifier – Training and Test Accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6BmzAGk5IDl",
        "colab_type": "code",
        "outputId": "0a69d704-0974-4745-bf21-1df9e47b4f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 0s 88us/step\n",
            "Training Accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctWFp3zX5IDo",
        "colab_type": "code",
        "outputId": "8ac9b435-99ac-49ae-e29e-3b286a3dd23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 75us/step\n",
            "Test Accuracy: 97.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDMoAU1r5IDv",
        "colab_type": "text"
      },
      "source": [
        "# Fully-Supervised Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA4C3_RL5IDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fully supervised classifier with the same network architecture as the SGAN Discriminator\n",
        "mnist_classifier = build_discriminator_supervised(build_discriminator_net(img_shape))\n",
        "mnist_classifier.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlJkb5fN5ID3",
        "colab_type": "code",
        "outputId": "2a99a2b5-7194-42e6-e913-808b7563363d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "imgs, labels = dataset.training_set()\n",
        "imgs = imgs[:num_labeled]\n",
        "labels = labels[:num_labeled]\n",
        "\n",
        "# One-hot encode labels\n",
        "labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# Train the classifier\n",
        "training = mnist_classifier.fit(x=imgs,\n",
        "                                y=labels,\n",
        "                                batch_size=32,\n",
        "                                epochs=200,\n",
        "                                verbose=1)\n",
        "losses = training.history['loss']\n",
        "accuracies = training.history['acc']"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.1292 - acc: 0.6460\n",
            "Epoch 2/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 0.4110 - acc: 0.8693\n",
            "Epoch 3/200\n",
            "1500/1500 [==============================] - 0s 212us/step - loss: 0.2558 - acc: 0.9193\n",
            "Epoch 4/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 0.1637 - acc: 0.9520\n",
            "Epoch 5/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.1026 - acc: 0.9720\n",
            "Epoch 6/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 0.0802 - acc: 0.9780\n",
            "Epoch 7/200\n",
            "1500/1500 [==============================] - 0s 180us/step - loss: 0.0590 - acc: 0.9833\n",
            "Epoch 8/200\n",
            "1500/1500 [==============================] - 0s 182us/step - loss: 0.0343 - acc: 0.9933\n",
            "Epoch 9/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0305 - acc: 0.9940\n",
            "Epoch 10/200\n",
            "1500/1500 [==============================] - 0s 198us/step - loss: 0.0252 - acc: 0.9973\n",
            "Epoch 11/200\n",
            "1500/1500 [==============================] - 0s 178us/step - loss: 0.0288 - acc: 0.9907\n",
            "Epoch 12/200\n",
            "1500/1500 [==============================] - 0s 180us/step - loss: 0.0231 - acc: 0.9960\n",
            "Epoch 13/200\n",
            "1500/1500 [==============================] - 0s 195us/step - loss: 0.0191 - acc: 0.9980\n",
            "Epoch 14/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 0.0167 - acc: 0.9960\n",
            "Epoch 15/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0125 - acc: 0.9993\n",
            "Epoch 16/200\n",
            "1500/1500 [==============================] - 0s 203us/step - loss: 0.0101 - acc: 0.9993\n",
            "Epoch 17/200\n",
            "1500/1500 [==============================] - 0s 184us/step - loss: 0.0088 - acc: 1.0000\n",
            "Epoch 18/200\n",
            "1500/1500 [==============================] - 0s 183us/step - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 19/200\n",
            "1500/1500 [==============================] - 0s 182us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 20/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 21/200\n",
            "1500/1500 [==============================] - 0s 179us/step - loss: 0.0041 - acc: 0.9993\n",
            "Epoch 22/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 23/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0061 - acc: 0.9993\n",
            "Epoch 24/200\n",
            "1500/1500 [==============================] - 0s 207us/step - loss: 0.0060 - acc: 0.9993\n",
            "Epoch 25/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 0.0064 - acc: 0.9993\n",
            "Epoch 26/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 0.0088 - acc: 0.9987\n",
            "Epoch 27/200\n",
            "1500/1500 [==============================] - 0s 189us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 28/200\n",
            "1500/1500 [==============================] - 0s 179us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 29/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 30/200\n",
            "1500/1500 [==============================] - 0s 176us/step - loss: 0.0048 - acc: 0.9987\n",
            "Epoch 31/200\n",
            "1500/1500 [==============================] - 0s 208us/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 32/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 33/200\n",
            "1500/1500 [==============================] - 0s 211us/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 34/200\n",
            "1500/1500 [==============================] - 0s 207us/step - loss: 0.0042 - acc: 0.9993\n",
            "Epoch 35/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0174 - acc: 0.9953\n",
            "Epoch 36/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 0.0300 - acc: 0.9893\n",
            "Epoch 37/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 0.0560 - acc: 0.9780\n",
            "Epoch 38/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0447 - acc: 0.9853\n",
            "Epoch 39/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 0.0379 - acc: 0.9887\n",
            "Epoch 40/200\n",
            "1500/1500 [==============================] - 0s 209us/step - loss: 0.0163 - acc: 0.9960\n",
            "Epoch 41/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 0.0125 - acc: 0.9980\n",
            "Epoch 42/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0134 - acc: 0.9940\n",
            "Epoch 43/200\n",
            "1500/1500 [==============================] - 0s 189us/step - loss: 0.0087 - acc: 0.9980\n",
            "Epoch 44/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 0.0058 - acc: 0.9993\n",
            "Epoch 45/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 0.0074 - acc: 0.9987\n",
            "Epoch 46/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0052 - acc: 0.9993\n",
            "Epoch 47/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 0.0087 - acc: 0.9973\n",
            "Epoch 48/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0095 - acc: 0.9980\n",
            "Epoch 49/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 0.0049 - acc: 0.9980\n",
            "Epoch 50/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 0.0052 - acc: 0.9987\n",
            "Epoch 51/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0056 - acc: 0.9987\n",
            "Epoch 52/200\n",
            "1500/1500 [==============================] - 0s 195us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 53/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 54/200\n",
            "1500/1500 [==============================] - 0s 207us/step - loss: 0.0063 - acc: 0.9980\n",
            "Epoch 55/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 56/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 57/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 58/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 59/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0039 - acc: 0.9993\n",
            "Epoch 60/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 61/200\n",
            "1500/1500 [==============================] - 0s 198us/step - loss: 0.0079 - acc: 0.9980\n",
            "Epoch 62/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 0.0097 - acc: 0.9967\n",
            "Epoch 63/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0059 - acc: 0.9973\n",
            "Epoch 64/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0067 - acc: 0.9973\n",
            "Epoch 65/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 0.0080 - acc: 0.9973\n",
            "Epoch 66/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 0.0052 - acc: 0.9987\n",
            "Epoch 67/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 68/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 69/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 8.9397e-04 - acc: 1.0000\n",
            "Epoch 70/200\n",
            "1500/1500 [==============================] - 0s 178us/step - loss: 4.9605e-04 - acc: 1.0000\n",
            "Epoch 71/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 72/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 6.8707e-04 - acc: 1.0000\n",
            "Epoch 73/200\n",
            "1500/1500 [==============================] - 0s 196us/step - loss: 3.6306e-04 - acc: 1.0000\n",
            "Epoch 74/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 3.7721e-04 - acc: 1.0000\n",
            "Epoch 75/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 76/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 9.9254e-04 - acc: 0.9993\n",
            "Epoch 77/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 4.1955e-04 - acc: 1.0000\n",
            "Epoch 78/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 3.4621e-04 - acc: 1.0000\n",
            "Epoch 79/200\n",
            "1500/1500 [==============================] - 0s 189us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 80/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 0.0038 - acc: 0.9987\n",
            "Epoch 81/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 0.0182 - acc: 0.9933\n",
            "Epoch 82/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0160 - acc: 0.9927\n",
            "Epoch 83/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0387 - acc: 0.9907\n",
            "Epoch 84/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 0.0347 - acc: 0.9873\n",
            "Epoch 85/200\n",
            "1500/1500 [==============================] - 0s 196us/step - loss: 0.0123 - acc: 0.9940\n",
            "Epoch 86/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 0.0110 - acc: 0.9960\n",
            "Epoch 87/200\n",
            "1500/1500 [==============================] - 0s 204us/step - loss: 0.0255 - acc: 0.9927\n",
            "Epoch 88/200\n",
            "1500/1500 [==============================] - 0s 211us/step - loss: 0.0211 - acc: 0.9933\n",
            "Epoch 89/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 0.0082 - acc: 0.9973\n",
            "Epoch 90/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 0.0066 - acc: 0.9980\n",
            "Epoch 91/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 0.0054 - acc: 0.9987\n",
            "Epoch 92/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 0.0081 - acc: 0.9967\n",
            "Epoch 93/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 0.0051 - acc: 0.9980\n",
            "Epoch 94/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 95/200\n",
            "1500/1500 [==============================] - 0s 198us/step - loss: 0.0036 - acc: 0.9993\n",
            "Epoch 96/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 0.0050 - acc: 0.9987\n",
            "Epoch 97/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 0.0143 - acc: 0.9953\n",
            "Epoch 98/200\n",
            "1500/1500 [==============================] - 0s 181us/step - loss: 0.0083 - acc: 0.9980\n",
            "Epoch 99/200\n",
            "1500/1500 [==============================] - 0s 178us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 100/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 101/200\n",
            "1500/1500 [==============================] - 0s 181us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 102/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0017 - acc: 0.9987\n",
            "Epoch 103/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0014 - acc: 0.9993\n",
            "Epoch 104/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 3.1996e-04 - acc: 1.0000\n",
            "Epoch 105/200\n",
            "1500/1500 [==============================] - 0s 191us/step - loss: 0.0012 - acc: 0.9993\n",
            "Epoch 106/200\n",
            "1500/1500 [==============================] - 0s 209us/step - loss: 7.6654e-04 - acc: 1.0000\n",
            "Epoch 107/200\n",
            "1500/1500 [==============================] - 0s 214us/step - loss: 6.1983e-04 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0072 - acc: 0.9973\n",
            "Epoch 109/200\n",
            "1500/1500 [==============================] - 0s 189us/step - loss: 0.0059 - acc: 0.9967\n",
            "Epoch 110/200\n",
            "1500/1500 [==============================] - 0s 204us/step - loss: 0.0060 - acc: 0.9987\n",
            "Epoch 111/200\n",
            "1500/1500 [==============================] - 0s 196us/step - loss: 0.0033 - acc: 0.9993\n",
            "Epoch 112/200\n",
            "1500/1500 [==============================] - 0s 182us/step - loss: 0.0071 - acc: 0.9980\n",
            "Epoch 113/200\n",
            "1500/1500 [==============================] - 0s 181us/step - loss: 0.0038 - acc: 0.9980\n",
            "Epoch 114/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 115/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 0.0065 - acc: 0.9973\n",
            "Epoch 116/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0040 - acc: 0.9987\n",
            "Epoch 117/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 4.8993e-04 - acc: 1.0000\n",
            "Epoch 118/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 2.7323e-04 - acc: 1.0000\n",
            "Epoch 119/200\n",
            "1500/1500 [==============================] - 0s 189us/step - loss: 4.9098e-04 - acc: 1.0000\n",
            "Epoch 120/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 4.9406e-04 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "1500/1500 [==============================] - 0s 191us/step - loss: 7.2453e-04 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 4.4504e-04 - acc: 1.0000\n",
            "Epoch 123/200\n",
            "1500/1500 [==============================] - 0s 182us/step - loss: 4.6158e-04 - acc: 1.0000\n",
            "Epoch 124/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 2.9870e-04 - acc: 1.0000\n",
            "Epoch 125/200\n",
            "1500/1500 [==============================] - 0s 188us/step - loss: 4.8400e-04 - acc: 1.0000\n",
            "Epoch 126/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 8.5119e-04 - acc: 0.9993\n",
            "Epoch 127/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 6.4405e-04 - acc: 1.0000\n",
            "Epoch 128/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 3.5390e-04 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 1.8990e-04 - acc: 1.0000\n",
            "Epoch 130/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 1.6568e-04 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "1500/1500 [==============================] - 0s 204us/step - loss: 0.0022 - acc: 0.9987\n",
            "Epoch 132/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 7.3813e-04 - acc: 1.0000\n",
            "Epoch 133/200\n",
            "1500/1500 [==============================] - 0s 191us/step - loss: 7.9877e-04 - acc: 1.0000\n",
            "Epoch 134/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 2.3881e-04 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 1.6805e-04 - acc: 1.0000\n",
            "Epoch 136/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 3.3529e-04 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "1500/1500 [==============================] - 0s 184us/step - loss: 2.8201e-04 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "1500/1500 [==============================] - 0s 198us/step - loss: 1.4620e-04 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 2.0559e-04 - acc: 1.0000\n",
            "Epoch 140/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 7.7622e-04 - acc: 0.9993\n",
            "Epoch 141/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 9.4524e-04 - acc: 1.0000\n",
            "Epoch 142/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 0.0044 - acc: 0.9980\n",
            "Epoch 143/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 0.0148 - acc: 0.9960\n",
            "Epoch 144/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 0.0346 - acc: 0.9920\n",
            "Epoch 145/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0382 - acc: 0.9900\n",
            "Epoch 146/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 0.0240 - acc: 0.9900\n",
            "Epoch 147/200\n",
            "1500/1500 [==============================] - 0s 182us/step - loss: 0.0192 - acc: 0.9960\n",
            "Epoch 148/200\n",
            "1500/1500 [==============================] - 0s 184us/step - loss: 0.0065 - acc: 0.9960\n",
            "Epoch 149/200\n",
            "1500/1500 [==============================] - 0s 195us/step - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 150/200\n",
            "1500/1500 [==============================] - 0s 191us/step - loss: 0.0026 - acc: 0.9993\n",
            "Epoch 151/200\n",
            "1500/1500 [==============================] - 0s 183us/step - loss: 0.0045 - acc: 0.9973\n",
            "Epoch 152/200\n",
            "1500/1500 [==============================] - 0s 177us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "1500/1500 [==============================] - 0s 208us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 0.0018 - acc: 0.9993\n",
            "Epoch 155/200\n",
            "1500/1500 [==============================] - 0s 204us/step - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "1500/1500 [==============================] - 0s 203us/step - loss: 0.0093 - acc: 0.9967\n",
            "Epoch 157/200\n",
            "1500/1500 [==============================] - 0s 209us/step - loss: 0.0066 - acc: 0.9980\n",
            "Epoch 158/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 6.6984e-04 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "1500/1500 [==============================] - 0s 204us/step - loss: 6.9144e-04 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "1500/1500 [==============================] - 0s 189us/step - loss: 2.8769e-04 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "1500/1500 [==============================] - 0s 191us/step - loss: 3.2642e-04 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0025 - acc: 0.9987\n",
            "Epoch 164/200\n",
            "1500/1500 [==============================] - 0s 197us/step - loss: 6.6352e-04 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 3.4400e-04 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "1500/1500 [==============================] - 0s 204us/step - loss: 3.3444e-04 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "1500/1500 [==============================] - 0s 211us/step - loss: 2.4791e-04 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 2.8257e-04 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "1500/1500 [==============================] - 0s 191us/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "1500/1500 [==============================] - 0s 211us/step - loss: 0.0011 - acc: 0.9993\n",
            "Epoch 171/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 5.3461e-04 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "1500/1500 [==============================] - 0s 215us/step - loss: 6.6000e-04 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0066 - acc: 0.9980\n",
            "Epoch 174/200\n",
            "1500/1500 [==============================] - 0s 205us/step - loss: 0.0027 - acc: 0.9987\n",
            "Epoch 175/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 9.1792e-04 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 0.0037 - acc: 0.9980\n",
            "Epoch 177/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0031 - acc: 0.9987\n",
            "Epoch 178/200\n",
            "1500/1500 [==============================] - 0s 198us/step - loss: 0.0147 - acc: 0.9967\n",
            "Epoch 179/200\n",
            "1500/1500 [==============================] - 0s 199us/step - loss: 0.0059 - acc: 0.9993\n",
            "Epoch 180/200\n",
            "1500/1500 [==============================] - 0s 228us/step - loss: 0.0081 - acc: 0.9973\n",
            "Epoch 181/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 0.0163 - acc: 0.9960\n",
            "Epoch 182/200\n",
            "1500/1500 [==============================] - 0s 182us/step - loss: 0.0029 - acc: 0.9993\n",
            "Epoch 183/200\n",
            "1500/1500 [==============================] - 0s 198us/step - loss: 0.0028 - acc: 0.9993\n",
            "Epoch 184/200\n",
            "1500/1500 [==============================] - 0s 202us/step - loss: 0.0024 - acc: 0.9993\n",
            "Epoch 185/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0040 - acc: 0.9980\n",
            "Epoch 186/200\n",
            "1500/1500 [==============================] - 0s 190us/step - loss: 0.0032 - acc: 0.9987\n",
            "Epoch 187/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 0.0058 - acc: 0.9980\n",
            "Epoch 188/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0098 - acc: 0.9967\n",
            "Epoch 189/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 0.0082 - acc: 0.9973\n",
            "Epoch 190/200\n",
            "1500/1500 [==============================] - 0s 206us/step - loss: 0.0052 - acc: 0.9980\n",
            "Epoch 191/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 0.0086 - acc: 0.9980\n",
            "Epoch 192/200\n",
            "1500/1500 [==============================] - 0s 185us/step - loss: 0.0098 - acc: 0.9967\n",
            "Epoch 193/200\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 0.0079 - acc: 0.9967\n",
            "Epoch 194/200\n",
            "1500/1500 [==============================] - 0s 193us/step - loss: 0.0035 - acc: 0.9993\n",
            "Epoch 195/200\n",
            "1500/1500 [==============================] - 0s 194us/step - loss: 0.0022 - acc: 0.9987\n",
            "Epoch 196/200\n",
            "1500/1500 [==============================] - 0s 192us/step - loss: 0.0028 - acc: 0.9987\n",
            "Epoch 197/200\n",
            "1500/1500 [==============================] - 0s 201us/step - loss: 6.3806e-04 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "1500/1500 [==============================] - 0s 186us/step - loss: 2.1147e-04 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "1500/1500 [==============================] - 0s 200us/step - loss: 0.0076 - acc: 0.9973\n",
            "Epoch 200/200\n",
            "1500/1500 [==============================] - 0s 210us/step - loss: 0.0081 - acc: 0.9987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZYhSXjS5ID8",
        "colab_type": "code",
        "outputId": "0c66a32e-3fd2-4476-a79b-5d5f3723425d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Plot classification loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(losses), label=\"Loss\")\n",
        "plt.title(\"Classification Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa5016d12e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAE/CAYAAACEto0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiISI4oWImIiIjkiIKViIiISI78/9ZuQGK4WdWXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OX9tt085ID_",
        "colab_type": "code",
        "outputId": "98aefe15-329b-4e2a-a655-9ac943b7677a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Plot classification accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(accuracies), label=\"Accuracy\")\n",
        "plt.title(\"Classification Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa5016450f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAE/CAYAAAB8VnbnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkBApdIiIiIiH4/0M0MuywuVNQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oMPnpnR5IEB",
        "colab_type": "code",
        "outputId": "6a7818d6-0969-48b8-8312-68d237e9e476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = mnist_classifier.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 2s 1ms/step\n",
            "Training Accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2B2kNsq5IEF",
        "colab_type": "code",
        "outputId": "914362bc-f242-4181-ccd3-e4b68893627a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = mnist_classifier.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 80us/step\n",
            "Test Accuracy: 95.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UexjatKq5IEJ",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}